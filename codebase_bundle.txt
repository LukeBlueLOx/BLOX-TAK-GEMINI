Project Bundle: /home/luke_blue_lox/PycharmProjects/BLOX-TAK-GEMINI
========================================

--- START FILE: prompts.txt ---
        base_prompt = (
            f"JesteÅ› ekspertem prawnym - zacytuj i potem wyjaÅ›nij mi wskazane artykuÅ‚y z Polskiego KPC, "
            f"a takÅ¼e jakie prawa mi przysÅ‚ugujÄ… w zwiÄ…zku z tym. Opisz to w jÄ™zyku Polskim i nastÄ™pnie dodaj takÅ¼e "
            f"tÅ‚umaczenie w jÄ™zyku Angielskim. Podaj rÃ³wnieÅ¼ na koÅ„cu treÅ›Ä‡ uÅ¼ytego promptu - analogicznie w jÄ™zyku "
            f"Polskim i Angielskim, a takÅ¼e wersjÄ™ modelu jaki zostaÅ‚ uÅ¼yty - czyli: gemini-1.5-flash,"
            f"z peÅ‚nym timestamp: " + full_timestamp
        )

        base_prompt = (
            f"JesteÅ› ekspertem prawnym - przeanalizuj dokument pod kÄ…tem Prawa Polskiego i Uni Europejskiej."
            f"Zacytuj artykuÅ‚y i opisz jakie moje prawa zostaÅ‚y zÅ‚amane"
            f"Wynik podaj w jÄ™zyku Polskim i Angielskim. Podaj rÃ³wnieÅ¼ na koÅ„cu treÅ›Ä‡ uÅ¼ytego promptu - analogicznie w jÄ™zyku "
            f"Polskim i Angielskim, a takÅ¼e wersjÄ™ modelu jaki zostaÅ‚ uÅ¼yty - czyli: gemini-1.5-flash,"
            f"z peÅ‚nym timestamp: " + full_timestamp
        )



        base_overall_prompt = (
        #f"JesteÅ› ekspertem prawnym. Otrzymujesz seriÄ™ analiz prawnych (lub ich fragmentÃ³w), "
        #f"ktÃ³re dotyczÄ… rÃ³Å¼nych przypadkÃ³w naruszenia praw i wolnoÅ›ci obywatelskich w Å›wietle Konstytucji RP. "
        #f"Twoim zadaniem jest stworzenie JEDNEGO, SPÃ“JNEGO podsumowania wszystkich tych analiz. "
        #f"Skup siÄ™ na: "
        #f"1. **GÅ‚Ã³wnych kategoriach naruszeÅ„ Konstytucji RP**, ktÃ³re powtarzajÄ… siÄ™ w analizach. "
        #f"2. **Wzorcach lub schematach dziaÅ‚ania**, jeÅ›li takie wystÄ™pujÄ… w opisanych przypadkach. "
        #f"3. **NajczÄ™Å›ciej naruszanych artykuÅ‚ach Konstytucji RP** (bez enumerowania kaÅ¼dego artykuÅ‚u z osobna, ale wskazanie dominujÄ…cych obszarÃ³w). "
        #f"4. **OgÃ³lnych wnioskach** dotyczÄ…cych natury problemÃ³w prawnych. "
        #f"Nie powtarzaj szczegÃ³Å‚Ã³w konkretnych przypadkÃ³w, chyba Å¼e sÄ… kluczowe dla zilustrowania wzorca. "
        #f"Podsumowanie powinno byÄ‡ zwiÄ™zÅ‚e, syntetyczne i skupione na ogÃ³lnym obrazie sytuacji prawnej. "
        #f"PomiÅ„ wszelkie wstÄ™py i zakoÅ„czenia, skup siÄ™ na meritum podsumowania."
    )
--- END FILE: prompts.txt ---

--- START FILE: requirements.txt ---
aiofiles==24.1.0
aiohappyeyeballs==2.6.1
aiohttp==3.12.13
aiosignal==1.3.2
annotated-types==0.7.0
anyio==4.9.0
asttokens==3.0.0
async-timeout==4.0.3
attrs==25.3.0
backcall==0.2.0
backoff==2.2.1
bcrypt==4.3.0
beautifulsoup4==4.13.4
bleach==6.2.0
build==1.2.2.post1
cachetools==5.5.2
certifi==2025.4.26
cffi==1.17.1
chardet==5.2.0
charset-normalizer==3.4.2
chromadb==1.0.13
click==8.2.1
coloredlogs==15.0.1
cryptography==43.0.3
dataclasses-json==0.6.7
decorator==5.2.1
defusedxml==0.7.1
distro==1.9.0
docopt==0.6.2
durationpy==0.10
emoji==2.14.1
exceptiongroup==1.3.0
executing==2.2.0
fastjsonschema==2.21.1
filelock==3.18.0
filetype==1.2.0
flatbuffers==25.2.10
frozenlist==1.7.0
fsspec==2025.5.1
generativeai==0.0.1
google-ai-generativelanguage==0.6.18
google-api-core==2.25.1
google-api-python-client==2.172.0
google-auth==2.40.3
google-auth-httplib2==0.2.0
google-generativeai==0.8.5
googleapis-common-protos==1.70.0
greenlet==3.2.3
grpcio==1.73.0
grpcio-status==1.71.0
h11==0.16.0
hf-xet==1.1.3
html5lib==1.1
httpcore==1.0.9
httplib2==0.22.0
httptools==0.6.4
httpx==0.28.1
httpx-sse==0.4.0
huggingface-hub==0.33.0
humanfriendly==10.0
idna==3.10
importlib_metadata==8.7.0
importlib_resources==6.5.2
ipython==8.12.3
jedi==0.19.2
Jinja2==3.1.6
joblib==1.5.1
jsonpatch==1.33
jsonpointer==3.0.0
jsonschema==4.24.0
jsonschema-specifications==2025.4.1
jupyter_client==8.6.3
jupyter_core==5.8.1
jupyterlab_pygments==0.3.0
kubernetes==33.1.0
langchain==0.3.25
langchain-chroma==0.2.4
langchain-community==0.3.25
langchain-core==0.3.65
langchain-google-genai==2.1.5
langchain-text-splitters==0.3.8
langdetect==1.0.9
langsmith==0.3.45
lxml==5.4.0
markdown-it-py==3.0.0
MarkupSafe==3.0.2
marshmallow==3.26.1
matplotlib-inline==0.1.7
mdurl==0.1.2
mistune==3.1.3
mmh3==5.1.0
mpmath==1.3.0
multidict==6.5.0
mypy_extensions==1.1.0
nbclient==0.10.2
nbconvert==7.16.6
nbformat==5.10.4
nest-asyncio==1.6.0
nltk==3.9.1
numpy==2.2.6
oauth2client==4.1.3
oauthlib==3.3.1
olefile==0.47
onnxruntime==1.22.0
opentelemetry-api==1.34.1
opentelemetry-exporter-otlp-proto-common==1.34.1
opentelemetry-exporter-otlp-proto-grpc==1.34.1
opentelemetry-proto==1.34.1
opentelemetry-sdk==1.34.1
opentelemetry-semantic-conventions==0.55b1
orjson==3.10.18
overrides==7.7.0
packaging==24.2
pandocfilters==1.5.1
parso==0.8.4
pdfminer.six==20250506
pdfplumber==0.11.7
pexpect==4.9.0
pickleshare==0.7.5
pillow==11.2.1
pipreqs==0.5.0
platformdirs==4.3.8
posthog==5.3.0
prompt_toolkit==3.0.51
propcache==0.3.2
proto-plus==1.26.1
protobuf==5.29.5
psutil==7.0.0
ptyprocess==0.7.0
pure_eval==0.2.3
py==1.4.22
pyasn1==0.6.1
pyasn1_modules==0.4.2
pybase64==1.4.1
pycparser==2.22
pydantic==2.11.6
pydantic-settings==2.9.1
pydantic_core==2.33.2
PyDrive2==1.21.3
Pygments==2.19.1
pyOpenSSL==24.2.1
pyparsing==3.2.3
pypdf==5.6.0
pypdfium2==4.30.1
PyPika==0.48.9
pyproject_hooks==1.2.0
pytest==2.6.0
python-dateutil==2.9.0.post0
python-dotenv==1.1.0
python-iso639==2025.2.18
python-magic==0.4.27
python-oxmsg==0.0.2
PyYAML==6.0.2
pyzmq==26.4.0
RapidFuzz==3.13.0
referencing==0.36.2
regex==2024.11.6
reportlab==4.4.1
requests==2.32.4
requests-oauthlib==2.0.0
requests-toolbelt==1.0.0
rich==14.0.0
rpds-py==0.25.1
rsa==4.9.1
shellingham==1.5.4
six==1.17.0
sniffio==1.3.1
soupsieve==2.7
SQLAlchemy==2.0.41
stack-data==0.6.3
sympy==1.14.0
tenacity==9.1.2
tinycss2==1.4.0
tokenizers==0.21.1
tomli==2.2.1
tornado==6.5.1
tqdm==4.67.1
traitlets==5.14.3
typer==0.16.0
typing-inspect==0.9.0
typing-inspection==0.4.1
typing_extensions==4.14.0
uno==0.3.3
unstructured==0.17.2
unstructured-client==0.36.0
uritemplate==4.2.0
urllib3==2.4.0
uvicorn==0.34.3
uvloop==0.21.0
watchfiles==1.1.0
wcwidth==0.2.13
webencodings==0.5.1
websocket-client==1.8.0
websockets==15.0.1
wrapt==1.17.2
yarg==0.1.9
yarl==1.20.1
zipp==3.23.0
zstandard==0.23.0

--- END FILE: requirements.txt ---

--- START FILE: README.md ---
<hr>

<div align="center"> 

### BLOX-TAK-GEMINI

</div> 

<hr>

<div align="center">

### âœŒï¸ğŸ¦…ğŸ‡ºğŸ‡¸ğŸ‡ªğŸ‡ºğŸ‡µğŸ‡±ğŸ‡ªğŸ‡ºğŸ‡ºğŸ‡¸ğŸ¦…âœŒï¸

https://www.linkedin.com/posts/lukebluelox_blox-tak-gemini-activity-7337727514405957632-m6BQ

https://www.linkedin.com/posts/lukebluelox_blox-tak-gemini-activity-7340991837509189632-7j_N

Gemini 2.5 PRO Generated Image (2K25Y)
<img src="Gemini_Generated_Image_qybg50qybg50qybg.jpeg" width="" height=""/>
<br>

Real-Oryginal Image (2K16Y)
<img src="Real-Oryginal.jpg" width="" height=""/>


<img src="BLOX-TAK_SF_WP.png" width="" height=""/>
<br>

<img src="1.png" width="" height=""/>
<br>

<img src="2.png" width="" height=""/>
<br>

<img src="3.png" width="" height=""/>
<br>

<img src="4.png" width="" height=""/>
<br>

<img src="5.png" width="" height=""/>

### âœŒâ™»ï¸ğŸŒŒğŸš€ğŸŒğŸŒğŸŒğŸ›°ğŸŒŒâ™»ï¸âœŒ

https://en.wikipedia.org/wiki/Gemini_(language_model)

https://en.wikipedia.org/wiki/Nvidia

</div>

<hr>

--- END FILE: README.md ---

--- START FILE: KNOWLEDGE_BASE/VECTOR_DB_LEGAL/initial.txt ---

--- END FILE: KNOWLEDGE_BASE/VECTOR_DB_LEGAL/initial.txt ---

--- START FILE: KNOWLEDGE_BASE/VECTOR_DB_CASE/initial.txt ---

--- END FILE: KNOWLEDGE_BASE/VECTOR_DB_CASE/initial.txt ---

--- START FILE: RAG/rag_query_assistant_test1.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# --- Standard Python Libraries ---
# --- Standardowe Biblioteki Pythona ---

# For filesystem operations like creating paths.
# Do operacji na systemie plikÃ³w, np. tworzenia Å›cieÅ¼ek.
import os

# For loading configuration from YAML files.
# Do wczytywania konfiguracji z plikÃ³w YAML.
import yaml

# For high-level file operations like deleting directory trees.
# Do operacji na plikach wysokiego poziomu, jak usuwanie drzew katalogÃ³w.
import shutil

# For creating and managing logs (to console and file).
# Do tworzenia i zarzÄ…dzania logami (do konsoli i pliku).
import logging
from logging.handlers import RotatingFileHandler

# --- LangChain & Google Libraries ---
# --- Biblioteki LangChain i Google ---

# LangChain's specific integrations for Google's Generative AI models (LLM and Embeddings).
# Specyficzne integracje LangChain dla modeli Generative AI od Google (LLM i Embeddings).
from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings

# LangChain's integration for the Chroma vector database.
# Integracja LangChain dla wektorowej bazy danych Chroma.
from langchain_chroma import Chroma

# A tool for creating reusable and structured prompts for the language model.
# NarzÄ™dzie do tworzenia reuÅ¼ywalnych i ustrukturyzowanych promptÃ³w dla modelu jÄ™zykowego.
from langchain.prompts import PromptTemplate

# A core component of LangChain Expression Language (LCEL) to pass inputs through the chain.
# Kluczowy komponent JÄ™zyka WyraÅ¼eÅ„ LangChain (LCEL) do przekazywania danych wejÅ›ciowych w Å‚aÅ„cuchu.
from langchain.schema.runnable import RunnablePassthrough

# A simple parser to convert the language model's output into a standard string.
# Prosty parser do konwersji wyniku modelu jÄ™zykowego na standardowy ciÄ…g znakÃ³w.
from langchain.schema.output_parser import StrOutputParser

# --- Local Project Modules ---
# --- Lokalne ModuÅ‚y Projektu ---

# Importing our custom function from the preprocessor script to get prepared documents.
# Importowanie naszej wÅ‚asnej funkcji ze skryptu preprocesora, aby pobraÄ‡ przygotowane dokumenty.
from RAG.rag_data_preprocessor import get_documents_from_source


def setup_logger(log_folder: str):
    """
    Sets up a logger to output to both console and a rotating file.
    Konfiguruje logger do zapisu zarÃ³wno do konsoli, jak i do rotacyjnego pliku.
    """
    if not os.path.exists(log_folder):
        os.makedirs(log_folder)

    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)

    if logger.hasHandlers():
        logger.handlers.clear()

    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    fh = RotatingFileHandler(os.path.join(log_folder, 'rag_assistant.log'), maxBytes=5 * 1024 * 1024, backupCount=3)
    fh.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    ch.setFormatter(formatter)
    fh.setFormatter(formatter)
    logger.addHandler(ch)
    logger.addHandler(fh)
    return logger


def load_configuration(config_path="scripts/config.yaml"):
    """
    Loads the main configuration file.
    Wczytuje gÅ‚Ã³wny plik konfiguracyjny.
    """
    try:
        with open(config_path, "r", encoding="utf-8") as cr:
            config = yaml.full_load(cr)

        base_path = config['base_path']
        rag_config = config['rag_pipeline_config']

        conf = {
            "GOOGLE_API_KEY": config['KEY'],
            "LEGAL_SOURCE": os.path.join(base_path, rag_config['legal_source_folder']),
            "CASE_SOURCE": os.path.join(base_path, rag_config['case_source_folder']),
            "LEGAL_DB": os.path.join(base_path, rag_config['vector_db_legal_folder']),
            "CASE_DB": os.path.join(base_path, rag_config['vector_db_case_folder']),
            "LOG_FOLDER": os.path.join(base_path, rag_config['log_folder']),
            "EMBEDDING_MODEL": rag_config['embedding_model'],
            "LLM_MODEL": rag_config['llm_model']
        }
        print("English: Configuration loaded successfully.\nPolski: Konfiguracja zaÅ‚adowana pomyÅ›lnie.")
        return conf
    except Exception as e:
        print(
            f"English: FATAL ERROR loading configuration from {config_path}: {e}\nPolski: KRYTYCZNY BÅÄ„D podczas Å‚adowania konfiguracji z {config_path}: {e}")
        return None


# --- Global Initialization ---
# --- Globalna Inicjalizacja ---
CONFIG = load_configuration()
if CONFIG:
    logger = setup_logger(CONFIG['LOG_FOLDER'])
    LLM = GoogleGenerativeAI(model=CONFIG['LLM_MODEL'], google_api_key=CONFIG['GOOGLE_API_KEY'])
    EMBEDDINGS = GoogleGenerativeAIEmbeddings(model=CONFIG['EMBEDDING_MODEL'], google_api_key=CONFIG['GOOGLE_API_KEY'])
else:
    logger = setup_logger('LOGS')


def initialize_vector_store(db_path: str, source_folder: str, corpus_type: str,
                            force_rebuild: bool = False) -> Chroma | None:
    """
    Initializes a Chroma vector store. Builds it if it doesn't exist or if rebuild is forced.
    Inicjalizuje bazÄ™ wektorowÄ… Chroma. Buduje jÄ…, jeÅ›li nie istnieje lub wymuszono przebudowÄ™.
    """
    if os.path.exists(db_path) and not force_rebuild:
        logger.info(f"Loading existing vector database from: {db_path}")
        print(
            f"English: Loading existing vector database from: {db_path}\nPolski: ÅadujÄ™ istniejÄ…cÄ… bazÄ™ wektorowÄ… z: {db_path}")
        try:
            db = Chroma(persist_directory=db_path, embedding_function=EMBEDDINGS)
            return db
        except Exception as e:
            logger.error(
                f"Failed to load existing database at {db_path}. It might be corrupted. Try rebuilding. Error: {e}")
            print(
                f"English: ERROR - Failed to load existing database at {db_path}. Try rebuilding. Error: {e}\nPolski: BÅÄ„D - Nie udaÅ‚o siÄ™ zaÅ‚adowaÄ‡ istniejÄ…cej bazy z {db_path}. SprÃ³buj jÄ… przebudowaÄ‡. BÅ‚Ä…d: {e}")
            return None

    logger.info(f"Rebuilding vector database for corpus '{corpus_type}'...")
    print(
        f"English: Rebuilding vector database for corpus '{corpus_type}'...\nPolski: PrzebudowujÄ™ bazÄ™ wektorowÄ… dla korpusu '{corpus_type}'...")

    if os.path.exists(db_path):
        shutil.rmtree(db_path)

    documents = get_documents_from_source(source_folder, corpus_type)
    if not documents:
        logger.error(f"No documents found for corpus '{corpus_type}'. Database not built.")
        print(
            f"English: ERROR - No documents found for corpus '{corpus_type}'. Database not built.\nPolski: BÅÄ„D - Nie znaleziono dokumentÃ³w dla korpusu '{corpus_type}'. Baza danych nie zostaÅ‚a zbudowana.")
        return None

    db = Chroma.from_documents(documents, EMBEDDINGS, persist_directory=db_path)
    logger.info(f"Successfully built and saved vector database at: {db_path}")
    print(
        f"English: Successfully built and saved vector database at: {db_path}\nPolski: PomyÅ›lnie zbudowano i zapisano bazÄ™ wektorowÄ… w: {db_path}")
    return db


def format_docs(docs: list) -> str:
    """
    Helper function to format retrieved documents for the prompt, including metadata.
    Funkcja pomocnicza do formatowania pobranych dokumentÃ³w na potrzeby promptu, wÅ‚Ä…czajÄ…c metadane.
    """
    formatted_docs = []
    for doc in docs:
        if doc.metadata.get("type") == "legal_corpus":
            header = f"Fragment z prawa: {doc.metadata.get('source', 'b.d.')}, ArtykuÅ‚: {doc.metadata.get('article', 'b.d.')}"
        elif doc.metadata.get("type") == "case_corpus":
            header = f"Fragment z dowodu: {doc.metadata.get('source_original', 'b.d.')} (w archiwum: {doc.metadata.get('source_archive', 'b.d.')})"
        else:
            header = "Fragment z nieznanego ÅºrÃ³dÅ‚a"

        content = doc.page_content
        formatted_docs.append(f"{header}\n---\n{content}\n---\n")
    return "\n".join(formatted_docs)


def perform_legal_analysis(query: str, legal_retriever, case_retriever):
    """
    Performs a multi-step RAG query to synthesize facts and law.
    Wykonuje wieloetapowe zapytanie RAG w celu syntezy faktÃ³w i prawa.
    """
    logger.info(f"Performing full legal analysis for query: '{query[:80]}...'")
    print(
        f"English: Performing full legal analysis for query: '{query[:80]}...'\nPolski: WykonujÄ™ peÅ‚nÄ… analizÄ™ prawnÄ… dla zapytania: '{query[:80]}...'")

    logger.info("Step 1: Retrieving facts from case_db...")
    print(
        "English: Step 1: Retrieving facts from case database...\nPolski: Krok 1: Pobieram fakty z bazy danych sprawy...")
    factual_context_docs = case_retriever.invoke(query)
    factual_context_str = format_docs(factual_context_docs)

    logger.info("Step 2: Retrieving law from legal_db...")
    print(
        "English: Step 2: Retrieving law from legal database...\nPolski: Krok 2: Pobieram prawo z bazy danych prawnej...")
    legal_context_docs = legal_retriever.invoke(query)
    legal_context_str = format_docs(legal_context_docs)

    logger.info("Step 3: Synthesizing final answer with LLM...")
    print(
        "English: Step 3: Synthesizing final answer with LLM...\nPolski: Krok 3: SyntezujÄ™ ostatecznÄ… odpowiedÅº za pomocÄ… LLM...")

    final_template = """JesteÅ› ekspertem prawnym i analitykiem. Twoim zadaniem jest staranna analiza przedstawionego stanu faktycznego w Å›wietle zaÅ‚Ä…czonych przepisÃ³w prawnych, aby odpowiedzieÄ‡ na pytanie uÅ¼ytkownika. OdpowiedÅº musi byÄ‡ spÃ³jna, logiczna i odwoÅ‚ywaÄ‡ siÄ™ zarÃ³wno do faktÃ³w, jak i do prawa, cytujÄ…c ÅºrÃ³dÅ‚a w nawiasach kwadratowych po kaÅ¼dej informacji, np. [ÅºrÃ³dÅ‚o: KODEKS_KARNY.pdf, Art. 148].

PYTANIE UÅ»YTKOWNIKA:
{question}

ZEBRANE FAKTY (Z DOKUMENTÃ“W SPRAWY):
{factual_context}

ZEBRANE PRZEPISY PRAWNE:
{legal_context}

KOMPLEKSOWA ANALIZA PRAWNA:"""

    prompt = PromptTemplate.from_template(final_template)

    analysis_chain = prompt | LLM | StrOutputParser()

    response = analysis_chain.invoke({
        "question": query,
        "factual_context": factual_context_str,
        "legal_context": legal_context_str
    })

    print("\n" + "=" * 25 + " WYNIK ANALIZY " + "=" * 25)
    print(response)
    print("=" * 65 + "\n")


if __name__ == "__main__":
    if not CONFIG:
        logger.critical("Configuration failed to load. Exiting.")
        print(
            "English: FATAL - Configuration failed to load. Exiting.\nPolski: BÅÄ„D KRYTYCZNY - Nie udaÅ‚o siÄ™ wczytaÄ‡ konfiguracji. Zamykanie.")
        exit()

    # --- Rebuild Flags ---
    # Ustaw na True, jeÅ›li chcesz wymusiÄ‡ przebudowanie konkretnej bazy
    # Set to True if you want to force a rebuild of a specific database
    REBUILD_LEGAL_DB = False
    REBUILD_CASE_DB = False

    legal_db = initialize_vector_store(
        db_path=CONFIG['LEGAL_DB'],
        source_folder=CONFIG['LEGAL_SOURCE'],
        corpus_type='legal',
        force_rebuild=REBUILD_LEGAL_DB
    )

    case_db = initialize_vector_store(
        db_path=CONFIG['CASE_DB'],
        source_folder=CONFIG['CASE_SOURCE'],
        corpus_type='case',
        force_rebuild=REBUILD_CASE_DB
    )

    if not legal_db or not case_db:
        logger.critical("One or more databases failed to initialize. Cannot proceed with queries.")
        print(
            "English: CRITICAL - One or more databases failed to initialize. Cannot proceed.\nPolski: BÅÄ„D KRYTYCZNY - Inicjalizacja jednej lub wiÄ™cej baz danych nie powiodÅ‚a siÄ™. Nie moÅ¼na kontynuowaÄ‡.")
    else:
        legal_retriever = legal_db.as_retriever(search_kwargs={'k': 5})
        case_retriever = case_db.as_retriever(search_kwargs={'k': 8})

        # --- Ask your question here ---
        # --- Zadaj swoje pytanie tutaj ---
        user_question = "Jakie prawa gwarantowane przez KonstytucjÄ™ RP mogÅ‚y zostaÄ‡ naruszone, biorÄ…c pod uwagÄ™ treÅ›Ä‡ moich pism, w ktÃ³rych opisujÄ™ brak Å›rodkÃ³w do Å¼ycia i problemy zdrowotne?"

        perform_legal_analysis(
            query=user_question,
            legal_retriever=legal_retriever,
            case_retriever=case_retriever
        )
--- END FILE: RAG/rag_query_assistant_test1.py ---

--- START FILE: RAG/rag_data_preprocessor.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# --- Libraries / Biblioteki ---

# For filesystem operations, e.g., creating paths and checking existence.
# Do operacji na systemie plikÃ³w, np. tworzenia Å›cieÅ¼ek i sprawdzania istnienia.
import os

# For using regular expressions to split text based on patterns.
# Do uÅ¼ywania wyraÅ¼eÅ„ regularnych do dzielenia tekstu na podstawie wzorcÃ³w.
import re

# For creating and managing logs (to console and file).
# Do tworzenia i zarzÄ…dzania logami (do konsoli i pliku).
import logging
from logging.handlers import RotatingFileHandler

# LangChain's standard data structure for a piece of text with metadata.
# Standardowa struktura danych LangChain dla fragmentu tekstu z metadanymi.
from langchain.schema.document import Document

# LangChain's tool for splitting large texts into smaller, overlapping chunks.
# NarzÄ™dzie LangChain do dzielenia duÅ¼ych tekstÃ³w na mniejsze, nakÅ‚adajÄ…ce siÄ™ fragmenty.
from langchain.text_splitter import RecursiveCharacterTextSplitter

# A robust library for extracting text and data from PDF files.
# Solidna biblioteka do wyciÄ…gania tekstu i danych z plikÃ³w PDF.
import pdfplumber


def setup_logger(log_folder: str):
    """
    Sets up a logger to output to both console and a rotating file.
    Konfiguruje logger do zapisu zarÃ³wno do konsoli, jak i do rotacyjnego pliku.
    """
    if not os.path.exists(log_folder):
        os.makedirs(log_folder)

    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)

    # Prevents adding multiple handlers if the function is called more than once
    # Zapobiega dodawaniu wielu handlerÃ³w, jeÅ›li funkcja zostanie wywoÅ‚ana wiÄ™cej niÅ¼ raz
    if logger.hasHandlers():
        logger.handlers.clear()

    # Console handler
    # Handler konsoli
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)

    # File handler
    # Handler pliku
    fh = RotatingFileHandler(
        os.path.join(log_folder, 'rag_preprocessor.log'),
        maxBytes=5 * 1024 * 1024,  # 5 MB
        backupCount=3
    )
    fh.setLevel(logging.INFO)

    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    ch.setFormatter(formatter)
    fh.setFormatter(formatter)

    logger.addHandler(ch)
    logger.addHandler(fh)

    return logger


# Initialize logger here, assuming a LOGS folder exists at the project root for now
# Inicjalizacja loggera, zakÅ‚adajÄ…c na razie folder LOGS w gÅ‚Ã³wnym katalogu projektu
logger = setup_logger('LOGS')


def process_legal_document(file_path: str) -> list[Document]:
    """
    Processes a structured legal PDF, splitting it by articles.
    Przetwarza ustrukturyzowany prawny plik PDF, dzielÄ…c go na artykuÅ‚y.
    """
    file_name = os.path.basename(file_path)
    logger.info(f"Processing legal document: {file_name}")
    print(f"English: Processing legal document: {file_name}\nPolski: Przetwarzam dokument prawny: {file_name}")

    try:
        with pdfplumber.open(file_path) as pdf:
            full_text = "\n".join(page.extract_text() or "" for page in pdf.pages)
    except Exception as e:
        logger.error(f"Could not read PDF file {file_path}: {e}")
        print(
            f"English: ERROR - Could not read PDF file {file_path}: {e}\nPolski: BÅÄ„D - Nie moÅ¼na odczytaÄ‡ pliku PDF {file_path}: {e}")
        return []

    chunks = re.split(r'(?=\nArt\.\s*\d+[a-zA-Z]?\.?)', full_text)

    documents = []
    for chunk in chunks:
        if not chunk.strip():
            continue

        match = re.search(r'^(Art\.\s*\d+[a-zA-Z]?\.?)', chunk)
        article_num = match.group(1).strip() if match else "N/A"

        doc = Document(
            page_content=chunk.strip(),
            metadata={
                "source": file_name,
                "article": article_num,
                "type": "legal_corpus"
            }
        )
        documents.append(doc)

    logger.info(f"  > Found and processed {len(documents)} articles/fragments.")
    print(
        f"English:   > Found and processed {len(documents)} articles/fragments.\nPolski:   > Znaleziono i przetworzono {len(documents)} artykuÅ‚Ã³w/fragmentÃ³w.")
    return documents


def process_case_document_archive(file_path: str) -> list[Document]:
    """
    Processes a 'Combined_Content' PDF. It splits the content back into its
    original source files in memory, then chunks them.
    Przetwarza plik PDF typu 'Combined_Content'. Dzieli zawartoÅ›Ä‡ z powrotem na
    oryginalne pliki w pamiÄ™ci, a nastÄ™pnie je chunkuje.
    """
    archive_name = os.path.basename(file_path)
    logger.info(f"Processing case document archive: {archive_name}")
    print(
        f"English: Processing case document archive: {archive_name}\nPolski: Przetwarzam archiwum dokumentÃ³w sprawy: {archive_name}")

    try:
        with pdfplumber.open(file_path) as pdf:
            full_text = "\n".join(page.extract_text() or "" for page in pdf.pages)
    except Exception as e:
        logger.error(f"Could not read PDF archive file {file_path}: {e}")
        print(
            f"English: ERROR - Could not read PDF archive file {file_path}: {e}\nPolski: BÅÄ„D - Nie moÅ¼na odczytaÄ‡ pliku archiwum PDF {file_path}: {e}")
        return []

    parts = re.split(r'--- BEGINNING OF FILE: (.*?) ---', full_text)

    documents = []
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)

    for i in range(1, len(parts), 2):
        original_filename = parts[i].strip()
        content = parts[i + 1].split('--- END OF FILE:')[0].strip()

        logger.info(f"  > Extracting virtual file: {original_filename}")
        print(
            f"English:   > Extracting virtual file: {original_filename}\nPolski:   > EkstrahujÄ™ wirtualny plik: {original_filename}")

        if not content:
            logger.warning(f"    > Content for {original_filename} is empty. Skipping.")
            print(
                f"English:     > Content for {original_filename} is empty. Skipping.\nPolski:     > ZawartoÅ›Ä‡ dla {original_filename} jest pusta. Pomijam.")
            continue

        chunks = text_splitter.split_text(content)

        for chunk_content in chunks:
            doc = Document(
                page_content=chunk_content,
                metadata={
                    "source_original": original_filename,
                    "source_archive": archive_name,
                    "type": "case_corpus"
                }
            )
            documents.append(doc)

    logger.info(f"  > Extracted {len(documents)} total chunks from the archive.")
    print(
        f"English:   > Extracted {len(documents)} total chunks from the archive.\nPolski:   > Wyekstrahowano {len(documents)} wszystkich chunkÃ³w z archiwum.")
    return documents


def get_documents_from_source(source_folder: str, corpus_type: str) -> list[Document]:
    """
    Main orchestrator function for the preprocessing step.
    GÅ‚Ã³wna funkcja orkiestrujÄ…ca dla kroku preprocessingu.
    """
    all_processed_docs = []

    logger.info(f"Starting data preprocessing for corpus '{corpus_type}' from folder: {source_folder}")
    print(
        f"\nEnglish: Starting data preprocessing for corpus '{corpus_type}' from folder: {source_folder}\nPolski: Rozpoczynam preprocessing danych dla korpusu '{corpus_type}' z folderu: {source_folder}")

    if not os.path.exists(source_folder):
        logger.error(f"Source folder not found: {source_folder}")
        print(
            f"English: ERROR - Source folder not found: {source_folder}\nPolski: BÅÄ„D - Folder ÅºrÃ³dÅ‚owy nie znaleziony: {source_folder}")
        return []

    for file_name in os.listdir(source_folder):
        if file_name.lower().endswith(".pdf"):
            file_path = os.path.join(source_folder, file_name)
            if corpus_type == 'legal':
                processed_docs = process_legal_document(file_path)
            elif corpus_type == 'case':
                processed_docs = process_case_document_archive(file_path)
            else:
                logger.warning(f"Unknown corpus type '{corpus_type}'. Skipping file {file_name}.")
                print(
                    f"English: WARNING - Unknown corpus type '{corpus_type}'. Skipping file {file_name}.\nPolski: OSTRZEÅ»ENIE - Nieznany typ korpusu '{corpus_type}'. Pomijam plik {file_name}.")
                processed_docs = []
            all_processed_docs.extend(processed_docs)

    logger.info(f"Finished preprocessing for '{corpus_type}'. Total documents prepared: {len(all_processed_docs)}")
    print(
        f"English: Finished preprocessing for '{corpus_type}'. Total documents prepared: {len(all_processed_docs)}\nPolski: ZakoÅ„czono preprocessing dla '{corpus_type}'. ÅÄ…czna liczba przygotowanych dokumentÃ³w: {len(all_processed_docs)}\n")
    return all_processed_docs
--- END FILE: RAG/rag_data_preprocessor.py ---

--- START FILE: RAG/rag_query_assistant_test.py ---
# --- Importing necessary libraries ---
# --- Importowanie niezbÄ™dnych bibliotek ---
import os  # Do operacji na systemie plikÃ³w / For filesystem operations
import yaml  # Do wczytywania plikÃ³w konfiguracyjnych / For loading configuration files
import shutil  # Do operacji na plikach wysokiego poziomu, np. usuwania folderÃ³w / For high-level file operations, e.g., removing directories
import time  # Do mierzenia czasu wykonania / For measuring execution time
import json  # Do pracy z plikami JSON (zapis logÃ³w) / For working with JSON files (log saving)
from datetime import datetime  # Do generowania znacznikÃ³w czasu / For generating timestamps
# Biblioteki LangChain do integracji z Google Gemini
# LangChain libraries for Google Gemini integration
from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings
# Biblioteka LangChain do przechowywania wektorÃ³w w ChromaDB
# LangChain library for storing vectors in ChromaDB
#from langchain.vectorstores.chroma import Chroma
from langchain_community.vectorstores import Chroma
# Biblioteka LangChain do Å‚adowania dokumentÃ³w z folderu
# LangChain library for loading documents from a directory
from langchain_community.document_loaders import DirectoryLoader
# Biblioteka LangChain do dzielenia tekstu na fragmenty (chunks)
# LangChain library for splitting text into chunks
from langchain.text_splitter import RecursiveCharacterTextSplitter
# Biblioteki LangChain do budowania Å‚aÅ„cucha RAG (prompt, przekazywanie danych, parser odpowiedzi)
# LangChain libraries for building the RAG chain (prompt, data passthrough, output parser)
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser


# --- Main Configuration Loading ---
# --- GÅ‚Ã³wne Å‚adowanie konfiguracji ---
def load_configuration():
    """
    Wczytuje konfiguracjÄ™ z pliku scripts/config.yaml i zwraca jÄ… jako sÅ‚ownik.
    Loads configuration from scripts/config.yaml file and returns it as a dictionary.
    """
    config_path = os.path.join(os.path.dirname(__file__), '..', 'scripts', 'config.yaml')
    if not os.path.exists(config_path): config_path = "scripts/config.yaml"

    try:
        with open(config_path, "r", encoding="utf-8") as cr:
            config = yaml.full_load(cr)
        conf = {
            "GOOGLE_API_KEY": config['KEY'],
            "BASE_PATH": config['base_path'],
            "TEXT_CACHE_FOLDER": os.path.join(config['base_path'], config['rag_pipeline_config']['text_cache_folder']),
            "VECTOR_DB_FOLDER": os.path.join(config['base_path'], config['rag_pipeline_config']['vector_db_folder']),
            "LOG_FOLDER": os.path.join(config['base_path'], "LOGS"),
            "EMBEDDING_MODEL": config['rag_pipeline_config']['embedding_model'],
            "LLM_MODEL": config['rag_pipeline_config']['llm_model']
        }
        print("Konfiguracja dla asystenta RAG zaÅ‚adowana pomyÅ›lnie.")
        print("RAG assistant configuration loaded successfully.")
        return conf
    except Exception as e:
        print(f"FATAL ERROR in configuration from path {config_path}: {e}")
        return None


# --- Log Management Function ---
# --- Funkcja ZarzÄ…dzania Logami ---
def save_session_log(log_folder, log_data):
    """
    Zapisuje log z sesji zapytaÅ„ i odpowiedzi do pliku JSON.
    Saves the query and answer session log to a JSON file.
    """
    session_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file_path = os.path.join(log_folder, f"rag_query_assistant_log_{session_timestamp}.json")
    try:
        with open(log_file_path, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, indent=4, ensure_ascii=False)
        print(f"\nLog sesji zapytaÅ„ zapisano do: {log_file_path}")
        print(f"Query session log saved to: {log_file_path}")
    except IOError as e:
        print(f"BÅÄ„D: Nie moÅ¼na zapisaÄ‡ pliku logu sesji: {e}")
        print(f"ERROR: Could not write session log file: {e}")


# --- Initialization ---
# --- Inicjalizacja ---
CONFIG = load_configuration()
LLM = None
EMBEDDINGS = None
if CONFIG:
    try:
        LLM = GoogleGenerativeAI(model=CONFIG['LLM_MODEL'], google_api_key=CONFIG['GOOGLE_API_KEY'])
        EMBEDDINGS = GoogleGenerativeAIEmbeddings(model=CONFIG['EMBEDDING_MODEL'],
                                                  google_api_key=CONFIG['GOOGLE_API_KEY'])
        os.makedirs(CONFIG['LOG_FOLDER'], exist_ok=True)
        print("Modele Gemini (LLM i Embeddings) gotowe do pracy.")
        print("Gemini model (LLM and Embeddings) are ready.")
    except Exception as e:
        print(f"BÅÄ„D: Nie moÅ¼na skonfigurowaÄ‡ Gemini API: {e}")
        CONFIG = None


# --- Core RAG Functions ---
# --- GÅ‚Ã³wne funkcje RAG ---
def build_vector_store_from_cache(session_log_ref):
    """
    Buduje (lub przebudowuje) bazÄ™ wektorowÄ… na podstawie plikÃ³w .txt z folderu cache.
    Builds (or rebuilds) the vector store based on .txt files from the cache folder.
    """
    if not CONFIG: return

    start_time = time.time()
    log_entry = {"action": "build_vector_store", "start_utc": datetime.utcnow().isoformat(), "status": "In-Progress"}

    try:
        vector_db_path = CONFIG['VECTOR_DB_FOLDER']
        if os.path.exists(vector_db_path): shutil.rmtree(vector_db_path); print(
            f"UsuniÄ™to istniejÄ…cÄ… bazÄ™ danych w: {vector_db_path}")

        loader = DirectoryLoader(CONFIG['TEXT_CACHE_FOLDER'], glob="**/*.txt", recursive=True)
        documents = loader.load()
        if not documents: raise FileNotFoundError(
            "Folder cache jest pusty. Uruchom najpierw 'rag_data_preprocessor.py'.")

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)
        docs = text_splitter.split_documents(documents)
        print(f"Budowanie bazy wektorowej z {len(docs)} fragmentÃ³w...");
        print(f"Building vector store from {len(docs)} chunks...")

        Chroma.from_documents(documents=docs, embedding=EMBEDDINGS, persist_directory=vector_db_path)
        log_entry["status"] = "Success"
        print(f"Baza wektorowa zostaÅ‚a pomyÅ›lnie zbudowana w: {vector_db_path}")

    except Exception as e:
        log_entry["status"] = "Failed";
        log_entry["error_message"] = str(e)
        print(f"KRYTYCZNY BÅÄ„D podczas budowania bazy wektorowej: {e}")

    log_entry["duration_seconds"] = round(time.time() - start_time, 2)
    session_log_ref["database_build_event"] = log_entry


def ask_assistant(query: str, session_log_ref):
    """
    Zadaje pytanie do istniejÄ…cej bazy wektorowej i zwraca odpowiedÅº.
    Asks a question to the existing vector store and returns the answer.
    """
    if not CONFIG: return

    start_time = time.time()
    log_entry = {"query": query}

    try:
        vector_db_path = CONFIG['VECTOR_DB_FOLDER']
        if not os.path.exists(vector_db_path): raise FileNotFoundError("Baza wektorowa nie istnieje.")

        vector_db = Chroma(persist_directory=vector_db_path, embedding_function=EMBEDDINGS)
        retriever = vector_db.as_retriever(search_kwargs={'k': 15})

        template = """JesteÅ› ekspertem prawnym specjalizujÄ…cym siÄ™ w analizie dokumentÃ³w. Odpowiedz na pytanie uÅ¼ytkownika precyzyjnie i wyÅ‚Ä…cznie na podstawie dostarczonego poniÅ¼ej kontekstu. JeÅ›li w kontekÅ›cie brakuje odpowiedzi, poinformuj o tym jasno. Cytuj kluczowe fakty.

KONTEKST Z TWOJEJ BAZY WIEDZY:
{context}

PYTANIE UÅ»YTKOWNIKA:
{question}

PRECYZYJNA ODPOWIEDÅ¹:"""
        prompt = PromptTemplate.from_template(template)

        rag_chain = (
                {"context": retriever, "question": RunnablePassthrough()}
                | prompt
                | LLM
                | StrOutputParser()
        )

        print("\n=======================================================")
        print(f"PYTANIE: {query}");
        print("---")

        response = rag_chain.invoke(query)
        print(f"ODPOWIEDÅ¹ ASYSTENTA:\n{response}")

        log_entry["response"] = response;
        log_entry["status"] = "Success"

    except Exception as e:
        error_message = f"BÅÄ„D podczas przetwarzania zapytania: {e}";
        print(error_message)
        log_entry["response"] = error_message;
        log_entry["status"] = "Failed"

    log_entry["duration_seconds"] = round(time.time() - start_time, 2)
    session_log_ref["queries_asked"].append(log_entry)
    print("=======================================================\n")


# --- Main Execution ---
# --- GÅ‚Ã³wne wykonanie ---
if __name__ == "__main__":
    """
    GÅ‚Ã³wny blok wykonawczy skryptu. Pozwala na budowÄ™ bazy lub zadawanie pytaÅ„.
    Main execution block of the script. Allows building the database or asking questions.
    """
    session_log_data = {"session_start_utc": datetime.utcnow().isoformat(), "database_build_event": None,
                        "queries_asked": []}

    try:
        # === KROK 1: Budowanie bazy wektorowej (jednorazowo) ===
        # === STEP 1: Building the vector store (one-time) ===
        #build_vector_store_from_cache(session_log_data)

        # === KROK 2: Zadawanie pytaÅ„ ===
        # === STEP 2: Asking questions ===
        if CONFIG and os.path.exists(CONFIG["VECTOR_DB_FOLDER"]):
            ask_assistant(
                "Czy posiadasz w bazie KonstytucjÄ™ RP?",
                session_log_data)
            ask_assistant("Jesli tak - znajdÅº artykuÅ‚ 13 i opisz mi jego znaczenie",
                          session_log_data)
        else:
            print(
                "\nBaza wektorowa nie istnieje. Uruchom najpierw `rag_data_preprocessor.py`, a nastÄ™pnie odkomentuj funkcjÄ™ `build_vector_store_from_cache()` w tym skrypcie i uruchom go.")
            print(
                "Vector store does not exist. First run `rag_data_preprocessor.py`, then uncomment the `build_vector_store_from_cache()` function in this script and run it.")

    finally:
        # Zapisz log caÅ‚ej sesji na koniec
        # Save the entire session log at the end
        save_session_log(CONFIG['LOG_FOLDER'], session_log_data)
--- END FILE: RAG/rag_query_assistant_test.py ---

--- START FILE: scripts/gemini_api_connection_test.py ---
import uno
import unohelper
import json
import http.client  # For making HTTP requests
import yaml

with open("config.yaml", "r") as cr:
    config_vals = yaml.full_load(cr)
KEY = config_vals['KEY']

# Main macro function to test Google Gemini API connection
# GÅ‚Ã³wna funkcja makra do testowania poÅ‚Ä…czenia z API Google Gemini
def test_gemini_connection(*args):
    # --- GEMINI API KEY CONFIGURATION ---
    # --- KONFIGURACJA KLUCZA API GEMINI ---
    # IMPORTANT: Replace "*****" with your real API key in config.yaml!
    # WAÅ»NE: ZastÄ…p "*****" swoim prawdziwym kluczem API w pliku config.yaml!
    GOOGLE_API_KEY = KEY
    print("Gemini API configured successfully.")
    print("Gemini API skonfigurowane pomyÅ›lnie.")

    API_HOST = "generativelanguage.googleapis.com"
    MODEL_NAME = "gemini-1.5-flash-8b"  # Using a simple model for testing
    # UÅ¼ywamy prostego modelu do testu

    # A very simple prompt to test the connection
    # Bardzo prosty prompt do przetestowania poÅ‚Ä…czenia
    test_prompt = "Say 'OK'." # English version for the model
    test_prompt_pl = "Powiedz 'OK'." # Polish version for clarity in context

    request_body = {
        "contents": [
            {
                "parts": [
                    {"text": test_prompt_pl} # Use the Polish prompt for the model
                ]
            }
        ],
        "generationConfig": {
            "temperature": 0.0,  # Low temperature for predictable output
            # Niska temperatura dla przewidywalnego wyniku
            "maxOutputTokens": 50,  # Short response
            # KrÃ³tka odpowiedÅº
        }
    }

    try:
        conn = http.client.HTTPSConnection(API_HOST)
        headers = {
            'Content-Type': 'application/json',
            'x-goog-api-key': GOOGLE_API_KEY
        }

        endpoint = f"/v1beta/models/{MODEL_NAME}:generateContent"
        body = json.dumps(request_body)

        print(f"Testing connection to Google Gemini API at {API_HOST}{endpoint}...")
        print(f"TestujÄ™ poÅ‚Ä…czenie z Google Gemini API na {API_HOST}{endpoint}...")

        conn.request("POST", endpoint, body=body, headers=headers)
        response = conn.getresponse()
        response_data = response.read().decode('utf-8')
        conn.close()

        print(f"Gemini API Response Status: {response.status}")
        print(f"Status odpowiedzi API Gemini: {response.status}")
        print(f"Full Gemini API Response (for diagnostics): {response_data}")
        print(f"PeÅ‚na odpowiedÅº API Gemini (do diagnostyki): {response_data}")

        if response.status == 200:
            result = json.loads(response_data)
            # Check if the response contains the expected text
            # Sprawdzamy, czy odpowiedÅº zawiera oczekiwany tekst
            if 'candidates' in result and len(result['candidates']) > 0 \
                    and 'content' in result['candidates'][0] \
                    and 'parts' in result['candidates'][0]['content'] \
                    and len(result['candidates'][0]['content']['parts']) > 0 \
                    and 'text' in result['candidates'][0]['content']['parts'][0]:

                model_response = result['candidates'][0]['content']['parts'][0]['text'].strip()
                if "OK" in model_response.upper():  # Check if the model returned "OK"
                    # Sprawdzamy, czy model zwrÃ³ciÅ‚ "OK"
                    print("SUCCESS: Connection to Google Gemini API successful! Model responded: " + model_response)
                    print("SUKCES: PoÅ‚Ä…czenie z Google Gemini API zakoÅ„czone sukcesem! Model odpowiedziaÅ‚: " + model_response)
                else:
                    print(f"WARNING: Google Gemini API connection successful, but model did not return 'OK'. Response: {model_response}")
                    print(f"OSTRZEÅ»ENIE: PoÅ‚Ä…czenie z Google Gemini API zakoÅ„czone sukcesem, ale model nie zwrÃ³ciÅ‚ 'OK'. OdpowiedÅº: {model_response}")
            else:
                print(f"ERROR: Successful connection, but invalid Gemini response structure. Status: {response.status}")
                print(f"BÅÄ„D: PomyÅ›lne poÅ‚Ä…czenie, ale nieprawidÅ‚owa struktura odpowiedzi Gemini. Status: {response.status}")

        elif response.status == 400:  # Bad Request (e.g., wrong API key, bad request format)
            # Bad Request (np. bÅ‚Ä™dny klucz API, zÅ‚y format zapytania)
            error_details = json.loads(response_data).get('error', {}).get('message', 'No details available.')
            # Brak szczegÃ³Å‚Ã³w.
            print(f"ERROR: Gemini API Error (400 - Bad Request). Check API key and request format. Details: {error_details}")
            print(f"BÅÄ„D: BÅ‚Ä…d API Gemini (400 - Bad Request). SprawdÅº klucz API i format zapytania. SzczegÃ³Å‚y: {error_details}")
        elif response.status == 403:  # Forbidden (e.g., API key lacks permissions or API is not enabled)
            # Forbidden (np. klucz API nie ma uprawnieÅ„ lub API nie jest wÅ‚Ä…czone)
            print(f"ERROR: Gemini API Error (403 - Forbidden). API key might lack permissions or API is not enabled. Response: {response_data}")
            print(f"BÅÄ„D: BÅ‚Ä…d API Gemini (403 - Forbidden). Klucz API moÅ¼e nie mieÄ‡ uprawnieÅ„ lub API nie jest wÅ‚Ä…czone. OdpowiedÅº: {response_data}")
        elif response.status == 404:  # Not Found (e.g., wrong URL, model does not exist)
            # Not Found (np. zÅ‚y URL, model nie istnieje)
            print(f"ERROR: Gemini API Error (404 - Not Found). Check model name ({MODEL_NAME}) or API address. Response: {response_data}")
            print(f"BÅÄ„D: BÅ‚Ä…d API Gemini (404 - Not Found). SprawdÅº nazwÄ™ modelu ({MODEL_NAME}) lub adres API. OdpowiedÅº: {response_data}")
        else:
            print(f"ERROR: Google Gemini server error: HTTP {response.status} - {response_data}")
            print(f"BÅÄ„D: BÅ‚Ä…d z serwera Google Gemini: HTTP {response.status} - {response_data}")

    except Exception as e:
        print(f"CRITICAL ERROR: During communication with Google Gemini: {e}")
        print(f"KRYTYCZNY BÅÄ„D: Podczas komunikacji z Google Gemini: {e}")


# Register the macro for LibreOffice
# Rejestracja makra dla LibreOffice
g_exportedScripts = test_gemini_connection,
--- END FILE: scripts/gemini_api_connection_test.py ---

--- START FILE: scripts/upload_folders-google_drive.py ---
import os
import yaml
from datetime import datetime
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive


def upload_folder_to_drive(drive, local_path, parent_folder_id, folder_name):
    """
    # Uploads a local folder and its contents to Google Drive.
    # PrzesyÅ‚a lokalny folder i jego zawartoÅ›Ä‡ na Google Drive.
    """
    # Create the folder on Google Drive
    # UtwÃ³rz folder na Google Drive
    folder_metadata = {
        'title': folder_name,
        'mimeType': 'application/vnd.google-apps.folder',
        'parents': [{'id': parent_folder_id}]
    }
    folder = drive.CreateFile(folder_metadata)
    folder.Upload()
    print(f"Created folder on Google Drive: '{folder_name}' (ID: {folder['id']})")
    print(f"Utworzono folder na Google Drive: '{folder_name}' (ID: {folder['id']})")

    # Upload files and subfolders
    # PrzeÅ›lij pliki i podfoldery
    for item_name in os.listdir(local_path):
        item_path = os.path.join(local_path, item_name)
        if os.path.isfile(item_path):
            # Upload file
            # PrzeÅ›lij plik
            file_metadata = {
                'title': item_name,
                'parents': [{'id': folder['id']}]
            }
            gfile = drive.CreateFile(file_metadata)
            gfile.SetContentFile(item_path)
            gfile.Upload()
            print(f"  Uploaded file: '{item_name}'")
            print(f"  PrzesÅ‚ano plik: '{item_name}'")
        elif os.path.isdir(item_path):
            # Recursively upload subfolder
            # Rekurencyjnie przeÅ›lij podfolder
            print(f"  Uploading subfolder: '{item_name}'...")
            print(f"  PrzesyÅ‚am podfolder: '{item_name}'...")
            upload_folder_to_drive(drive, item_path, folder['id'], item_name)


def main():
    # Load configuration
    # Wczytaj konfiguracjÄ™
    try:
        with open('config.yaml', 'r') as f:
            config = yaml.safe_load(f)
    except FileNotFoundError:
        print("Error: 'config.yaml' file not found. Make sure it exists in the same directory as the script.")
        print("BÅ‚Ä…d: Plik 'config.yaml' nie znaleziony. Upewnij siÄ™, Å¼e istnieje w tym samym katalogu co skrypt.")
        return
    except yaml.YAMLError as e:
        print(f"Error parsing 'config.yaml' file: {e}")
        print(f"BÅ‚Ä…d podczas parsowania pliku 'config.yaml': {e}")
        return

    folders_to_check = config.get('folders_to_check', [])
    base_path = config.get('base_path', os.getcwd())
    google_drive_parent_folder_id = config.get('google_drive_parent_folder_id')

    if not google_drive_parent_folder_id:
        print("Error: 'google_drive_parent_folder_id' not found in config.yaml.")
        print("BÅ‚Ä…d: 'google_drive_parent_folder_id' nie zostaÅ‚ znaleziony w pliku config.yaml.")
        return

    # Authenticate with Google Drive
    # Uwierzytelnij siÄ™ z Google Drive
    gauth = GoogleAuth()
    # Try to load saved credentials
    # SprÃ³buj zaÅ‚adowaÄ‡ zapisane poÅ›wiadczenia
    gauth.LoadCredentialsFile("credentials.json")
    if gauth.credentials is None:
        # Authenticate if no credentials found
        # Uwierzytelnij, jeÅ›li nie znaleziono poÅ›wiadczeÅ„
        gauth.LocalWebserverAuth()
    elif gauth.access_token_expired:
        # Refresh token if expired
        # OdÅ›wieÅ¼ token, jeÅ›li wygasÅ‚
        gauth.Refresh()
    else:
        # Initialize the saved creds
        # Zainicjuj zapisane poÅ›wiadczenia
        gauth.Authorize()
    gauth.SaveCredentialsFile(
        "credentials.json")  # Save credentials for next run / Zapisz poÅ›wiadczenia do nastÄ™pnego uruchomienia

    drive = GoogleDrive(gauth)

    # Get current date for the target folder name
    # Pobierz aktualnÄ… datÄ™ dla nazwy folderu docelowego
    current_date_folder_name = datetime.now().strftime("%Y-%m-%d")

    # Create the main date folder on Google Drive
    # UtwÃ³rz gÅ‚Ã³wny folder z datÄ… na Google Drive
    date_folder_metadata = {
        'title': current_date_folder_name,
        'mimeType': 'application/vnd.google-apps.folder',
        'parents': [{'id': google_drive_parent_folder_id}]
    }
    date_folder = drive.CreateFile(date_folder_metadata)
    date_folder.Upload()
    print(f"\nMain folder created on Google Drive: '{current_date_folder_name}' (ID: {date_folder['id']})")
    print(f"\nUtworzono gÅ‚Ã³wny folder na Google Drive: '{current_date_folder_name}' (ID: {date_folder['id']})")

    # Check and upload folders
    # SprawdÅº i przeÅ›lij foldery
    for folder_name in folders_to_check:
        full_path = os.path.join(base_path, folder_name)
        print(f"\nChecking folder: '{full_path}'")
        print(f"\nSprawdzam folder: '{full_path}'")

        if not os.path.exists(full_path):
            print(f"  Folder does not exist. Skipping.")
            print(f"  Folder nie istnieje. Pomijam.")
            continue

        if not os.path.isdir(full_path):
            print(f"  This is not a directory. Skipping.")
            print(f"  To nie jest katalog. Pomijam.")
            continue

        if not os.listdir(full_path):
            print(f"  Folder is empty. Skipping.")
            print(f"  Folder jest pusty. Pomijam.")
        else:
            print(f"  Folder is not empty. Starting upload...")
            print(f"  Folder nie jest pusty. Rozpoczynam przesyÅ‚anie...")
            try:
                upload_folder_to_drive(drive, full_path, date_folder['id'], folder_name)
                print(f"  Finished uploading folder '{folder_name}'.")
                print(f"  ZakoÅ„czono przesyÅ‚anie folderu '{folder_name}'.")
            except Exception as e:
                print(f"  An error occurred while uploading folder '{folder_name}': {e}")
                print(f"  WystÄ…piÅ‚ bÅ‚Ä…d podczas przesyÅ‚ania folderu '{folder_name}': {e}")

    print("\nProcess finished.")
    print("\nProces zakoÅ„czony.")


if __name__ == '__main__':
    main()
--- END FILE: scripts/upload_folders-google_drive.py ---

--- START FILE: scripts/google_drive_auth.py ---
from pydrive2.auth import GoogleAuth

gauth = GoogleAuth()
gauth.LocalWebserverAuth() # This will open a browser for authentication / To otworzy przeglÄ…darkÄ™ do uwierzytelnienia
print("Authentication successful!") # Uwierzytelnienie zakoÅ„czone sukcesem!
--- END FILE: scripts/google_drive_auth.py ---

--- START FILE: scripts/audio-text.py ---
import os
import google.generativeai as genai
import datetime
import time
import json  # Added for JSON log saving / Dodane do zapisu logÃ³w w formacie JSON
import yaml

with open("config.yaml", "r") as cr:
    config_vals = yaml.full_load(cr)
KEY = config_vals['KEY']

# --- PATH CONFIGURATION ---
# --- KONFIGURACJA ÅšCIEÅ»EK ---
# Folder for input audio files.
# Folder na wejÅ›ciowe pliki audio.
AUDIO_INPUT_FOLDER = os.path.join(os.path.dirname(__file__), "audio_input")
# Folder for output transcription and analysis results.
# Folder na wyniki transkrypcji i analizy.
AUDIO_OUTPUT_FOLDER = os.path.join(os.path.dirname(__file__), "audio_output")
# Folder for logs of audio processing.
# Folder na logi przetwarzania audio.
AUDIO_LOG_FOLDER = os.path.join(os.path.dirname(__file__), "audio_logs")

# Create directories if they don't exist.
# Tworzy katalogi, jeÅ›li nie istniejÄ….
os.makedirs(AUDIO_INPUT_FOLDER, exist_ok=True)
os.makedirs(AUDIO_OUTPUT_FOLDER, exist_ok=True)
os.makedirs(AUDIO_LOG_FOLDER, exist_ok=True)

# --- GEMINI API KEY CONFIGURATION ---
# --- KONFIGURACJA KLUCZA API GEMINI ---
# IMPORTANT: Replace "*****" with your real API key in config.yaml!
# WAÅ»NE: ZastÄ…p "*****" swoim prawdziwym kluczem API w pliku config.yaml!
GEMINI_API_KEY = KEY
# Configure the Gemini API with the provided key.
# Konfiguruje API Gemini za pomocÄ… podanego klucza.
genai.configure(api_key=GEMINI_API_KEY)
print("Gemini API configured successfully.")
print("Gemini API skonfigurowane pomyÅ›lnie.")


# --- Functions for Audio File Handling and Gemini Audio-to-Text ---
# --- Funkcje do obsÅ‚ugi plikÃ³w audio i Gemini Audio-to-Text ---

def upload_audio_to_gemini_files_api(audio_file_path):
    # Print message indicating file upload.
    # WyÅ›wietla komunikat o przesyÅ‚aniu pliku.
    print(f"Uploading audio file '{os.path.basename(audio_file_path)}' to Gemini Files API...")
    print(f"PrzesyÅ‚anie pliku audio '{os.path.basename(audio_file_path)}' do Gemini Files API...")
    try:
        # Upload the file using genai.upload_file.
        # PrzesyÅ‚a plik za pomocÄ… genai.upload_file.
        file = genai.upload_file(path=audio_file_path)
        print(f"File '{os.path.basename(audio_file_path)}' uploaded. Gemini File Name: {file.name}")
        print(f"Plik '{os.path.basename(audio_file_path)}' przesÅ‚any. Nazwa pliku Gemini: {file.name}")
        return file
    except Exception as e:
        # Error message if upload fails.
        # Komunikat o bÅ‚Ä™dzie, jeÅ›li przesÅ‚anie pliku siÄ™ nie powiedzie.
        print(f"ERROR: Could not upload file '{audio_file_path}' to Gemini Files API: {e}")
        print(f"BÅÄ„D: Nie moÅ¼na przesÅ‚aÄ‡ pliku '{audio_file_path}' do Gemini Files API: {e}")
        return None


def transcribe_audio_with_gemini_model(audio_file_object, model_name, log_data):
    # Print message indicating audio transcription.
    # WyÅ›wietla komunikat o transkrypcji audio.
    print(f"Transcribing audio using Gemini model '{model_name}'...")
    print(f"Transkrypcja audio za pomocÄ… modelu Gemini '{model_name}'...")

    # Initialize the GenerativeModel.
    # Inicjuje model GenerativeModel.
    model = genai.GenerativeModel(model_name=model_name)

    # Prompt for transcription and stylistic/grammatical correction in Polish.
    # Prompt do transkrypcji i poprawy stylistycznej/gramatycznej w jÄ™zyku polskim.
    prompt_parts = [
        "Transcribe the audio recording into text. Return the text stylistically and grammatically corrected in Polish."
        "Przetranskrybuj nagranie audio na tekst. ZwrÃ³Ä‡ tekst poprawiony stylistycznie i gramatycznie w jÄ™zyku polskim."
    ]

    # Count tokens for input (audio file and prompt).
    # Liczy tokeny dla inputu (pliku audio i promptu).
    try:
        # Note: count_tokens accepts a list of objects, including the audio_file_object and prompt part.
        # Uwaga: count_tokens przyjmuje listÄ™ obiektÃ³w, w tym audio_file_object i czÄ™Å›Ä‡ promptu.
        count_response = model.count_tokens([audio_file_object, prompt_parts[0]])
        input_token_count = count_response.total_tokens
        print(f"Input token count: {input_token_count}")
        print(f"Liczba tokenÃ³w wejÅ›ciowych: {input_token_count}")
        log_data["input_tokens"] = input_token_count
    except Exception as e:
        # Warning if input tokens cannot be counted.
        # OstrzeÅ¼enie, jeÅ›li nie moÅ¼na policzyÄ‡ tokenÃ³w wejÅ›ciowych.
        print(f"WARNING: Could not count input tokens: {e}")
        print(f"OSTRZEÅ»ENIE: Nie moÅ¼na policzyÄ‡ tokenÃ³w wejÅ›ciowych: {e}")
        log_data["input_tokens"] = "ERROR"

    response_text = None
    try:
        # Pass the audio and prompt to generate_content.
        # Przekazuje audio i prompt do generate_content.
        response = model.generate_content([audio_file_object, prompt_parts[0]])
        response_text = response.text.strip()
        # Get output and total token counts from usage metadata.
        # Pobiera liczbÄ™ tokenÃ³w wyjÅ›ciowych i caÅ‚kowitÄ… z metadanych uÅ¼ycia.
        log_data["output_tokens"] = response.usage_metadata.candidates_token_count if hasattr(response.usage_metadata,
                                                                                              'candidates_token_count') else "N/A"
        log_data["total_tokens"] = response.usage_metadata.total_token_count if hasattr(response.usage_metadata,
                                                                                        'total_token_count') else "N/A"
        print(f"Output token count: {log_data['output_tokens']}")
        print(f"Liczba tokenÃ³w wyjÅ›ciowych: {log_data['output_tokens']}")
        print(f"Total tokens (input+output): {log_data['total_tokens']}")
        print(f"ÅÄ…czna liczba tokenÃ³w (wejÅ›cie+wyjÅ›cie): {log_data['total_tokens']}")
        return response_text
    except Exception as e:
        # Error message if audio transcription/analysis fails.
        # Komunikat o bÅ‚Ä™dzie, jeÅ›li transkrypcja/analiza audio siÄ™ nie powiedzie.
        print(f"ERROR: Could not transcribe/analyze audio with Gemini: {e}")
        print(f"BÅÄ„D: Nie moÅ¼na przetranskrybowaÄ‡/analizowaÄ‡ audio z Gemini: {e}")
        log_data["error"] = str(e)
        return None


def delete_gemini_file(file_object):
    # Only proceed if file_object exists.
    # Kontynuuje tylko jeÅ›li istnieje file_object.
    if file_object:
        # Print message indicating file deletion.
        # WyÅ›wietla komunikat o usuwaniu pliku.
        print(f"Deleting file '{file_object.name}' from Gemini Files API...")
        print(f"Usuwanie pliku '{file_object.name}' z Gemini Files API...")
        try:
            # Delete the file from Gemini Files API.
            # Usuwa plik z Gemini Files API.
            genai.delete_file(file_object.name)
            print("File deleted successfully.")
            print("Plik usuniÄ™ty pomyÅ›lnie.")
        except Exception as e:
            # Error message if deletion fails.
            # Komunikat o bÅ‚Ä™dzie, jeÅ›li usuniÄ™cie siÄ™ nie powiedzie.
            print(f"ERROR: Could not delete file '{file_object.name}' from Gemini Files API: {e}")
            print(f"BÅÄ„D: Nie moÅ¼na usunÄ…Ä‡ pliku '{file_object.name}' z Gemini Files API: {e}")


def get_transcriber_function(transcriber_type="gemini_native"):
    # Check if the transcriber type is 'gemini_native'.
    # Sprawdza, czy typ transkrypcji to 'gemini_native'.
    if transcriber_type == "gemini_native":
        # Manually set the model to a more stable and proven one for audio.
        # RÄ™czne ustawienie modelu na bardziej stabilny i sprawdzony dla audio.
        supported_audio_model = "gemini-1.5-flash"

        print(f"Audio transcription model set to: {supported_audio_model}")
        print(f"Ustawiono model do transkrypcji audio na: {supported_audio_model}")

        # Define the inner function for Gemini native audio transcription.
        # Definiuje wewnÄ™trznÄ… funkcjÄ™ do natywnej transkrypcji audio Gemini.
        def _transcribe_and_analyze_with_gemini_native(audio_file_path, log_data):
            # Upload the audio file to Gemini Files API.
            # PrzesyÅ‚a plik audio do Gemini Files API.
            file_obj = upload_audio_to_gemini_files_api(audio_file_path)
            if file_obj:
                log_data["gemini_file_name"] = file_obj.name
                try:
                    # Transcribe and analyze the audio using the Gemini model.
                    # Transkrybuje i analizuje audio za pomocÄ… modelu Gemini.
                    analysis_text = transcribe_audio_with_gemini_model(file_obj, model_name=supported_audio_model,
                                                                       log_data=log_data)
                    return analysis_text
                finally:
                    # Ensure the file is deleted from Gemini Files API after processing.
                    # Upewnia siÄ™, Å¼e plik zostanie usuniÄ™ty z Gemini Files API po przetworzeniu.
                    delete_gemini_file(file_obj)
            return None

        return _transcribe_and_analyze_with_gemini_native
    else:
        # Raise an error for an unknown transcriber type.
        # Podnosi bÅ‚Ä…d dla nieznanego typu transkrypcji.
        raise ValueError(f"Unknown transcription type: {transcriber_type}. Please choose 'gemini_native'.")
        raise ValueError(f"Nieznany typ transkrypcji: {transcriber_type}. ProszÄ™ wybraÄ‡ 'gemini_native'.")


def process_audio_folder():
    print(f"\n--- Starting scan and processing of audio files from folder: {AUDIO_INPUT_FOLDER} ---")
    print(f"\n--- Rozpoczynam skanowanie i przetwarzanie plikÃ³w audio z folderu: {AUDIO_INPUT_FOLDER} ---")

    # Supported audio file extensions.
    # ObsÅ‚ugiwane rozszerzenia plikÃ³w audio.
    audio_files = [f for f in os.listdir(AUDIO_INPUT_FOLDER)
                   if f.lower().endswith((".wav", ".mp3", ".m4a", ".flac"))]

    if not audio_files:
        print("INFO: No audio files found in the folder for analysis.")
        print("INFO: Brak plikÃ³w audio w folderze do analizy.")
        return

    # Get the transcriber function.
    # Pobiera funkcjÄ™ do transkrypcji.
    transcribe_and_analyze_function = get_transcriber_function("gemini_native")

    total_files_processed = 0
    total_tokens_used = 0
    total_processing_time = 0.0

    # Global list to collect logs for all files.
    # Globalna lista do zbierania logÃ³w dla wszystkich plikÃ³w.
    overall_logs = []

    for audio_file in audio_files:
        audio_path = os.path.join(AUDIO_INPUT_FOLDER, audio_file)
        base_name = os.path.splitext(audio_file)[0]
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        output_analysis_path = os.path.join(AUDIO_OUTPUT_FOLDER, f"{base_name}_analysis_{timestamp}.txt")
        log_file_path = os.path.join(AUDIO_LOG_FOLDER, f"{base_name}_log_{timestamp}.json")

        # Dictionary for logs of the current file.
        # SÅ‚ownik na logi dla bieÅ¼Ä…cego pliku.
        current_log_data = {
            "timestamp": timestamp,
            "audio_file": audio_file,
            "status": "processing",
            "start_time_utc": datetime.datetime.utcnow().isoformat()
        }

        print(f"\n--- Processing file: {audio_file} ---")
        print(f"\n--- Przetwarzanie pliku: {audio_file} ---")
        file_start_time = time.time()

        analysis_result_text = None
        try:
            # Transcribe and analyze the audio file.
            # Transkrybuje i analizuje plik audio.
            analysis_result_text = transcribe_and_analyze_function(audio_path, current_log_data)

            if analysis_result_text:
                print(f"SUCCESS: Analysis of file '{audio_file}' completed.")
                print(f"SUKCES: Analiza pliku '{audio_file}' zakoÅ„czona.")
                with open(output_analysis_path, "w", encoding="utf-8") as f:
                    f.write(analysis_result_text)
                print(f"Analysis result saved to: {output_analysis_path}")
                print(f"Wynik analizy zapisano do: {output_analysis_path}")
                current_log_data["status"] = "SUCCESS"
            else:
                print(f"WARNING: No analysis result for file '{audio_file}'.")
                print(f"OSTRZEÅ»ENIE: Brak wyniku analizy dla pliku '{audio_file}'.")
                with open(output_analysis_path, "w", encoding="utf-8") as f:
                    f.write("[NO ANALYSIS RESULT FROM GEMINI]")
                    f.write("[BRAK WYNIKU ANALIZY Z GEMINI]")
                current_log_data["status"] = "WARNING_NO_RESULT"
        except Exception as e:
            print(f"ERROR: An error occurred while processing '{audio_file}': {e}")
            print(f"BÅÄ„D: WystÄ…piÅ‚ bÅ‚Ä…d podczas przetwarzania '{audio_file}': {e}")
            with open(output_analysis_path, "w", encoding="utf-8") as f:
                f.write(f"[PROCESSING ERROR: {e}]")
                f.write(f"[BÅÄ„D PRZETWARZANIA: {e}]")
            current_log_data["status"] = "ERROR"
            current_log_data["exception_details"] = str(e)

        file_end_time = time.time()
        duration = file_end_time - file_start_time
        print(f"Processing time for '{audio_file}': {duration:.2f} seconds.")
        print(f"Czas przetwarzania dla '{audio_file}': {duration:.2f} sekundy.")

        current_log_data["duration_seconds"] = round(duration, 2)
        current_log_data["end_time_utc"] = datetime.datetime.utcnow().isoformat()

        # Save logs for the current file to a JSON file.
        # Zapisuje logi dla bieÅ¼Ä…cego pliku do pliku JSON.
        with open(log_file_path, "w", encoding="utf-8") as log_f:
            json.dump(current_log_data, log_f, indent=4, ensure_ascii=False)
        print(f"Logs for file '{audio_file}' saved to: {log_file_path}")
        print(f"Logi dla pliku '{audio_file}' zapisano do: {log_file_path}")

        overall_logs.append(current_log_data)

        total_files_processed += 1
        total_processing_time += duration
        # Check if total_tokens is an integer before adding.
        # Sprawdza, czy total_tokens jest liczbÄ… caÅ‚kowitÄ… przed dodaniem.
        if "total_tokens" in current_log_data and isinstance(current_log_data["total_tokens"], int):
            total_tokens_used += current_log_data["total_tokens"]

    print(f"\n--- Finished processing all audio files. ---")
    print(f"\n--- ZakoÅ„czono przetwarzanie wszystkich plikÃ³w audio. ---")
    print(f"Total files processed: {total_files_processed}")
    print(f"ÅÄ…cznie przetworzono plikÃ³w: {total_files_processed}")
    print(f"Total execution time: {total_processing_time:.2f} seconds.")
    print(f"CaÅ‚kowity czas dziaÅ‚ania: {total_processing_time:.2f} sekundy.")
    print(f"Total token usage for all files: {total_tokens_used} tokens.")
    print(f"CaÅ‚kowite zuÅ¼ycie tokenÃ³w dla wszystkich plikÃ³w: {total_tokens_used} tokenÃ³w.")

    # Optionally: Save a summary log for the entire session.
    # Opcjonalnie: Zapisz sumaryczny log dla caÅ‚ej sesji.
    summary_log_path = os.path.join(AUDIO_LOG_FOLDER,
                                    f"summary_log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    summary_data = {
        "overall_summary": {
            "total_files_processed": total_files_processed,
            "total_processing_time_seconds": round(total_processing_time, 2),
            "total_tokens_used": total_tokens_used,
            "session_start_time_utc": overall_logs[0]["start_time_utc"] if overall_logs else "N/A",
            "session_end_time_utc": overall_logs[-1]["end_time_utc"] if overall_logs else "N/A"
        },
        "file_details": overall_logs
    }
    with open(summary_log_path, "w", encoding="utf-8") as sum_f:
        json.dump(summary_data, sum_f, indent=4, ensure_ascii=False)
    print(f"Session summary logs saved to: {summary_log_path}")
    print(f"Sumaryczne logi sesji zapisano do: {summary_log_path}")


if __name__ == "__main__":
    # Call the main function to process the audio folder.
    # WywoÅ‚uje gÅ‚Ã³wnÄ… funkcjÄ™ do przetwarzania folderu z audio.
    process_audio_folder()
    print("\n--- Script audio-text.py execution finished. ---")
    print("\n--- ZakoÅ„czono dziaÅ‚anie skryptu audio-text.py ---")
--- END FILE: scripts/audio-text.py ---

--- START FILE: scripts/test.py ---
# --- Importing necessary libraries ---
# --- Importowanie niezbÄ™dnych bibliotek ---
# For filesystem operations like creating paths and folders. / Do operacji na systemie plikÃ³w, jak tworzenie Å›cieÅ¼ek i folderÃ³w.
import os
# For opening and extracting text from PDF files. / Do otwierania i wyciÄ…gania tekstu z plikÃ³w PDF.
import pdfplumber
# For generating unique timestamps for filenames. / Do generowania unikalnych znacznikÃ³w czasu dla nazw plikÃ³w.
import datetime
# For loading configuration files in YAML format. / Do wczytywania plikÃ³w konfiguracyjnych w formacie YAML.
import yaml
# For system interaction, e.g., to exit the script. / Do interakcji z systemem, np. do przerwania dziaÅ‚ania skryptu.
import sys
# For using regular expressions to fix line wrapping. / Do uÅ¼ywania wyraÅ¼eÅ„ regularnych w celu naprawy zawijania wierszy.
import re
# For structured logging in JSON format. / Do strukturalnego logowania w formacie JSON.
import json
# For timing operations and pausing the script. / Do mierzenia czasu operacji i pauzowania skryptu.
import time
# The official Google library for interacting with the Gemini API. / Oficjalna biblioteka Google do interakcji z API Gemini.
import google.generativeai as genai
# For handling specific API errors like rate limiting. / Do obsÅ‚ugi specyficznych bÅ‚Ä™dÃ³w API, takich jak limity zapytaÅ„.
from google.api_core import exceptions

# --- ReportLab imports for advanced PDF creation with paragraphs ---
# --- Importy ReportLab do zaawansowanego tworzenia PDF z akapitami ---
from reportlab.lib.pagesizes import A4
from reportlab.platypus import SimpleDocTemplate, Paragraph
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.pdfbase import pdfmetrics
from reportlab.lib.enums import TA_JUSTIFY


# --- MAIN CONFIGURATION LOADING FUNCTION ---
# --- GÅÃ“WNA FUNKCJA ÅADUJÄ„CA KONFIGURACJÄ˜ ---
def load_configuration(config_path='config.yaml'):
    """
    Loads configuration from a YAML file.
    Wczytuje konfiguracjÄ™ z pliku YAML.
    """
    try:
        with open(config_path, "r", encoding="utf-8") as cr:
            config = yaml.full_load(cr)

        if 'base_path' not in config:
            raise ValueError("Key 'base_path' is required.")
        if 'translator_script_config' not in config:
            raise ValueError("Section 'translator_script_config' is required.")
        if 'KEY' not in config or not config['KEY'] or config['KEY'] == "TWOJ_KLUCZ_API_GEMINI" or config[
            'KEY'] == "*****":
            raise ValueError("Gemini API Key ('KEY') is missing or is a placeholder.")

        base_path = config['base_path']
        translator_config = config['translator_script_config']

        conf = {
            "GEMINI_API_KEY": config['KEY'],
            "SOURCE_FOLDER": os.path.join(base_path, translator_config['source_folder']),
            "OUTPUT_FOLDER": os.path.join(base_path, translator_config['output_folder']),
            "LOG_FOLDER": os.path.join(base_path, translator_config.get('log_folder', 'LOGS')),
            "FONT_PATH": os.path.join(base_path, translator_config['font_path']),
            "FONT_NAME": translator_config['font_name'],
            "FONT_SIZE": translator_config.get('font_size', 10),
            "MODEL_NAME": translator_config.get('model_name', 'gemini-1.5-flash'),
            "CHUNK_SIZE_CHARS": translator_config.get('chunk_size_chars', 30000),
            "TARGET_LANGUAGES": translator_config.get('target_languages', ['English', 'Polish', 'Czech'])
        }
        print("Configuration loaded successfully.")
        print("Konfiguracja zaÅ‚adowana pomyÅ›lnie.")
        return conf
    except Exception as e:
        print(f"FATAL ERROR in configuration: {e}")
        print(f"BÅÄ„D KRYTYCZNY w konfiguracji: {e}")
        return None


# --- Initialization ---
# --- Inicjalizacja ---
CONFIG = load_configuration()
MODEL = None
if CONFIG:
    try:
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]
        genai.configure(api_key=CONFIG['GEMINI_API_KEY'])
        MODEL = genai.GenerativeModel(CONFIG['MODEL_NAME'], safety_settings=safety_settings)
        print("Gemini API configured successfully.")
        print("Gemini API skonfigurowane pomyÅ›lnie.")
    except Exception as e:
        CONFIG = None
    if CONFIG:
        os.makedirs(CONFIG['SOURCE_FOLDER'], exist_ok=True)
        os.makedirs(CONFIG['OUTPUT_FOLDER'], exist_ok=True)
        os.makedirs(CONFIG['LOG_FOLDER'], exist_ok=True)
        try:
            pdfmetrics.registerFont(TTFont(CONFIG['FONT_NAME'], CONFIG['FONT_PATH']))
            print(f"Font '{CONFIG['FONT_NAME']}' registered successfully.")
            print(f"Czcionka '{CONFIG['FONT_NAME']}' zarejestrowana pomyÅ›lnie.")
        except Exception as e:
            print(f"ERROR: Could not register font. Defaulting to Helvetica. Error: {e}")
            print(f"BÅÄ„D: Nie moÅ¼na zarejestrowaÄ‡ czcionki. UÅ¼ywam domyÅ›lnej Helvetica. BÅ‚Ä…d: {e}")
            CONFIG['FONT_NAME'] = "Helvetica"
else:
    print("Exiting script due to configuration errors.")
    print("Zamykanie skryptu z powodu bÅ‚Ä™dÃ³w konfiguracji.")
    sys.exit(1)


# --- Core Functions ---
# --- GÅ‚Ã³wne Funkcje ---

def extract_full_text_from_pdf(pdf_path):
    """
    Extracts all text from a PDF file into a single string.
    Ekstrahuje caÅ‚y tekst z pliku PDF do jednego ciÄ…gu znakÃ³w.
    """
    print(f"Extracting full text from '{os.path.basename(pdf_path)}'...")
    print(f"EkstrahujÄ™ peÅ‚ny tekst z '{os.path.basename(pdf_path)}'...")
    full_text = []
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    full_text.append(page_text)
        return "\n\n".join(full_text)
    except Exception as e:
        print(f"ERROR: Could not read PDF file {pdf_path}: {e}")
        print(f"BÅÄ„D: Nie moÅ¼na odczytaÄ‡ pliku PDF {pdf_path}: {e}")
        return None


def chunk_text(text, chunk_size):
    """
    Splits a large text into smaller chunks based on a character size limit.
    Dzieli duÅ¼y tekst na mniejsze kawaÅ‚ki na podstawie limitu znakÃ³w.
    """
    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]


def translate_text_in_chunks(text_to_translate, target_language):
    """
    Translates text, handles rate limits and overloads by retrying, and manages other safety blocks.
    TÅ‚umaczy tekst, obsÅ‚uguje limity i przeciÄ…Å¼enia poprzez ponawianie prÃ³b i zarzÄ…dza innymi blokadami.
    """
    if not text_to_translate or not text_to_translate.strip():
        return "", 0, 0

    chunks = chunk_text(text_to_translate, CONFIG['CHUNK_SIZE_CHARS'])
    print(f"Text divided into {len(chunks)} chunk(s) for translation to {target_language}.")
    print(f"Tekst podzielony na {len(chunks)} czÄ™Å›ci do tÅ‚umaczenia na {target_language}.")

    translated_parts = []
    total_input_tokens = 0
    total_output_tokens = 0

    base_prompt = f"You are a professional translator. Translate the following document fragment into {target_language}. Preserve the original formatting, including paragraph breaks. Return only the translated text, without any additional comments, explanations or introductions."

    for i, chunk in enumerate(chunks):
        print(f"  Translating chunk {i + 1}/{len(chunks)}...")
        print(f"  TÅ‚umaczÄ™ czÄ™Å›Ä‡ {i + 1}/{len(chunks)}...")
        prompt = f"{base_prompt}\n\nFragment to translate:\n\n{chunk}"

        max_retries = 5
        current_retry = 0
        delay = 15

        while current_retry < max_retries:
            try:
                response = MODEL.generate_content(prompt)

                if not response.parts:
                    block_reason = response.prompt_feedback.block_reason.name if response.prompt_feedback else "UNKNOWN"
                    print(
                        f"  WARNING: Chunk {i + 1} translation blocked by API. Reason: {block_reason}. Inserting original text.")
                    print(
                        f"  OSTRZEÅ»ENIE: TÅ‚umaczenie fragmentu {i + 1} zablokowane przez API. PowÃ³d: {block_reason}. Wstawiam oryginalny tekst.")
                    warning_msg = f"[API TRANSLATION BLOCKED (REASON: {block_reason}) - ORIGINAL TEXT INSERTED BELOW]"
                    translated_parts.append(f"\n\n--- {warning_msg} ---\n\n{chunk}\n\n")
                    break

                translated_parts.append(response.text)
                if hasattr(response, 'usage_metadata'):
                    total_input_tokens += response.usage_metadata.prompt_token_count
                    total_output_tokens += response.usage_metadata.candidates_token_count
                break

            except (exceptions.ResourceExhausted, exceptions.ServiceUnavailable, exceptions.DeadlineExceeded) as e:
                current_retry += 1
                error_type = type(e).__name__
                if current_retry >= max_retries:
                    print(f"  ERROR: Max retries exceeded for chunk. Error: {error_type}. Inserting original text.")
                    print(
                        f"  BÅÄ„D: Przekroczono maksymalnÄ… liczbÄ™ prÃ³b dla fragmentu. BÅ‚Ä…d: {error_type}. Wstawiam oryginalny tekst.")
                    translated_parts.append(
                        f"\n\n[TRANSLATION FAILED AFTER RETRIES: {error_type}] - ORIGINAL TEXT INSERTED BELOW\n\n{chunk}\n\n")
                    break

                print(
                    f"  API temporary error ({error_type}). Retrying in {delay} seconds... (Attempt {current_retry}/{max_retries})")
                print(
                    f"  Tymczasowy bÅ‚Ä…d API ({error_type}). Ponawiam prÃ³bÄ™ za {delay} sekund... (PrÃ³ba {current_retry}/{max_retries})")
                time.sleep(delay)
                delay *= 2

            except Exception as e:
                print(f"  An unexpected API error occurred: {e}. Inserting original text.")
                print(f"  WystÄ…piÅ‚ nieoczekiwany bÅ‚Ä…d API: {e}. Wstawiam oryginalny tekst.")
                translated_parts.append(
                    f"\n\n[UNEXPECTED TRANSLATION ERROR: {e}] - ORIGINAL TEXT INSERTED BELOW\n\n{chunk}\n\n")
                break

        time.sleep(1)

    print(f"Token Usage for {target_language}: Input={total_input_tokens}, Output={total_output_tokens}")
    print(f"ZuÅ¼ycie tokenÃ³w dla {target_language}: WejÅ›cie={total_input_tokens}, WyjÅ›cie={total_output_tokens}")
    return "\n\n".join(translated_parts), total_input_tokens, total_output_tokens


def reflow_text(text: str) -> str:
    """
    Intelligently joins lines for better PDF formatting.
    Inteligentnie Å‚Ä…czy linie dla lepszego formatowania PDF.
    """
    reflowed = re.sub(r'(?<!\n)\n(?!\n)', ' ', text)
    reflowed = re.sub(r'\n{2,}', '<br/><br/>', reflowed)
    return reflowed


def save_text_to_pdf(text_content, output_pdf_path):
    """
    Saves the given text content to a PDF file using Platypus for proper text wrapping.
    Zapisuje podany tekst do pliku PDF, uÅ¼ywajÄ…c biblioteki Platypus do poprawnego zawijania tekstu.
    """
    font_name, font_size = CONFIG['FONT_NAME'], CONFIG['FONT_SIZE']
    print(f"Saving PDF to: {output_pdf_path}...")
    print(f"ZapisujÄ™ PDF do: {output_pdf_path}...")

    doc = SimpleDocTemplate(output_pdf_path, pagesize=A4, rightMargin=72, leftMargin=72, topMargin=72, bottomMargin=72)
    styles = getSampleStyleSheet()
    style = ParagraphStyle(name='Normal_Justified', parent=styles['Normal'], fontName=font_name, fontSize=font_size,
                           leading=font_size * 1.5, alignment=TA_JUSTIFY)

    processed_text = reflow_text(text_content)
    story = [Paragraph(processed_text, style)]

    try:
        doc.build(story)
        print(f"Successfully saved PDF: {output_pdf_path}")
        print(f"PomyÅ›lnie zapisano PDF: {output_pdf_path}")
    except Exception as e:
        print(f"ERROR saving PDF: {e}")
        print(f"BÅÄ„D podczas zapisywania PDF: {e}")


# --- Main Execution ---
# --- GÅ‚Ã³wne Wykonanie ---
def main():
    """
    Main function to find, translate, and save PDFs into separate files per language.
    GÅ‚Ã³wna funkcja do znajdowania, tÅ‚umaczenia i zapisywania plikÃ³w PDF w osobnych plikach dla kaÅ¼dego jÄ™zyka.
    """
    print(f"Starting PDF translation from folder: {CONFIG['SOURCE_FOLDER']}")
    print(f"Rozpoczynam tÅ‚umaczenie PDF z folderu: {CONFIG['SOURCE_FOLDER']}")

    pdf_files = sorted([f for f in os.listdir(CONFIG['SOURCE_FOLDER']) if f.lower().endswith(".pdf")])

    if not pdf_files:
        print("INFO: No PDF files found in the source folder for translation.")
        print("INFO: Brak plikÃ³w PDF w folderze ÅºrÃ³dÅ‚owym do tÅ‚umaczenia.")
        return

    for pdf_file in pdf_files:
        print(f"\n--- Processing file: {pdf_file} ---")
        print(f"--- Przetwarzam plik: {pdf_file} ---")

        pdf_path = os.path.join(CONFIG['SOURCE_FOLDER'], pdf_file)
        original_text = extract_full_text_from_pdf(pdf_path)

        if not original_text:
            print(f"WARNING: No text extracted from '{pdf_file}'. Skipping.")
            print(f"OSTRZEÅ»ENIE: Nie wyekstrahowano tekstu z '{pdf_file}'. Pomijam.")
            continue

        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = os.path.splitext(pdf_file)[0]

        # Mapping language names to file suffixes, sorted alphabetically by language name.
        # Mapowanie nazw jÄ™zykÃ³w na przyrostki plikÃ³w, posortowane alfabetycznie wedÅ‚ug nazwy jÄ™zyka.
        lang_suffix_map = {
            "english": "EN",
            "polish": "PL",
            "czech": "CS",
            "ukrainian": "UA",
            "german": "DE",
            "french": "FR",
            "spanish": "ES",
            "russian": "RU",
            "arabic": "AR",
            "chinese": "ZH",
            "hebrew": "HE",
            "japanese": "JA",
            "persian": "FA",
        }

        for lang in CONFIG.get('TARGET_LANGUAGES', ['English', 'Polish']):
            print(f"\n--- Starting translation for {lang} ---")
            print(f"--- Rozpoczynam tÅ‚umaczenie na {lang} ---")

            translated_text, _, _ = translate_text_in_chunks(original_text, lang)

            if translated_text:
                lang_lower = lang.lower()
                lang_suffix = lang_suffix_map.get(lang_lower, lang_lower[:2].upper())
                output_path = os.path.join(CONFIG['OUTPUT_FOLDER'], f"{base_name}_{timestamp}_{lang_suffix}.pdf")

                save_text_to_pdf(translated_text, output_path)
            else:
                print(f"WARNING: Translation for {lang} resulted in empty text. No file will be saved.")
                print(f"OSTRZEÅ»ENIE: TÅ‚umaczenie na {lang} zwrÃ³ciÅ‚o pusty tekst. Plik nie zostanie zapisany.")

            sleep_duration = 20
            print(
                f"--- Finished {lang}. Waiting for {sleep_duration} seconds before next language to manage rate limits... ---")
            print(
                f"--- UkoÅ„czono {lang}. Czekam {sleep_duration} sekund przed kolejnym jÄ™zykiem w celu zarzÄ…dzania limitami... ---")
            time.sleep(sleep_duration)

    print("\n--- Finished translation process for all files. ---")
    print("\n--- ZakoÅ„czono proces tÅ‚umaczenia dla wszystkich plikÃ³w. ---")


if __name__ == "__main__":
    if CONFIG:
        main()
--- END FILE: scripts/test.py ---

--- START FILE: scripts/config.yaml ---
# --- API Key Configuration ---
# --- Konfiguracja Klucza API ---
KEY: "*****" # <-- Wklej tutaj swÃ³j prawdziwy klucz / Paste your real key here

# --- Main Project Configuration ---
# --- GÅ‚Ã³wna Konfiguracja Projektu ---
# The main, absolute path to the project directory. All other paths will be built based on this one.
# GÅ‚Ã³wna, absolutna Å›cieÅ¼ka do katalogu projektu. Wszystkie inne Å›cieÅ¼ki bÄ™dÄ… budowane na jej podstawie.
base_path: "/home/luke_blue_lox/PycharmProjects/BLOX-TAK-GEMINI"

# --- RAG Pipeline Configuration (Updated for 2-DB Architecture) ---
# --- Konfiguracja Potoku RAG (Zaktualizowana dla Architektury 2 Baz) ---
rag_pipeline_config:
  # Source folders for the two types of corpora
  # Foldery ÅºrÃ³dÅ‚owe dla dwÃ³ch typÃ³w korpusÃ³w
  legal_source_folder: "TEMP/LAW"
  case_source_folder: "TEMP/CASE_DOCS"

  # Vector Database folders for each corpus
  # Foldery baz wektorowych dla kaÅ¼dego korpusu
  vector_db_legal_folder: "KNOWLEDGE_BASE/VECTOR_DB_LEGAL"
  vector_db_case_folder: "KNOWLEDGE_BASE/VECTOR_DB_CASE"

  # Folder for log files generated by the RAG scripts
  # Folder na pliki logÃ³w generowane przez skrypty RAG
  log_folder: "LOGS"

  # Model used for creating vector embeddings
  # Model uÅ¼ywany do tworzenia wektorÃ³w (embeddings)
  embedding_model: "models/text-embedding-004"

  # Large Language Model used for generating answers
  # DuÅ¼y Model JÄ™zykowy uÅ¼ywany do generowania odpowiedzi
  llm_model: "gemini-1.5-flash"

# --- Configuration for the Content Merging Script ---
# --- Konfiguracja dla Skryptu ÅÄ…czÄ…cego TreÅ›ci ---
merger_script_config:
  source_folder: "TEMP"
  output_folder: "FOR_ANALYSIS"
  font_path: "UbuntuMono-Regular.ttf"
  font_name: "UbuntuMono"
  font_size: 12
  model_name: "gemini-1.5-flash"
  ocr_prompt: "GEMINI, Make OCR. Do not add any additional information, just the text."
  audio_prompt: "Transcribe the audio recording. Correct the text for grammar and style, returning only the final, clean text in Polish."
  ocr_resolution: 150

# --- Configuration for the Translation Script ---
# --- Konfiguracja dla Skryptu TÅ‚umaczÄ…cego ---
translator_script_config:
  source_folder: "FOR_ANALYSIS"
  output_folder: "TRANSLATIONS"
  font_path: "UbuntuMono-Regular.ttf"
  font_name: "UbuntuMono"
  font_size: 12
  model_name: "gemini-1.5-flash"
  chunk_size_chars: 30000

  # List of target languages for translation
  # Lista jÄ™zykÃ³w docelowych dla tÅ‚umaczenia
  target_languages:
    - "English"
    - "Polish"
    #- "Arabic"
    #- "Chinese"
    #- "Czech"
    #- "French"
    #- "German"
    #- "Hebrew"
    #- "Japanese"
    #- "Persian"
    #- "Russian"
    #- "Spanish"
    #- "Ukrainian"

# --- Configuration for Other Scripts (e.g., Google Drive Upload) ---
# --- Konfiguracja dla Innych SkryptÃ³w (np. PrzesyÅ‚anie na Dysk Google) ---
folders_to_check:
  #- image_output
  #- TRANSLATIONS
  #- audio_input
  #- FOR_ANALYSIS
  #- LOGS
  #- audio_logs
  #- image_input
  #- PROCESSED_OUTPUT
  #- audio_output
  #- image_logs
  #- TEMP
  #- KNOWLEDGE_BASE

# Parent folder ID on Google Drive, used by the upload script
# ID folderu nadrzÄ™dnego na Dysku Google, uÅ¼ywane przez skrypt do przesyÅ‚ania plikÃ³w
google_drive_parent_folder_id: "*****"
--- END FILE: scripts/config.yaml ---

--- START FILE: scripts/image-text.py ---
import os
import google.generativeai as genai
import datetime
import time
import json
from reportlab.lib.pagesizes import A4
from reportlab.pdfgen import canvas
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
import yaml

with open("config.yaml", "r") as cr:
    config_vals = yaml.full_load(cr)
KEY = config_vals['KEY']

# --- PATH CONFIGURATION ---
# --- KONFIGURACJA ÅšCIEÅ»EK ---
# Folder for input images.
# Folder na obrazy wejÅ›ciowe.
IMAGE_INPUT_FOLDER = os.path.join(os.path.dirname(__file__), "image_input")
# Folder for output analysis results.
# Folder na wyniki analizy.
IMAGE_OUTPUT_FOLDER = os.path.join(os.path.dirname(__file__), "image_output")
# Folder for logs of image processing.
# Folder na logi przetwarzania obrazÃ³w.
IMAGE_LOG_FOLDER = os.path.join(os.path.dirname(__file__), "image_logs")

# Define the path to the Ubuntu Mono font file.
# Make sure 'UbuntuMono-R.ttf' is in the same directory as this script, or provide a full path.
# Zdefiniuj Å›cieÅ¼kÄ™ do pliku czcionki Ubuntu Mono.
# Upewnij siÄ™, Å¼e 'UbuntuMono-R.ttf' znajduje siÄ™ w tym samym katalogu co ten skrypt, lub podaj peÅ‚nÄ… Å›cieÅ¼kÄ™.
UBUNTU_MONO_FONT_PATH = os.path.join(os.path.dirname(__file__), "/home/luke_blue_lox/PycharmProjects/BLOX-TAK-GEMINI/UbuntuMono-Regular.ttf")

# Create directories if they don't exist.
# Tworzy katalogi, jeÅ›li nie istniejÄ….
os.makedirs(IMAGE_INPUT_FOLDER, exist_ok=True)
os.makedirs(IMAGE_OUTPUT_FOLDER, exist_ok=True)
os.makedirs(IMAGE_LOG_FOLDER, exist_ok=True)

# Register the Ubuntu Mono font with ReportLab.
# Zarejestruj czcionkÄ™ Ubuntu Mono w ReportLab.
try:
    pdfmetrics.registerFont(TTFont('UbuntuMono', UBUNTU_MONO_FONT_PATH))
    print(f"Font 'UbuntuMono' registered successfully from {UBUNTU_MONO_FONT_PATH}.")
    print(f"Czcionka 'UbuntuMono' zarejestrowana pomyÅ›lnie z {UBUNTU_MONO_FONT_PATH}.")
except Exception as e:
    print(
        f"ERROR: Could not register Ubuntu Mono font from {UBUNTU_MONO_FONT_PATH}. Please ensure the file exists and is accessible. Error: {e}")
    print(
        f"BÅÄ„D: Nie moÅ¼na zarejestrowaÄ‡ czcionki Ubuntu Mono z {UBUNTU_MONO_FONT_PATH}. Upewnij siÄ™, Å¼e plik istnieje i jest dostÄ™pny. BÅ‚Ä…d: {e}")
    # Fallback to a default font if Ubuntu Mono cannot be registered.
    # W razie problemÃ³w z rejestracjÄ…, uÅ¼yj domyÅ›lnej czcionki.
    DEFAULT_FONT = "Helvetica"
else:
    DEFAULT_FONT = "UbuntuMono"

FONT_SIZE = 12  # Font size for the PDF output / Rozmiar czcionki dla wyjÅ›cia PDF

# --- GEMINI API KEY CONFIGURATION ---
# --- KONFIGURACJA KLUCZA API GEMINI ---
# IMPORTANT: Replace "*****" with your real API key in config.yaml!
# WAÅ»NE: ZastÄ…p "*****" swoim prawdziwym kluczem API w pliku config.yaml!
GEMINI_API_KEY = KEY
# Configure the Gemini API with the provided key.
# Konfiguruje API Gemini za pomocÄ… podanego klucza.
genai.configure(api_key=GEMINI_API_KEY)
print("Gemini API configured successfully.")
print("Gemini API skonfigurowane pomyÅ›lnie.")


# --- Functions for Image File Handling and Gemini Vision ---
# --- Funkcje do obsÅ‚ugi plikÃ³w graficznych i Gemini Vision ---

def upload_image_to_gemini_files_api(image_file_path):
    # Print message indicating file upload.
    # WyÅ›wietla komunikat o przesyÅ‚aniu pliku.
    print(f"Uploading image file '{os.path.basename(image_file_path)}' to Gemini Files API...")
    print(f"PrzesyÅ‚anie pliku graficznego '{os.path.basename(image_file_path)}' do Gemini Files API...")
    try:
        # Upload the file using genai.upload_file.
        # PrzesyÅ‚a plik za pomocÄ… genai.upload_file.
        file = genai.upload_file(path=image_file_path)
        print(f"File '{os.path.basename(image_file_path)}' uploaded. Gemini File Name: {file.name}")
        print(f"Plik '{os.path.basename(image_file_path)}' przesÅ‚any. Nazwa pliku Gemini: {file.name}")
        return file
    except Exception as e:
        # Error message if upload fails.
        # Komunikat o bÅ‚Ä™dzie, jeÅ›li przesÅ‚anie pliku siÄ™ nie powiedzie.
        print(f"ERROR: Could not upload file '{image_file_path}' to Gemini Files API: {e}")
        print(f"BÅÄ„D: Nie moÅ¼na przesÅ‚aÄ‡ pliku '{image_file_path}' do Gemini Files API: {e}")
        return None


def analyze_image_with_gemini_model(image_file_object, model_name, prompt, log_data):
    # Print message indicating image analysis.
    # WyÅ›wietla komunikat o analizie obrazu.
    print(f"Analyzing image with Gemini model '{model_name}'...")
    print(f"Analiza obrazu za pomocÄ… modelu Gemini '{model_name}'...")

    # Initialize the GenerativeModel.
    # Inicjuje model GenerativeModel.
    model = genai.GenerativeModel(model_name=model_name)

    # Count tokens for input (image file and prompt).
    # Liczy tokeny dla inputu (pliku graficznego i promptu).
    try:
        # Note: For images, 'count_tokens' accepts a list of objects, including the image_file_object.
        # Uwaga: Dla obrazÃ³w 'count_tokens' przyjmuje listÄ™ obiektÃ³w, w tym image_file_object.
        count_response = model.count_tokens([image_file_object, prompt])
        input_token_count = count_response.total_tokens
        print(f"Input token count: {input_token_count}")
        print(f"Liczba tokenÃ³w wejÅ›ciowych: {input_token_count}")
        log_data["input_tokens"] = input_token_count
    except Exception as e:
        # Warning if input tokens cannot be counted.
        # OstrzeÅ¼enie, jeÅ›li nie moÅ¼na policzyÄ‡ tokenÃ³w wejÅ›ciowych.
        print(f"WARNING: Could not count input tokens: {e}")
        print(f"OSTRZEÅ»ENIE: Nie moÅ¼na policzyÄ‡ tokenÃ³w wejÅ›ciowych: {e}")
        log_data["input_tokens"] = "ERROR"

    response_text = None
    try:
        # Pass the image and prompt to generate_content.
        # Przekazuje obraz i prompt do generate_content.
        response = model.generate_content([image_file_object, prompt])
        response_text = response.text.strip()
        # Get output and total token counts from usage metadata.
        # Pobiera liczbÄ™ tokenÃ³w wyjÅ›ciowych i caÅ‚kowitÄ… z metadanych uÅ¼ycia.
        log_data["output_tokens"] = response.usage_metadata.candidates_token_count if hasattr(response.usage_metadata,
                                                                                              'candidates_token_count') else "N/A"
        log_data["total_tokens"] = response.usage_metadata.total_token_count if hasattr(response.usage_metadata,
                                                                                        'total_token_count') else "N/A"
        print(f"Output token count: {log_data['output_tokens']}")
        print(f"Liczba tokenÃ³w wyjÅ›ciowych: {log_data['output_tokens']}")
        print(f"Total tokens (input+output): {log_data['total_tokens']}")
        print(f"ÅÄ…czna liczba tokenÃ³w (wejÅ›cie+wyjÅ›cie): {log_data['total_tokens']}")
        return response_text
    except Exception as e:
        # Error message if image analysis fails.
        # Komunikat o bÅ‚Ä™dzie, jeÅ›li analiza obrazu siÄ™ nie powiedzie.
        print(f"ERROR: Could not analyze image with Gemini: {e}")
        print(f"BÅÄ„D: Nie moÅ¼na analizowaÄ‡ obrazu z Gemini: {e}")
        log_data["error"] = str(e)
        return None


def delete_gemini_file(file_object):
    # Only proceed if file_object exists.
    # Kontynuuje tylko jeÅ›li istnieje file_object.
    if file_object:
        # Print message indicating file deletion.
        # WyÅ›wietla komunikat o usuwaniu pliku.
        print(f"Deleting file '{file_object.name}' from Gemini Files API...")
        print(f"Usuwanie pliku '{file_object.name}' z Gemini Files API...")
        try:
            # Delete the file from Gemini Files API.
            # Usuwa plik z Gemini Files API.
            genai.delete_file(file_object.name)
            print("File deleted successfully.")
            print("Plik usuniÄ™ty pomyÅ›lnie.")
        except Exception as e:
            # Error message if deletion fails.
            # Komunikat o bÅ‚Ä™dzie, jeÅ›li usuniÄ™cie siÄ™ nie powiedzie.
            print(f"ERROR: Could not delete file '{file_object.name}' from Gemini Files API: {e}")
            print(f"BÅÄ„D: Nie moÅ¼na usunÄ…Ä‡ pliku '{file_object.name}' z Gemini Files API: {e}")


def get_image_analyzer_function(analyzer_type="gemini_native"):
    # Check if the analyzer type is 'gemini_native'.
    # Sprawdza, czy typ analizatora to 'gemini_native'.
    if analyzer_type == "gemini_native":
        # Model for image analysis (multimodal).
        # Model do analizy obrazÃ³w (multimodalny).
        # Use "gemini-1.5-flash" or "gemini-1.5-pro" for more advanced analysis.
        # UÅ¼yj "gemini-1.5-flash" lub "gemini-1.5-pro" dla bardziej zaawansowanych analiz.
        supported_image_model = "gemini-1.5-flash"

        print(f"Image analysis model set to: {supported_image_model}")
        print(f"Ustawiono model do analizy obrazÃ³w na: {supported_image_model}")

        # Define the inner function for Gemini native image analysis.
        # Definiuje wewnÄ™trznÄ… funkcjÄ™ do natywnej analizy obrazÃ³w Gemini.
        def _analyze_image_with_gemini_native(image_file_path, prompt_for_image, log_data):
            # Upload the image to Gemini Files API.
            # PrzesyÅ‚a obraz do Gemini Files API.
            file_obj = upload_image_to_gemini_files_api(image_file_path)
            if file_obj:
                log_data["gemini_file_name"] = file_obj.name
                try:
                    # Analyze the image using the Gemini model.
                    # Analizuje obraz za pomocÄ… modelu Gemini.
                    analysis_text = analyze_image_with_gemini_model(file_obj,
                                                                    model_name=supported_image_model,
                                                                    prompt=prompt_for_image,
                                                                    log_data=log_data)
                    return analysis_text
                finally:
                    # Ensure the file is deleted from Gemini Files API after analysis.
                    # Upewnia siÄ™, Å¼e plik zostanie usuniÄ™ty z Gemini Files API po analizie.
                    delete_gemini_file(file_obj)
            return None

        return _analyze_image_with_gemini_native
    else:
        # Raise an error for an unknown analyzer type.
        # Podnosi bÅ‚Ä…d dla nieznanego typu analizatora.
        raise ValueError(f"Unknown image analysis type: {analyzer_type}. Please choose 'gemini_native'.")
        raise ValueError(f"Nieznany typ analizy obrazu: {analyzer_type}. ProszÄ™ wybraÄ‡ 'gemini_native'.")


# --- New function to save text to PDF ---
# --- Nowa funkcja do zapisywania tekstu do PDF ---
def save_text_to_pdf(text_content, output_pdf_path, font_name, font_size):
    print(f"Saving text to PDF: {output_pdf_path} with font '{font_name}' {font_size}pt...")
    print(f"Zapisywanie tekstu do PDF: {output_pdf_path} czcionkÄ… '{font_name}' {font_size}pkt...")
    try:
        c = canvas.Canvas(output_pdf_path, pagesize=A4)
        c.setFont(font_name, font_size)
        width, height = A4

        # Margins / Marginesy
        left_margin = 50
        top_margin = height - 50
        line_height = font_size * 1.2  # Adjust for spacing / Dostosuj do odstÄ™pÃ³w

        y_position = top_margin

        # Split text into lines, handling long lines
        # Podziel tekst na linie, obsÅ‚ugujÄ…c dÅ‚ugie linie
        lines = []
        for paragraph in text_content.split('\n'):
            wrapped_lines = []
            if paragraph.strip():  # Avoid processing empty paragraphs
                # Calculate max characters per line for the given font and size
                # Oblicz maksymalnÄ… liczbÄ™ znakÃ³w na liniÄ™ dla danej czcionki i rozmiaru
                char_width = pdfmetrics.stringWidth('M', font_name, font_size)  # Width of a typical character
                max_chars_per_line = int((width - 2 * left_margin) / char_width) if char_width > 0 else 100

                # ReportLab's textobject handles wrapping better for monospaced fonts,
                # but a manual wrap ensures we control chunking.
                # Obiekt tekstowy ReportLab lepiej obsÅ‚uguje zawijanie dla czcionek monospaced,
                # ale rÄ™czne zawijanie zapewnia kontrolÄ™ nad chunkowaniem.
                current_line = ""
                for word in paragraph.split(' '):
                    if pdfmetrics.stringWidth(current_line + (word + ' '), font_name, font_size) < (
                            width - 2 * left_margin):
                        current_line += (word + ' ')
                    else:
                        wrapped_lines.append(current_line.strip())
                        current_line = word + ' '
                if current_line.strip():
                    wrapped_lines.append(current_line.strip())
            else:
                wrapped_lines.append("")  # Keep empty lines for paragraph breaks
            lines.extend(wrapped_lines)

        for line in lines:
            if y_position < 50:  # Check if new page is needed (50 is bottom margin)
                c.showPage()
                c.setFont(font_name, font_size)
                y_position = top_margin

            c.drawString(left_margin, y_position, line)
            y_position -= line_height

        c.save()
        print(f"Successfully saved PDF to: {output_pdf_path}")
        print(f"PomyÅ›lnie zapisano PDF do: {output_pdf_path}")
    except Exception as e:
        print(f"ERROR: Could not save text to PDF '{output_pdf_path}': {e}")
        print(f"BÅÄ„D: Nie moÅ¼na zapisaÄ‡ tekstu do PDF '{output_pdf_path}': {e}")


def process_image_folder():
    print(f"\n--- Starting scan and processing of image files from folder: {IMAGE_INPUT_FOLDER} ---")
    print(f"\n--- Rozpoczynam skanowanie i przetwarzanie plikÃ³w graficznych z folderu: {IMAGE_INPUT_FOLDER} ---")

    # Supported image file extensions.
    # ObsÅ‚ugiwane rozszerzenia plikÃ³w graficznych.
    image_files = [f for f in os.listdir(IMAGE_INPUT_FOLDER)
                   if f.lower().endswith((".png", ".jpg", ".jpeg", ".webp", ".pdf"))]

    if not image_files:
        print("INFO: No image files found in the folder for analysis.")
        print("INFO: Brak plikÃ³w graficznych w folderze do analizy.")
        return

    # You can define a single prompt for all images or customize it for each.
    # MoÅ¼esz zdefiniowaÄ‡ pojedynczy prompt dla wszystkich obrazÃ³w lub dostosowaÄ‡ go dla kaÅ¼dego z osobna.
    # Example of a simple prompt:
    # PrzykÅ‚ad prostego promptu:
    default_image_prompt = "GEMINI, Make OCR, And Make Output in Polish and English Language. Do not add any additional information, just the text."  # "Describe in detail what you see in the image. Indicate the most important elements, colors, text (if present), and the general mood or context."
    # default_image_prompt = "Opisz szczegÃ³Å‚owo co widzisz na obrazie. WskaÅ¼ najwaÅ¼niejsze elementy, kolory, tekst (jeÅ›li wystÄ™puje) oraz ogÃ³lny nastrÃ³j lub kontekst."

    # Example of a more advanced prompt (if you want to analyze documents, for example):
    # PrzykÅ‚ad bardziej zaawansowanego promptu (jeÅ›li chcesz analizowaÄ‡ np. dokumenty):
    # default_image_prompt = "Transcribe all visible text in the image. Then, summarize the main content of the document and indicate if it contains any dates or key data."
    # default_image_prompt = "Przetranskrybuj caÅ‚y tekst widoczny na obrazie. NastÄ™pnie, podsumuj gÅ‚Ã³wnÄ… treÅ›Ä‡ dokumentu i wskaÅ¼, czy zawiera jakiekolwiek daty lub kluczowe dane."

    # Get the image analyzer function.
    # Pobiera funkcjÄ™ analizujÄ…cÄ… obrazy.
    analyze_image_function = get_image_analyzer_function("gemini_native")

    total_files_processed = 0
    total_tokens_used = 0
    total_processing_time = 0.0

    # Global list to collect logs for all files.
    # Globalna lista do zbierania logÃ³w dla wszystkich plikÃ³w.
    overall_logs = []

    for image_file in image_files:
        image_path = os.path.join(IMAGE_INPUT_FOLDER, image_file)
        base_name = os.path.splitext(image_file)[0]
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        output_txt_path = os.path.join(IMAGE_OUTPUT_FOLDER, f"{base_name}_analysis_{timestamp}.txt")
        output_pdf_path = os.path.join(IMAGE_OUTPUT_FOLDER, f"{base_name}_analysis_{timestamp}.pdf")  # New PDF path
        log_file_path = os.path.join(IMAGE_LOG_FOLDER, f"{base_name}_log_{timestamp}.json")

        # Dictionary for logs of the current file.
        # SÅ‚ownik na logi dla bieÅ¼Ä…cego pliku.
        current_log_data = {
            "timestamp": timestamp,
            "image_file": image_file,
            "status": "processing",
            "start_time_utc": datetime.datetime.utcnow().isoformat()
        }

        print(f"\n--- Processing file: {image_file} ---")
        print(f"\n--- Przetwarzanie pliku: {image_file} ---")
        file_start_time = time.time()

        analysis_result_text = None
        try:
            # Pass the prompt to the image analysis function.
            # Przekazuje prompt do funkcji analizujÄ…cej obraz.
            analysis_result_text = analyze_image_function(image_path, default_image_prompt, current_log_data)

            if analysis_result_text:
                print(f"SUCCESS: Analysis of file '{image_file}' completed.")
                print(f"SUKCES: Analiza pliku '{image_file}' zakoÅ„czona.")

                # Save to TXT
                # Zapisz do TXT
                with open(output_txt_path, "w", encoding="utf-8") as f:
                    f.write(analysis_result_text)
                print(f"Analysis result saved to TXT: {output_txt_path}")
                print(f"Wynik analizy zapisano do TXT: {output_txt_path}")

                # Save to PDF
                # Zapisz do PDF
                save_text_to_pdf(analysis_result_text, output_pdf_path, DEFAULT_FONT, FONT_SIZE)

                current_log_data["status"] = "SUCCESS"
            else:
                print(f"WARNING: No analysis result for file '{image_file}'.")
                print(f"OSTRZEÅ»ENIE: Brak wyniku analizy dla pliku '{image_file}'.")
                with open(output_txt_path, "w", encoding="utf-8") as f:
                    f.write("[NO ANALYSIS RESULT FROM GEMINI]")
                    f.write("[BRAK WYNIKU ANALIZY Z GEMINI]")
                # Optionally create an empty or warning PDF
                # Opcjonalnie stwÃ³rz pusty lub ostrzegawczy PDF
                save_text_to_pdf("[NO ANALYSIS RESULT FROM GEMINI]\n[BRAK WYNIKU ANALIZY Z GEMINI]", output_pdf_path,
                                 DEFAULT_FONT, FONT_SIZE)
                current_log_data["status"] = "WARNING_NO_RESULT"
        except Exception as e:
            print(f"ERROR: An error occurred while processing '{image_file}': {e}")
            print(f"BÅÄ„D: WystÄ…piÅ‚ bÅ‚Ä…d podczas przetwarzania '{image_file}': {e}")
            with open(output_txt_path, "w", encoding="utf-8") as f:
                f.write(f"[PROCESSING ERROR: {e}]")
                f.write(f"[BÅÄ„D PRZETWARZANIA: {e}]")
            # Optionally create an error PDF
            # Opcjonalnie stwÃ³rz PDF z bÅ‚Ä™dem
            save_text_to_pdf(f"[PROCESSING ERROR: {e}]\n[BÅÄ„D PRZETWARZANIA: {e}]", output_pdf_path, DEFAULT_FONT,
                             FONT_SIZE)
            current_log_data["status"] = "ERROR"
            current_log_data["exception_details"] = str(e)

        file_end_time = time.time()
        duration = file_end_time - file_start_time
        print(f"Processing time for '{image_file}': {duration:.2f} seconds.")
        print(f"Czas przetwarzania dla '{image_file}': {duration:.2f} sekundy.")

        current_log_data["duration_seconds"] = round(duration, 2)
        current_log_data["end_time_utc"] = datetime.datetime.utcnow().isoformat()

        # Save logs for the current file to a JSON file.
        # Zapisuje logi dla bieÅ¼Ä…cego pliku do pliku JSON.
        with open(log_file_path, "w", encoding="utf-8") as log_f:
            json.dump(current_log_data, log_f, indent=4, ensure_ascii=False)
        print(f"Logs for file '{image_file}' saved to: {log_file_path}")
        print(f"Logi dla pliku '{image_file}' zapisano do: {log_file_path}")

        overall_logs.append(current_log_data)

        total_files_processed += 1
        total_processing_time += duration
        # Check if total_tokens is an integer before adding.
        # Sprawdza, czy total_tokens jest liczbÄ… caÅ‚kowitÄ… przed dodaniem.
        if "total_tokens" in current_log_data and isinstance(current_log_data["total_tokens"], int):
            total_tokens_used += current_log_data["total_tokens"]

    print(f"\n--- Finished processing all image files. ---")
    print(f"\n--- ZakoÅ„czono przetwarzanie wszystkich plikÃ³w graficznych. ---")
    print(f"Total files processed: {total_files_processed}")
    print(f"ÅÄ…cznie przetworzono plikÃ³w: {total_files_processed}")
    print(f"Total execution time: {total_processing_time:.2f} seconds.")
    print(f"CaÅ‚kowity czas dziaÅ‚ania: {total_processing_time:.2f} sekundy.")
    print(f"Total token usage for all files: {total_tokens_used} tokens.")
    print(f"CaÅ‚kowite zuÅ¼ycie tokenÃ³w dla wszystkich plikÃ³w: {total_tokens_used} tokenÃ³w.")

    # Optionally: Save a summary log for the entire session.
    # Opcjonalnie: Zapisz sumaryczny log dla caÅ‚ej sesji.
    summary_log_path = os.path.join(IMAGE_LOG_FOLDER,
                                    f"summary_log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    summary_data = {
        "overall_summary": {
            "total_files_processed": total_files_processed,
            "total_processing_time_seconds": round(total_processing_time, 2),
            "total_tokens_used": total_tokens_used,
            "session_start_time_utc": overall_logs[0]["start_time_utc"] if overall_logs else "N/A",
            "session_end_time_utc": overall_logs[-1]["end_time_utc"] if overall_logs else "N/A"
        },
        "file_details": overall_logs
    }
    with open(summary_log_path, "w", encoding="utf-8") as sum_f:
        json.dump(summary_data, sum_f, indent=4, ensure_ascii=False)
    print(f"Session summary logs saved to: {summary_log_path}")
    print(f"Sumaryczne logi sesji zapisano do: {summary_log_path}")


if __name__ == "__main__":
    # Call the main function to process the image folder.
    # WywoÅ‚uje gÅ‚Ã³wnÄ… funkcjÄ™ do przetwarzania folderu z obrazami.
    process_image_folder()
    print("\n--- Script image-text.py execution finished. ---")
    print("\n--- ZakoÅ„czono dziaÅ‚anie skryptu image-text.py ---")
--- END FILE: scripts/image-text.py ---

--- START FILE: scripts/analysis-summary.py ---
import os
import pdfplumber
import google.generativeai as genai
import textwrap
import datetime
import json  # Import needed for log files
import yaml

# --- PDF Generation Libraries ---
# --- Biblioteki do generowania PDF ---
from reportlab.lib.pagesizes import A4
from reportlab.pdfgen import canvas
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont

with open("config.yaml", "r") as cr:
    config_vals = yaml.full_load(cr)
KEY = config_vals['KEY']

# --- Path Configuration ---
# --- Konfiguracja Å›cieÅ¼ek ---
PDF_INPUT_FOLDER = "/home/luke_blue_lox/PycharmProjects/BLOX-TAK-GEMINI/FOR_ANALYSIS"
OUTPUT_FOLDER = "/home/luke_blue_lox/PycharmProjects/BLOX-TAK-GEMINI/PROCESSED_OUTPUT"
LOG_FOLDER = "/home/luke_blue_lox/PycharmProjects/BLOX-TAK-GEMINI/LOGS"

# Aktualny czas
now = datetime.datetime.now()

# Format ISO 8601 z milisekundami
full_timestamp = now.strftime("%Y-%m-%dT%H:%M:%S.%f%z")

# Define the path to the Ubuntu Mono font file.
# Make sure 'UbuntuMono-R.ttf' is in the same directory as this script, or provide a full path.
# Zdefiniuj Å›cieÅ¼kÄ™ do pliku czcionki Ubuntu Mono.
# Upewnij siÄ™, Å¼e 'UbuntuMono-R.ttf' znajduje siÄ™ w tym samym katalogu co ten skrypt, lub podaj peÅ‚nÄ… Å›cieÅ¼kÄ™.
UBUNTU_MONO_FONT_PATH = os.path.join(os.path.dirname(__file__), "/home/luke_blue_lox/PycharmProjects/BLOX-TAK-GEMINI/UbuntuMono-Regular.ttf")

os.makedirs(OUTPUT_FOLDER, exist_ok=True)
os.makedirs(LOG_FOLDER, exist_ok=True)

# Register the Ubuntu Mono font with ReportLab.
# Zarejestruj czcionkÄ™ Ubuntu Mono w ReportLab.
try:
    pdfmetrics.registerFont(TTFont('UbuntuMono', UBUNTU_MONO_FONT_PATH))
    print(f"Font 'UbuntuMono' registered successfully from {UBUNTU_MONO_FONT_PATH}.")
    print(f"Czcionka 'UbuntuMono' zarejestrowana pomyÅ›lnie z {UBUNTU_MONO_FONT_PATH}.")
except Exception as e:
    print(
        f"ERROR: Could not register Ubuntu Mono font from {UBUNTU_MONO_FONT_PATH}. Please ensure the file exists and is accessible. Error: {e}")
    print(
        f"BÅÄ„D: Nie moÅ¼na zarejestrowaÄ‡ czcionki Ubuntu Mono z {UBUNTU_MONO_FONT_PATH}. Upewnij siÄ™, Å¼e plik istnieje i jest dostÄ™pny. BÅ‚Ä…d: {e}")
    # Fallback to a default font if Ubuntu Mono cannot be registered.
    # W razie problemÃ³w z rejestracjÄ…, uÅ¼yj domyÅ›lnej czcionki.
    DEFAULT_FONT = "Helvetica"
else:
    DEFAULT_FONT = "UbuntuMono"

FONT_SIZE = 12  # Font size for the PDF output / Rozmiar czcionki dla wyjÅ›cia PDF

# --- GEMINI API KEY CONFIGURATION ---
# --- KONFIGURACJA KLUCZA API GEMINI ---
# IMPORTANT: Replace "*****" with your real API key in config.yaml!
# WAÅ»NE: ZastÄ…p "*****" swoim prawdziwym kluczem API w pliku config.yaml!
GOOGLE_API_KEY = KEY
genai.configure(api_key=GOOGLE_API_KEY)
print("Gemini API configured successfully.")
print("Gemini API skonfigurowane pomyÅ›lnie.")

MODEL_NAME = "gemini-1.5-flash"

generation_config = {
    "temperature": 0.2,
    "top_p": 0.9,
    "top_k": 30,
    "max_output_tokens": 4096,
}

safety_settings = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]

model = genai.GenerativeModel(
    model_name=MODEL_NAME,
    generation_config=generation_config,
    safety_settings=safety_settings
)

CHUNK_SIZE = 100_000  # Rozmiar chunka dla pojedynczych dokumentÃ³w
OVERALL_SUMMARY_CHUNK_SIZE = 50_000  # Rozmiar chunka dla globalnego podsumowania - moÅ¼na dostosowaÄ‡


# --- Funkcja do ekstrakcji CAÅEGO tekstu ---
def extract_text_from_pdf_full(pdf_path):
    text = ""
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages):
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
    except Exception as e:
        print(f"BÅÄ„D: Nie moÅ¼na wyodrÄ™bniÄ‡ tekstu z {pdf_path}: {e}")
        return None
    return text.strip()


# --- Funkcja do ekstrakcji tekstu z WYBRANYCH STRON ---
def extract_selected_pages_from_pdf(pdf_path, start_page, end_page):
    text = ""
    try:
        with pdfplumber.open(pdf_path) as pdf:
            num_pages = len(pdf.pages)

            actual_start_index = max(0, start_page - 1)
            actual_end_index = min(num_pages, end_page)

            if actual_start_index >= num_pages:
                print(
                    f"OSTRZEÅ»ENIE: Strona poczÄ…tkowa {start_page} wykracza poza liczbÄ™ stron PDF ({num_pages}). Zwracam pusty tekst.")
                return ""
            if actual_start_index >= actual_end_index:
                print(
                    f"OSTRZEÅ»ENIE: Zakres stron ({start_page}-{end_page}) jest nieprawidÅ‚owy lub pusty. Zwracam pusty tekst.")
                return ""

            print(
                f"Ekstrakcja stron od {actual_start_index + 1} do {actual_end_index} z pliku '{os.path.basename(pdf_path)}'...")

            for i in range(actual_start_index, actual_end_index):
                page = pdf.pages[i]
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
                else:
                    print(f"  INFO: Strona {i + 1} jest pusta lub nie zawiera tekstu.")
    except Exception as e:
        print(f"BÅÄ„D: Nie moÅ¼na wyodrÄ™bniÄ‡ tekstu z {pdf_path} dla stron {start_page}-{end_page}: {e}")
        return None
    return text.strip()


# --- Funkcja do interaktywnego pobierania zakresu stron ---
def get_page_range_input(pdf_file_name, total_pages):
    while True:
        choice = input(f"Dla pliku '{pdf_file_name}' (Å‚Ä…cznie stron: {total_pages}):\n"
                       f"1. PrzetwÃ³rz wszystkie strony\n"
                       f"2. Podaj zakres stron (np. 10-20)\n"
                       f"Wybierz opcjÄ™ (1/2): ").strip()

        if choice == '1':
            return 1, total_pages
        elif choice == '2':
            page_range_str = input("Podaj zakres stron (np. 10-20): ").strip()
            try:
                start_str, end_str = page_range_str.split('-')
                start_page = int(start_str)
                end_page = int(end_str)
                if 1 <= start_page <= end_page <= total_pages:
                    return start_page, end_page
                else:
                    print(
                        f"BÅÄ„D: NieprawidÅ‚owy zakres stron. Upewnij siÄ™, Å¼e {1} <= poczÄ…tek <= koniec <= {total_pages}.")
            except ValueError:
                print("BÅÄ„D: NieprawidÅ‚owy format. UÅ¼yj formatu 'START-KONIEC', np. '10-20'.")
        else:
            print("NieprawidÅ‚owy wybÃ³r. ProszÄ™ wybraÄ‡ 1 lub 2.")


def analyze_text_with_gemini(text_to_analyze, prompt_prefix=""):
    if not text_to_analyze.strip():
        print("INFO: Brak tekstu do analizy. Zwracam pusty string.")
        return "", 0, 0

    chunks = textwrap.wrap(text_to_analyze, CHUNK_SIZE, break_long_words=False, replace_whitespace=False)

    if not chunks:
        print("OSTRZEÅ»ENIE: Tekst nie zostaÅ‚ podzielony na chunki prawidÅ‚owo. Zwracam pusty string.")
        return "", 0, 0

    full_analysis = []
    total_input_tokens = 0
    total_output_tokens = 0

    print(f"Tekst zostanie podzielony na {len(chunks)} czÄ™Å›ci do analizy.")

    for i, chunk in enumerate(chunks):
        print(f"AnalizujÄ™ czÄ™Å›Ä‡ {i + 1}/{len(chunks)} ({len(chunk)} znakÃ³w)...")

        base_prompt = (
                f"JesteÅ› wysoce doÅ›wiadczonym ekspertem prawnym, specjalizujÄ…cym siÄ™ w prawie cywilnym, egzekucyjnym i socjalnym w Polsce. "
                f"Przeanalizuj dokument pod kÄ…tem Prawa Polskiego i Unii Europejskiej, zidentyfikuj kluczowe fakty prawne, terminy, strony, roszczenia, zobowiÄ…zania, dowody oraz oÅ›wiadczenia dotyczÄ…ce sytuacji finansowej i zdrowotnej Åukasza Andruszkiewicza. "
                f"SzczegÃ³lnÄ… uwagÄ™ zwrÃ³Ä‡ na: odniesienia do sytuacji finansowej, dÅ‚ugÃ³w, dochodÃ³w, zatrudnienia, prÃ³b znalezienia pracy; szczegÃ³Å‚y stanu zdrowia, wypadkÃ³w, urazÃ³w, braku ubezpieczenia; wzmianki o prÃ³bach uzyskania pomocy od instytucji; koniecznoÅ›Ä‡ podjÄ™cia pracy zdalnej; oÅ›wiadczenia dotyczÄ…ce braku majÄ…tku i trudnoÅ›ci egzystencji; adresatÃ³w, daty i sygnatury akt. Zacytuj odpowiednie artykuÅ‚y i opisz jakie prawa zostaÅ‚y zÅ‚amane. "
                f"Wynik podaj w jÄ™zyku Polskim i Angielskim. Podaj rÃ³wnieÅ¼ na koÅ„cu treÅ›Ä‡ uÅ¼ytego promptu - analogicznie w jÄ™zyku "
                f"Polskim i Angielskim, a takÅ¼e wersjÄ™ modelu jaki zostaÅ‚ uÅ¼yty - czyli: gemini-1.5-flash,"
                f"z peÅ‚nym timestamp: " + full_timestamp
        )
        prompt = f"{prompt_prefix}\n\n{base_prompt}\n\nTekst do analizy:\n\n{chunk}"

        current_chunk_input_tokens = 0
        current_chunk_output_tokens = 0
        chunk_analysis_part = ""

        try:
            current_chunk_input_tokens = model.count_tokens(prompt).total_tokens
            total_input_tokens += current_chunk_input_tokens
            print(f"  Szacowane tokeny wejÅ›ciowe dla tej czÄ™Å›ci: {current_chunk_input_tokens}")

            response = model.generate_content(prompt)

            if response.parts:
                chunk_analysis_part = "".join([part.text for part in response.parts]).strip()
                full_analysis.append(chunk_analysis_part)

                try:
                    if hasattr(response, '_result') and 'usageMetadata' in response._result:
                        usage_metadata = response._result['usageMetadata']
                        current_chunk_output_tokens = usage_metadata.get('candidatesTokenCount', 0)
                        print(
                            f"  Wygenerowane tokeny wyjÅ›ciowe dla tej czÄ™Å›ci (z usageMetadata): {current_chunk_output_tokens}")
                    else:
                        print(
                            "  Brak usageMetadata w odpowiedzi, mimo obecnoÅ›ci treÅ›ci. Oszacowanie tokenÃ³w wyjÅ›ciowych na podstawie tekstu.")
                        current_chunk_output_tokens = model.count_tokens(chunk_analysis_part).total_tokens
                        print(f"  Oszacowane tokeny wyjÅ›ciowe dla tej czÄ™Å›ci (z tekstu): {current_chunk_output_tokens}")
                except Exception as usage_e:
                    print(f"BÅÄ„D podczas pobierania usageMetadata dla czÄ™Å›ci {i + 1}: {usage_e}")
                    print("  Oszacowanie tokenÃ³w wyjÅ›ciowych na podstawie wygenerowanego tekstu.")
                    current_chunk_output_tokens = model.count_tokens(chunk_analysis_part).total_tokens
                    print(f"  Oszacowane tokeny wyjÅ›ciowe dla tej czÄ™Å›ci (z tekstu): {current_chunk_output_tokens}")
            else:
                print(
                    f"OSTRZEÅ»ENIE: Gemini nie zwrÃ³ciÅ‚o Å¼adnego tekstu (response.parts jest puste) dla czÄ™Å›ci {i + 1}.")
                if hasattr(response, 'prompt_feedback') and response.prompt_feedback:
                    print(f"  Feedback od Gemini (Prompt Feedback): {response.prompt_feedback}")
                    full_analysis.append(f"[PROBLEM: FEEDBACK OD GEMINI - {response.prompt_feedback}]\n")
                if hasattr(response, 'candidates') and response.candidates:
                    print("  DostÄ™pne kandydujÄ…ce odpowiedzi (moÅ¼e zawieraÄ‡ powody blokady):")
                    for candidate in response.candidates:
                        print(f"    Finish Reason: {candidate.finish_reason}")
                        if hasattr(candidate, 'safety_ratings'):
                            print(f"    Safety Ratings: {candidate.safety_ratings}")
                            full_analysis.append(f"[PROBLEM: BEZPIECZEÅƒSTWO - {candidate.safety_ratings}]\n")
                        if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                            print(f"    Content parts (puste?): {candidate.content.parts}")
                else:
                    print("  Brak 'candidates' w odpowiedzi (coÅ› poszÅ‚o bardzo Åºle).")
                full_analysis.append(f"[BRAK ANALIZY DLA TEJ CZÄ˜ÅšCI: PROBLEM Z API LUB TREÅšCIÄ„]\n")

            total_output_tokens += current_chunk_output_tokens

        except genai.types.BlockedPromptException as e:
            print(f"BÅÄ„D: Å»Ä…danie zablokowane przez Gemini (safety settings) dla czÄ™Å›ci {i + 1}: {e}")
            if hasattr(e, 'response') and e.response:
                print(f"  SzczegÃ³Å‚y bÅ‚Ä™du z API Gemini: {e.response.text}")
                full_analysis.append(f"[ANALIZA ZABLOKOWANA PRZEZ API: {e.response.text}]\n")
            else:
                full_analysis.append(f"[ANALIZA ZABLOKOWANA ZE WZGLÄ˜DÃ“W BEZPIECZEÅƒSTWA - BLOCKED PROMPT EXCEPTION]\n")
        except Exception as e:
            print(f"KRYTYCZNY BÅÄ„D: Podczas komunikacji z Google Gemini dla czÄ™Å›ci {i + 1}: {e}")
            print(f"  Typ bÅ‚Ä™du: {type(e).__name__}")
            if hasattr(e, 'response') and e.response:
                print(f"  SzczegÃ³Å‚y bÅ‚Ä™du z API Gemini: {e.response.text}")
            full_analysis.append(f"[BÅÄ„D ANALIZY DLA TEJ CZÄ˜ÅšCI: {e}]\n")

    final_analysis = "\n\n---\n\n".join(full_analysis)
    return final_analysis, total_input_tokens, total_output_tokens


# --- Nowa funkcja do globalnego podsumowania ---
def summarize_overall_legal_findings(all_summaries_text):
    print("\n\n--- Generowanie globalnego podsumowania prawnego ---")
    if not all_summaries_text.strip():
        print("Brak danych do globalnego podsumowania. Zwracam pusty tekst.")
        return "", 0, 0

    overall_chunks = textwrap.wrap(all_summaries_text, OVERALL_SUMMARY_CHUNK_SIZE, break_long_words=False,
                                   replace_whitespace=False)

    overall_summary_parts = []
    overall_input_tokens = 0
    overall_output_tokens = 0

    base_overall_prompt = (
        f"JesteÅ› wysoce doÅ›wiadczonym ekspertem prawnym, specjalizujÄ…cym siÄ™ w prawie cywilnym, egzekucyjnym i socjalnym w Polsce. "
        f"Twoim zadaniem jest przygotowanie kompleksowych i rzeczowych wyjaÅ›nieÅ„ dla Kancelarii Komorniczych, "
        f"bazujÄ…c na caÅ‚oÅ›ci dostarczonego dokumentu (poÅ‚Ä…czonego z wielu ÅºrÃ³deÅ‚). "
        f"Celem jest przedstawienie aktualnej, bardzo trudnej sytuacji finansowej, zdrowotnej i Å¼yciowej dÅ‚uÅ¼nika Åukasza Andruszkiewicza, "
        f"zgodnie z wezwaniem od Komornika Joanny Majdak (odwoÅ‚ujÄ…c siÄ™ do treÅ›ci dokumentu). "
        f"W swoich wyjaÅ›nieniach, uwzglÄ™dnij i szczegÃ³Å‚owo opisz nastÄ™pujÄ…ce punkty, odwoÅ‚ujÄ…c siÄ™ do treÅ›ci dokumentu i przekazanych informacji:"
        f"\n\n1.  **Potwierdzenie aktualnej trudnej sytuacji:** Jasno zaznacz, Å¼e sytuacja dÅ‚uÅ¼nika nie ulegÅ‚a poprawie od poprzednich wyjaÅ›nieÅ„ i jest bardzo trudna (odwoÅ‚aj siÄ™ do dokumentÃ³w np. 'SPRZECIW_Nc-e_1932318_24_2025-04-27.pdf' oraz 'Gmail - WyjaÅ›nienia 2024-08-12.PDF', wskazujÄ…c na pogorszenie i brak moÅ¼liwoÅ›ci spÅ‚aty). "
        f"\n\n2.  **SzczegÃ³Å‚owy opis stanu zdrowia i jego wpÅ‚ywu na sytuacjÄ™:** "
        f"    Wspomnij o wypadku z 1 maja, nieleczonych urazach (oczodÃ³Å‚, policzek, zÄ™by, staw skroniowo-Å¼uchwowy, kolano, Å›ciÄ™gno Achillesa), braku ubezpieczenia zdrowotnego i niemoÅ¼noÅ›ci odbycia badaÅ„ kontrolnych (odwoÅ‚aj siÄ™ do 'Gmail - KM 1623_22.pdf' oraz 'Cover Letter.pdf'). "
        f"    PodkreÅ›l zagroÅ¼enie neuralgiÄ… nerwu trÃ³jdzielnego jako konsekwencjÄ™ urazÃ³w ('SPRZECIW_Nc-e_1932318_24_2025-04-27.pdf'). "
        f"    Wspomnij o braku wsparcia instytucjonalnego w kwestii zdrowia i ubezpieczenia (np. odmowa PUP). "
        f"\n\n3.  **Opis sytuacji finansowej i majÄ…tkowej:** "
        f"    Zadeklaruj brak moÅ¼liwoÅ›ci spÅ‚aty zadÅ‚uÅ¼enia. "
        f"    Jasno okreÅ›l, Å¼e jedynÄ… rzeczÄ…, jakÄ… udaÅ‚o siÄ™ nabyÄ‡, sÄ… przedmioty z faktury 'F_2025_19384_1.pdf' (adapter dysku NVME M.2, Raspberry Pi 256GB SSD, koszt dostawy), podajÄ…c ich wartoÅ›Ä‡. "
        f"    WyjaÅ›nij pochodzenie Å›rodkÃ³w na ten zakup: ostatnie odÅ‚oÅ¼one pieniÄ…dze od zeszÅ‚ego roku, w tym 100 PLN od Brata na Å›wiÄ™ta i 100 PLN od rodzicÃ³w za rozliczenie zeznaÅ„ podatkowych w bieÅ¼Ä…cym roku. "
        f"    PodkreÅ›l, Å¼e byÅ‚ to **konieczny wydatek** w celu dokoÅ„czenia portfolio zwiÄ…zanego z ekosystemem TAK, co jest kluczowe dla prÃ³b zarobienia pieniÄ™dzy. "
        f"    Wspomnij o aktywnych, lecz dotychczas bezskutecznych prÃ³bach znalezienia zatrudnienia/wspÃ³Å‚pracy, co prowadzi do braku dochodÃ³w. "
        f"    PotwierdÅº, Å¼e dÅ‚uÅ¼nik nie posiada innych znaczÄ…cych Å›rodkÃ³w ani majÄ…tku poza wymienionymi. "
        f"    OdwoÅ‚aj siÄ™ do wszelkich wczeÅ›niejszych oÅ›wiadczeÅ„ o trudnej sytuacji finansowej, wyzysku, braku wsparcia od PaÅ„stwa Polskiego i UE, oraz Å¼Ä…daniach odszkodowaÅ„ (np. w 'SPRZECIW_2025-03-03.pdf', 'ODWOÅANIE-SeriaP_Nr0360-2025-01-13_GOV-PL_2025-01-30'). "
        f"\n\n4.  **Konsekwencje i oczekiwania dÅ‚uÅ¼nika:** "
        f"    Wspomnij o koniecznoÅ›ci prowadzenia korespondencji z zagranicy i braku odpowiedzi. "
        f"    Zaznacz, Å¼e dÅ‚uÅ¼nik Å¼Ä…da prawnika, ktÃ³rego wynagrodzenie pokryje Fundusz SprawiedliwoÅ›ci, oraz renty czasowej z ubezpieczeniem zdrowotnym, aby mÃ³gÅ‚ zadbaÄ‡ o swoje zdrowie i przeprowadziÄ‡ upadÅ‚oÅ›Ä‡. ('SPRZECIW_2025-03-03.pdf') "
        f"    PodkreÅ›l, Å¼e dÅ‚uÅ¼nik nie jest w stanie obecnie stawiaÄ‡ siÄ™ przed instytucjami w regionie (Dolny ÅšlÄ…sk) ze wzglÄ™du na doÅ›wiadczenia (odwoÅ‚aj siÄ™ do 'Pismo WÅ‚aÅ›ciwe.pdf' oraz 'SPRZECIW_2025-03-03.pdf')."
        f"\n\nZadbaj o to, aby wyjaÅ›nienia byÅ‚y kompleksowe, spÃ³jne, rzeczowe i empatyczne, jednoczeÅ›nie Å›ciÅ›le trzymajÄ…c siÄ™ faktÃ³w zawartych w dokumentacji. Tekst wygenerowany przez model bÄ™dzie stanowiÅ‚ trzon pisma do kancelarii komorniczej."
    )

    for i, chunk in enumerate(overall_chunks):
        print(f"AnalizujÄ™ czÄ™Å›Ä‡ {i + 1}/{len(overall_chunks)} globalnego podsumowania ({len(chunk)} znakÃ³w)...")
        prompt = f"{base_overall_prompt}\n\nAnalizy do podsumowania:\n\n{chunk}"

        chunk_input_tokens = 0
        chunk_output_tokens = 0
        current_summary_part = ""

        try:
            chunk_input_tokens = model.count_tokens(prompt).total_tokens
            overall_input_tokens += chunk_input_tokens
            print(f"  Szacowane tokeny wejÅ›ciowe dla tej czÄ™Å›ci globalnego podsumowania: {chunk_input_tokens}")

            response = model.generate_content(prompt)

            if response.parts:
                current_summary_part = "".join([part.text for part in response.parts]).strip()
                overall_summary_parts.append(current_summary_part)

                try:
                    if hasattr(response, '_result') and 'usageMetadata' in response._result:
                        usage_metadata = response._result['usageMetadata']
                        chunk_output_tokens = usage_metadata.get('candidatesTokenCount', 0)
                        print(
                            f"  Wygenerowane tokeny wyjÅ›ciowe dla tej czÄ™Å›ci (z usageMetadata): {chunk_output_tokens}")
                    else:
                        print(
                            "  Brak usageMetadata w odpowiedzi, mimo obecnoÅ›ci treÅ›ci. Oszacowanie tokenÃ³w wyjÅ›ciowych na podstawie tekstu.")
                        chunk_output_tokens = model.count_tokens(current_summary_part).total_tokens
                        print(f"  Oszacowane tokeny wyjÅ›ciowe dla tej czÄ™Å›ci (z tekstu): {chunk_output_tokens}")
                except Exception as usage_e:
                    print(
                        f"BÅÄ„D podczas pobierania usageMetadata dla czÄ™Å›ci globalnego podsumowania {i + 1}: {usage_e}")
                    print("  Oszacowanie tokenÃ³w wyjÅ›ciowych na podstawie wygenerowanego tekstu.")
                    chunk_output_tokens = model.count_tokens(current_summary_part).total_tokens
                    print(f"  Oszacowane tokeny wyjÅ›ciowe dla tej czÄ™Å›ci (z tekstu): {chunk_output_tokens}")
            else:
                print(f"OSTRZEÅ»ENIE: Gemini nie zwrÃ³ciÅ‚o Å¼adnego tekstu dla globalnego podsumowania (czÄ™Å›Ä‡ {i + 1}).")
                if hasattr(response, 'prompt_feedback') and response.prompt_feedback:
                    print(f"  Feedback od Gemini: {response.prompt_feedback}")
                overall_summary_parts.append(f"[BRAK PODSUMOWANIA DLA TEJ CZÄ˜ÅšCI: PROBLEM Z API LUB TREÅšCIÄ„]\n")

            overall_output_tokens += chunk_output_tokens

        except genai.types.BlockedPromptException as e:
            print(f"BÅÄ„D: Globalne podsumowanie zablokowane przez Gemini (safety settings) dla czÄ™Å›ci {i + 1}: {e}")
            overall_summary_parts.append(f"[GLOBALNE PODSUMOWANIE ZABLOKOWANE PRZEZ API]\n")
        except Exception as e:
            print(
                f"KRYTYCZNY BÅÄ„D: Podczas komunikacji z Google Gemini dla globalnego podsumowania (czÄ™Å›Ä‡ {i + 1}): {e}")
            overall_summary_parts.append(f"[GLOBALNE PODSUMOWANIE: BÅÄ„D ANALIZY DLA TEJ CZÄ˜ÅšCI: {e}]\n")

    final_overall_summary = "\n\n".join(overall_summary_parts)
    return final_overall_summary, overall_input_tokens, overall_output_tokens


def write_usage_summary(total_files, total_input_tokens, total_output_tokens, total_duration,
                        overall_summary_input_tokens=0, overall_summary_output_tokens=0):
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file_path = os.path.join(LOG_FOLDER, f"gemini_usage_summary_{timestamp}.txt")

    summary_content = (
        f"--- Podsumowanie Uruchomienia BLOX-TAK-GEMINI ---\n"
        f"Data i czas uruchomienia: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        f"Przetworzone plikÃ³w PDF: {total_files}\n"
        f"UÅ¼yty model Gemini: {MODEL_NAME}\n"
        f"CaÅ‚kowita liczba tokenÃ³w wejÅ›ciowych (prompt - per dokument): {total_input_tokens}\n"
        f"CaÅ‚kowita liczba tokenÃ³w wyjÅ›ciowych (generacja - per dokument): {total_output_tokens}\n"
        f"CaÅ‚kowita liczba tokenÃ³w wejÅ›ciowych (prompt - globalne podsumowanie): {overall_summary_input_tokens}\n"
        f"CaÅ‚kowita liczba tokenÃ³w wyjÅ›ciowych (generacja - globalne podsumowanie): {overall_summary_output_tokens}\n"
        f"ÅÄ…czny czas przetwarzania: {total_duration:.2f} sekund\n"
        f"--------------------------------------------------\n"
        f"UWAGA: JeÅ›li tokeny wyjÅ›ciowe wynoszÄ… 0 lub sÄ… podejrzanie niskie mimo wygenerowanej treÅ›ci, "
        f"moÅ¼e to oznaczaÄ‡ problem z API lub blokadÄ™ treÅ›ci przez filtry bezpieczeÅ„stwa Gemini, "
        f"lub bÅ‚Ä…d w pobieraniu usageMetadata. SprawdÅº logi konsoli powyÅ¼ej dla 'Feedback od Gemini' lub 'Safety Ratings'.\n"
    )

    try:
        with open(log_file_path, "w", encoding="utf-8") as f:
            f.write(summary_content)
        print(f"\n--- Podsumowanie zuÅ¼ycia zapisano do: {log_file_path} ---")
    except Exception as e:
        print(f"BÅÄ„D: Nie moÅ¼na zapisaÄ‡ pliku podsumowania zuÅ¼ycia: {e}")


# --- New function to save text to PDF ---
# --- Nowa funkcja do zapisywania tekstu do PDF ---
def save_text_to_pdf(text_content, output_pdf_path, font_name, font_size):
    print(f"Saving text to PDF: {output_pdf_path} with font '{font_name}' {font_size}pt...")
    print(f"Zapisywanie tekstu do PDF: {output_pdf_path} czcionkÄ… '{font_name}' {font_size}pkt...")
    try:
        c = canvas.Canvas(output_pdf_path, pagesize=A4)
        c.setFont(font_name, font_size)
        width, height = A4

        # Margins / Marginesy
        left_margin = 50
        top_margin = height - 50
        line_height = font_size * 1.2  # Adjust for spacing / Dostosuj do odstÄ™pÃ³w

        y_position = top_margin

        lines = []
        for paragraph in text_content.split('\n'):
            wrapped_lines = []
            if paragraph.strip():
                char_width = pdfmetrics.stringWidth('M', font_name, font_size)
                max_chars_per_line = int((width - 2 * left_margin) / char_width) if char_width > 0 else 100

                current_line = ""
                words = paragraph.split(' ')
                for word in words:
                    if pdfmetrics.stringWidth(current_line + word + ' ', font_name, font_size) < (
                            width - 2 * left_margin):
                        current_line += word + ' '
                    else:
                        wrapped_lines.append(current_line.strip())
                        current_line = word + ' '
                if current_line.strip():
                    wrapped_lines.append(current_line.strip())
            else:
                wrapped_lines.append("")
            lines.extend(wrapped_lines)

        for line in lines:
            if y_position < 50:
                c.showPage()
                c.setFont(font_name, font_size)
                y_position = top_margin

            c.drawString(left_margin, y_position, line)
            y_position -= line_height

        c.save()
        print(f"Successfully saved PDF to: {output_pdf_path}")
        print(f"PomyÅ›lnie zapisano PDF do: {output_pdf_path}")
    except Exception as e:
        print(f"ERROR: Could not save text to PDF '{output_pdf_path}': {e}")
        print(f"BÅÄ„D: Nie moÅ¼na zapisaÄ‡ tekstu do PDF '{output_pdf_path}': {e}")


def process_all_pdfs_with_gemini():
    print(f"Rozpoczynam analizÄ™ plikÃ³w PDF z folderu: {PDF_INPUT_FOLDER}")

    pdf_files = [f for f in os.listdir(PDF_INPUT_FOLDER) if f.lower().endswith(".pdf")]

    if not pdf_files:
        print("INFO: Brak plikÃ³w PDF w folderze do analizy.")
        return

    overall_total_input_tokens = 0
    overall_total_output_tokens = 0
    processed_files_count = 0

    legal_summaries = []
    all_individual_analyses_text = []

    start_time = datetime.datetime.now()

    for pdf_file in pdf_files:
        pdf_path = os.path.join(PDF_INPUT_FOLDER, pdf_file)
        base_name = os.path.splitext(pdf_file)[0]
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        output_txt_path = os.path.join(OUTPUT_FOLDER, f"{base_name}_legal_analysis_{timestamp}.txt")
        output_pdf_path = os.path.join(OUTPUT_FOLDER, f"{base_name}_legal_analysis_{timestamp}.pdf")

        print(f"\n--- Przetwarzanie pliku: {pdf_file} ---")

        num_pages = 0
        try:
            with pdfplumber.open(pdf_path) as pdf:
                num_pages = len(pdf.pages)
        except Exception as e:
            print(
                f"BÅÄ„D: Nie moÅ¼na otworzyÄ‡ pliku PDF '{pdf_file}' w celu sprawdzenia liczby stron: {e}. Pomijam plik.")
            error_msg = f"[BÅÄ„D EKSTRAKCJI TEKSTU Z PDF: {e}]"
            with open(output_txt_path, "w", encoding="utf-8") as f:
                f.write(error_msg)
            save_text_to_pdf(error_msg, output_pdf_path, DEFAULT_FONT, FONT_SIZE)
            continue

        start_p, end_p = get_page_range_input(pdf_file, num_pages)

        extracted_text = extract_selected_pages_from_pdf(pdf_path, start_p, end_p)

        if extracted_text is None:
            print(f"BÅÄ„Z: Pomijam plik '{pdf_file}' z powodu problemÃ³w z ekstrakcjÄ… tekstu.")
            summary_content = f"--- Analiza dla pliku: {pdf_file} ---\n[BÅÄ„D EKSTRAKCJI TEKSTU Z PDF]\n"
            error_msg = "[BÅÄ„D EKSTRAKCJI TEKSTU Z PDF]"
            with open(output_txt_path, "w", encoding="utf-8") as f:
                f.write(error_msg)
            save_text_to_pdf(error_msg, output_pdf_path, DEFAULT_FONT, FONT_SIZE)
            legal_summaries.append(summary_content)
            all_individual_analyses_text.append(summary_content)
            continue

        if not extracted_text:
            print(
                f"INFO: Plik '{pdf_file}' jest pusty lub nie zawiera tekstu po ekstrakcji dla wybranego zakresu. Pomijam analizÄ™.")
            summary_content = f"--- Analiza dla pliku: {pdf_file} ---\n[PLIK PUSTY LUB BEZ TEKSTU DO ANALIZY]\n"
            info_msg = "[PLIK PUSTY LUB BEZ TEKSTU DO ANALIZY]"
            with open(output_txt_path, "w", encoding="utf-8") as f:
                f.write(info_msg)
            save_text_to_pdf(info_msg, output_pdf_path, DEFAULT_FONT, FONT_SIZE)
            legal_summaries.append(summary_content)
            all_individual_analyses_text.append(summary_content)
            continue

        legal_analysis_result, file_input_tokens, file_output_tokens = analyze_text_with_gemini(extracted_text)

        overall_total_input_tokens += file_input_tokens
        overall_total_output_tokens += file_output_tokens
        processed_files_count += 1

        if legal_analysis_result is not None and legal_analysis_result.strip():
            try:
                with open(output_txt_path, "w", encoding="utf-8") as f:
                    f.write(legal_analysis_result)
                print(f"SUKCES: Wynik analizy prawnej zapisano do TXT: {output_txt_path}")

                save_text_to_pdf(legal_analysis_result, output_pdf_path, DEFAULT_FONT, FONT_SIZE)

                summary_content = f"--- Analiza dla pliku: {pdf_file} ---\n" + legal_analysis_result + "\n"
            except Exception as e:
                print(f"BÅÄ„D: Nie moÅ¼na zapisaÄ‡ wyniku analizy dla {pdf_file}: {e}")
                summary_content = f"--- Analiza dla pliku: {pdf_file} ---\n[BÅÄ„D ZAPISU WYNIKU ANALIZY: {e}]\n"
                save_text_to_pdf(summary_content, output_pdf_path, DEFAULT_FONT, FONT_SIZE)
        else:
            print(
                f"OSTRZEÅ»ENIE: Nie uzyskano sensownego wyniku analizy dla '{pdf_file}'. Nie zapisano pliku wyjÅ›ciowego TXT.")
            summary_content = f"--- Analiza dla pliku: {pdf_file} ---\n[NIE UZYSKANO WYNIKU ANALIZY Z GEMINI]\n"
            info_msg = "[NIE UZYSKANO WYNIKU ANALIZY Z GEMINI]"
            with open(output_txt_path, "w", encoding="utf-8") as f:
                f.write(info_msg)
            save_text_to_pdf(info_msg, output_pdf_path, DEFAULT_FONT, FONT_SIZE)

        legal_summaries.append(summary_content)
        all_individual_analyses_text.append(
            legal_analysis_result if legal_analysis_result else summary_content)

        print(f"--- ZuÅ¼ycie tokenÃ³w dla pliku '{pdf_file}' ---")
        print(f"  Tokeny wejÅ›ciowe: {file_input_tokens}")
        print(f"  Tokeny wyjÅ›ciowe: {file_output_tokens}")
        print("-------------------------------------------------")

    end_time = datetime.datetime.now()
    total_processing_duration = (end_time - start_time).total_seconds()

    print("\n--- ZakoÅ„czono przetwarzanie wszystkich plikÃ³w PDF. ---")

    print("\n\n--- ZBIORCZE PODSUMOWANIE PRAWNE DLA KAÅ»DEGO DOKUMENTU ---")
    for summary in legal_summaries:
        print(summary)
    print("----------------------------------------------------------\n")

    combined_analyses_for_overall_summary = "\n\n".join(filter(None, all_individual_analyses_text))
    overall_legal_summary, overall_summary_input_tokens, overall_summary_output_tokens = \
        summarize_overall_legal_findings(combined_analyses_for_overall_summary)

    print("\n\n--- GLOBALNE PODSUMOWANIE PRAWNE (dla wszystkich dokumentÃ³w) ---")
    if overall_legal_summary.strip():
        print(overall_legal_summary)
        global_summary_txt_file_path = os.path.join(OUTPUT_FOLDER,
                                                    f"GLOBAL_LEGAL_SUMMARY_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
        try:
            with open(global_summary_txt_file_path, "w", encoding="utf-8") as f:
                f.write(overall_legal_summary)
            print(f"\nGlobalne podsumowanie zapisano do TXT: {global_summary_txt_file_path}")
        except Exception as e:
            print(f"BÅÄ„D: Nie moÅ¼na zapisaÄ‡ globalnego podsumowania do TXT: {e}")

        global_summary_pdf_file_path = os.path.join(OUTPUT_FOLDER,
                                                    f"GLOBAL_LEGAL_SUMMARY_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf")
        save_text_to_pdf(overall_legal_summary, global_summary_pdf_file_path, DEFAULT_FONT, FONT_SIZE)

    else:
        print("[BRAK GLOBALNEGO PODSUMOWANIA - MOÅ»LIWY PROBLEM Z API LUB BRAK TREÅšCI]")
        save_text_to_pdf("[BRAK GLOBALNEGO PODSUMOWANIA - MOÅ»LIWY PROBLEM Z API LUB BRAK TREÅšCI]",
                         os.path.join(OUTPUT_FOLDER,
                                      f"GLOBAL_LEGAL_SUMMARY_ERROR_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"),
                         DEFAULT_FONT, FONT_SIZE)

    print("------------------------------------------------------------------\n")

    write_usage_summary(processed_files_count, overall_total_input_tokens, overall_total_output_tokens,
                        total_processing_duration, overall_summary_input_tokens, overall_summary_output_tokens)


if __name__ == "__main__":
    process_all_pdfs_with_gemini()
--- END FILE: scripts/analysis-summary.py ---

--- START FILE: scripts/pdf_merger_with_titles.py ---
import os
import pdfplumber
import datetime
from reportlab.lib.pagesizes import A4
from reportlab.pdfgen import canvas
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont

# --- Path Configuration / Konfiguracja Å›cieÅ¼ek ---
# Source folder for PDF files
# Folder ÅºrÃ³dÅ‚owy dla plikÃ³w PDF
PDF_SOURCE_FOLDER = "/home/luke_blue_lox/PycharmProjects/BLOX-TAK-GEMINI/TEMP"
# Destination folder for the output PDF file intended for analysis
# Folder docelowy dla wynikowego pliku PDF przeznaczonego do analizy
OUTPUT_FOR_ANALYSIS_FOLDER = "/home/luke_blue_lox/PycharmProjects/BLOX-TAK-GEMINI/FOR_ANALYSIS"

# Define the path to the Ubuntu Mono font file.
# Ensure 'UbuntuMono-Regular.ttf' is in the same location or provide a full path.
# Zdefiniuj Å›cieÅ¼kÄ™ do pliku czcionki Ubuntu Mono.
# Upewnij siÄ™, Å¼e 'UbuntuMono-Regular.ttf' znajduje siÄ™ w tej samej lokalizacji lub podaj peÅ‚nÄ… Å›cieÅ¼kÄ™.
UBUNTU_MONO_FONT_PATH = os.path.join(os.path.dirname(__file__),
                                     "/home/luke_blue_lox/PycharmProjects/BLOX-TAK-GEMINI/UbuntuMono-Regular.ttf")

# Create folders if they do not exist
# UtwÃ³rz foldery, jeÅ›li nie istniejÄ…
os.makedirs(PDF_SOURCE_FOLDER, exist_ok=True)
os.makedirs(OUTPUT_FOR_ANALYSIS_FOLDER, exist_ok=True)

# Register the Ubuntu Mono font with ReportLab.
# Zarejestruj czcionkÄ™ Ubuntu Mono w ReportLab.
try:
    pdfmetrics.registerFont(TTFont('UbuntuMono', UBUNTU_MONO_FONT_PATH))
    print(f"Font 'UbuntuMono' registered successfully from {UBUNTU_MONO_FONT_PATH}.")
    print(f"Czcionka 'UbuntuMono' zarejestrowana pomyÅ›lnie z {UBUNTU_MONO_FONT_PATH}.")
    DEFAULT_FONT = "UbuntuMono"
except Exception as e:
    print(
        f"ERROR: Could not register Ubuntu Mono font from {UBUNTU_MONO_FONT_PATH}. Please ensure the file exists and is accessible. Error: {e}")
    print(
        f"BÅÄ„D: Nie moÅ¼na zarejestrowaÄ‡ czcionki Ubuntu Mono z {UBUNTU_MONO_FONT_PATH}. Upewnij siÄ™, Å¼e plik istnieje i jest dostÄ™pny. BÅ‚Ä…d: {e}")
    DEFAULT_FONT = "Helvetica"  # Fallback to a default font / Fallback do domyÅ›lnej czcionki

FONT_SIZE = 12  # Font size for PDF output / Rozmiar czcionki dla wyjÅ›cia PDF


# --- Function to extract text from SELECTED PAGES / Funkcja do ekstrakcji tekstu z WYBRANYCH STRON ---
def extract_selected_pages_from_pdf(pdf_path, start_page, end_page):
    """
    Extracts text from a specified page range of a PDF file.
    Ekstrahuje tekst z okreÅ›lonego zakresu stron pliku PDF.

    Args:
        pdf_path (str): Path to the PDF file. / ÅšcieÅ¼ka do pliku PDF.
        start_page (int): Starting page number (1-based). / Numer strony poczÄ…tkowej (od 1).
        end_page (int): Ending page number (inclusive). / Numer strony koÅ„cowej (wÅ‚Ä…cznie).

    Returns:
        str: Extracted text. Returns an empty string if no text, or None on error.
             Wyekstrahowany tekst. Zwraca pusty string, jeÅ›li nie ma tekstu lub None w przypadku bÅ‚Ä™du.
    """
    text = ""
    try:
        with pdfplumber.open(pdf_path) as pdf:
            num_pages = len(pdf.pages)

            # Adjust page indices to Python's 0-based range and ensure they are within bounds
            # Dostosuj indeksy stron do zakresu Pythona (0-based) i upewnij siÄ™, Å¼e sÄ… w granicach
            actual_start_index = max(0, start_page - 1)
            actual_end_index = min(num_pages, end_page)

            if actual_start_index >= num_pages:
                print(
                    f"WARNING: Start page {start_page} exceeds the number of PDF pages ({num_pages}). Returning empty text.")
                print(
                    f"OSTRZEÅ»ENIE: Strona poczÄ…tkowa {start_page} wykracza poza liczbÄ™ stron PDF ({num_pages}). Zwracam pusty tekst.")
                return ""
            if actual_start_index >= actual_end_index:
                print(f"WARNING: Page range ({start_page}-{end_page}) is invalid or empty. Returning empty text.")
                print(
                    f"OSTRZEÅ»ENIE: Zakres stron ({start_page}-{end_page}) jest nieprawidÅ‚owy lub pusty. Zwracam pusty tekst.")
                return ""

            print(
                f"Extracting pages from {actual_start_index + 1} to {actual_end_index} from file '{os.path.basename(pdf_path)}'...")
            print(
                f"Ekstrakcja stron od {actual_start_index + 1} do {actual_end_index} z pliku '{os.path.basename(pdf_path)}'...")

            for i in range(actual_start_index, actual_end_index):
                page = pdf.pages[i]
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
                else:
                    print(f"  INFO: Page {i + 1} is empty or contains no text.")
                    print(f"  INFO: Strona {i + 1} jest pusta lub nie zawiera tekstu.")
    except Exception as e:
        print(f"ERROR: Could not extract text from {pdf_path} for pages {start_page}-{end_page}: {e}")
        print(f"BÅÄ„D: Nie moÅ¼na wyodrÄ™bniÄ‡ tekstu z {pdf_path} dla stron {start_page}-{end_page}: {e}")
        return None
    return text.strip()


# --- Function for interactive page range input / Funkcja do interaktywnego pobierania zakresu stron ---
def get_page_range_input(pdf_file_name, total_pages):
    """
    Interactively prompts the user for the page range to process for a given PDF file.
    Interaktywnie prosi uÅ¼ytkownika o zakres stron do przetworzenia dla danego pliku PDF.

    Args:
        pdf_file_name (str): Name of the PDF file. / Nazwa pliku PDF.
        total_pages (int): Total number of pages in the PDF file. / CaÅ‚kowita liczba stron w pliku PDF.

    Returns:
        tuple: A tuple (start_page, end_page) with the selected range.
               Krotka (start_page, end_page) z wybranym zakresem.
    """
    while True:
        choice = input(f"For file '{pdf_file_name}' (total pages: {total_pages}):\n"
                       f"Dla pliku '{pdf_file_name}' (Å‚Ä…cznie stron: {total_pages}):\n"
                       f"1. Process all pages / PrzetwÃ³rz wszystkie strony\n"
                       f"2. Specify page range (e.g., 10-20) / Podaj zakres stron (np. 10-20)\n"
                       f"Choose an option (1/2): / Wybierz opcjÄ™ (1/2): ").strip()

        if choice == '1':
            return 1, total_pages
        elif choice == '2':
            page_range_str = input("Enter page range (e.g., 10-20): / Podaj zakres stron (np. 10-20): ").strip()
            try:
                start_str, end_str = page_range_str.split('-')
                start_page = int(start_str)
                end_page = int(end_str)
                if 1 <= start_page <= end_page <= total_pages:
                    return start_page, end_page
                else:
                    print(f"ERROR: Invalid page range. Ensure that {1} <= start <= end <= {total_pages}.")
                    print(
                        f"BÅÄ„D: NieprawidÅ‚owy zakres stron. Upewnij siÄ™, Å¼e {1} <= poczÄ…tek <= koniec <= {total_pages}.")
            except ValueError:
                print("ERROR: Invalid format. Use 'START-END' format, e.g., '10-20'.")
                print("BÅÄ„D: NieprawidÅ‚owy format. UÅ¼yj formatu 'START-KONIEC', np. '10-20'.")
        else:
            print("Invalid choice. Please select 1 or 2.")
            print("NieprawidÅ‚owy wybÃ³r. ProszÄ™ wybraÄ‡ 1 lub 2.")


# Function to save text to PDF with titles / Funkcja do zapisywania tekstu do PDF z tytuÅ‚ami
def save_text_to_pdf_with_titles(text_content, output_pdf_path, font_name, font_size):
    """
    Saves the given text content to a PDF file, handling text wrapping and page breaks.
    Zapisuje podany tekst do pliku PDF, z uwzglÄ™dnieniem zawijania tekstu i podziaÅ‚u na strony.

    Args:
        text_content (str): Text content to save. / Tekst do zapisania.
        output_pdf_path (str): Path where the PDF file should be saved. / ÅšcieÅ¼ka, gdzie ma zostaÄ‡ zapisany plik PDF.
        font_name (str): Name of the registered font. / Nazwa zarejestrowanej czcionki.
        font_size (int): Font size. / Rozmiar czcionki.
    """
    print(f"Saving combined text to PDF: {output_pdf_path} with font '{font_name}' {font_size}pt...")
    print(f"Zapisywanie poÅ‚Ä…czonego tekstu do PDF: {output_pdf_path} czcionkÄ… '{font_name}' {font_size}pkt...")
    try:
        c = canvas.Canvas(output_pdf_path, pagesize=A4)
        c.setFont(font_name, font_size)
        width, height = A4

        left_margin = 50  # Left margin / Lewy margines
        top_margin = height - 50  # Top margin / GÃ³rny margines
        line_height = font_size * 1.4  # Larger spacing for readability / WiÄ™kszy odstÄ™p dla czytelnoÅ›ci

        y_position = top_margin

        # Splitting content into lines, considering paragraph breaks
        # Dzielenie zawartoÅ›ci na linie, z uwzglÄ™dnieniem podziaÅ‚u na akapity
        lines = []
        for paragraph in text_content.split('\n'):
            wrapped_lines = []
            if paragraph.strip():
                # Using pdfmetrics.stringWidth for precise text wrapping
                # UÅ¼ywamy pdfmetrics.stringWidth do precyzyjnego zawijania
                current_line = ""
                words = paragraph.split(' ')
                for word in words:
                    # Check if adding the word fits in the current line
                    # SprawdÅº, czy dodanie sÅ‚owa zmieÅ›ci siÄ™ w linii
                    if pdfmetrics.stringWidth(current_line + word + ' ', font_name, font_size) < (
                            width - 2 * left_margin):
                        current_line += word + ' '
                    else:
                        # If not, add the current line and start a new one
                        # JeÅ›li nie, dodaj bieÅ¼Ä…cÄ… liniÄ™ i rozpocznij nowÄ…
                        wrapped_lines.append(current_line.strip())
                        current_line = word + ' '
                if current_line.strip():  # Add any remaining words in the current line
                    # Dodaj pozostaÅ‚e sÅ‚owa w bieÅ¼Ä…cej linii
                    wrapped_lines.append(current_line.strip())
            else:
                wrapped_lines.append("")  # Preserve empty lines for paragraph spacing
                # Zachowaj puste linie dla odstÄ™pÃ³w miÄ™dzy akapitami
            lines.extend(wrapped_lines)

        for line in lines:
            if y_position < 50:  # If reaching the bottom margin, create a new page
                # JeÅ›li dochodzimy do dolnego marginesu, nowa strona
                c.showPage()
                c.setFont(font_name, font_size)
                y_position = top_margin

            c.drawString(left_margin, y_position, line)
            y_position -= line_height

        c.save()
        print(f"Successfully saved combined PDF to: {output_pdf_path}")
        print(f"PomyÅ›lnie zapisano poÅ‚Ä…czony PDF do: {output_pdf_path}")
    except Exception as e:
        print(f"ERROR: Could not save text to PDF '{output_pdf_path}': {e}")
        print(f"BÅÄ„D: Nie moÅ¼na zapisaÄ‡ tekstu do PDF '{output_pdf_path}': {e}")


def merge_pdfs_with_titles():
    """
    Main function to merge PDF files from the source folder.
    It interactively prompts the user for page selection for each file,
    then combines the extracted text into a single PDF document.
    GÅ‚Ã³wna funkcja do Å‚Ä…czenia plikÃ³w PDF z folderu ÅºrÃ³dÅ‚owego.
    Interaktywnie prosi uÅ¼ytkownika o wybÃ³r stron dla kaÅ¼dego pliku,
    a nastÄ™pnie Å‚Ä…czy wyekstrahowany tekst w jeden dokument PDF.
    """
    print(f"Starting extraction and merging of PDF files from folder: {PDF_SOURCE_FOLDER}")
    print(f"Rozpoczynam ekstrakcjÄ™ i Å‚Ä…czenie plikÃ³w PDF z folderu: {PDF_SOURCE_FOLDER}")

    # Get a list of PDF files from the source folder, sorted alphabetically
    # Pobierz listÄ™ plikÃ³w PDF z folderu ÅºrÃ³dÅ‚owego, posortowanych alfabetycznie
    pdf_files = sorted([f for f in os.listdir(PDF_SOURCE_FOLDER) if f.lower().endswith(".pdf")])

    if not pdf_files:
        print("INFO: No PDF files found in the source folder for processing.")
        print("INFO: Brak plikÃ³w PDF w folderze ÅºrÃ³dÅ‚owym do przetworzenia.")
        return

    combined_text = []  # List to store extracted text from all files
    # Lista do przechowywania wyekstrahowanego tekstu z wszystkich plikÃ³w

    for pdf_file in pdf_files:
        pdf_path = os.path.join(PDF_SOURCE_FOLDER, pdf_file)
        file_title = os.path.splitext(pdf_file)[0]  # File title without extension
        # TytuÅ‚ pliku bez rozszerzenia

        print(f"\n--- Processing file: {pdf_file} ---")
        print(f"\n--- Przetwarzanie pliku: {pdf_file} ---")

        num_pages = 0
        try:
            with pdfplumber.open(pdf_path) as pdf:
                num_pages = len(pdf.pages)
        except Exception as e:
            print(f"ERROR: Could not open PDF file '{pdf_file}' to check page count: {e}. Skipping file.")
            print(
                f"BÅÄ„D: Nie moÅ¼na otworzyÄ‡ pliku PDF '{pdf_file}' w celu sprawdzenia liczby stron: {e}. Pomijam plik.")
            combined_text.append(f"\n\n--- FILE SKIPPED (PDF OPEN ERROR): {file_title} ---\n\n")
            combined_text.append(f"\n\n--- PLIK POMINIÄ˜TY (BÅÄ„D OTWARCIA PDF): {file_title} ---\n\n")
            continue

        start_p, end_p = get_page_range_input(pdf_file, num_pages)  # Get page range from user
        # Pobierz zakres stron od uÅ¼ytkownika
        extracted_text = extract_selected_pages_from_pdf(pdf_path, start_p, end_p)  # Extract text
        # Wyekstrahuj tekst

        if extracted_text is not None:
            # Add file title as a separator before and after its content
            # Dodaj tytuÅ‚ pliku jako separator przed i po jego zawartoÅ›ci
            combined_text.append(f"\n\n--- BEGINNING OF FILE: {file_title} ---\n\n")
            combined_text.append(f"\n\n--- POCZÄ„TEK PLIKU: {file_title} ---\n\n")
            combined_text.append(extracted_text)
            combined_text.append(f"\n\n--- END OF FILE: {file_title} ---\n\n")
            combined_text.append(f"\n\n--- KONIEC PLIKU: {file_title} ---\n\n")
        else:
            print(f"WARNING: Could not extract text from '{pdf_file}'. Skipping this file.")
            print(f"OSTRZEÅ»ENIE: Nie moÅ¼na wyodrÄ™bniÄ‡ tekstu z '{pdf_file}'. Pomijam ten plik.")
            combined_text.append(f"\n\n--- FILE SKIPPED (EXTRACTION ERROR): {file_title} ---\n\n")
            combined_text.append(f"\n\n--- PLIK POMINIÄ˜TY (BÅÄ„D EKSTRAKCJI): {file_title} ---\n\n")

    if combined_text:
        final_combined_text = "\n".join(combined_text)  # Join all text fragments into a single string
        # PoÅ‚Ä…cz wszystkie fragmenty tekstu w jeden string
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        output_pdf_path = os.path.join(OUTPUT_FOR_ANALYSIS_FOLDER, f"Combined_Legal_Documents_{timestamp}.pdf")

        # Save the combined text to a PDF file
        # Zapisz poÅ‚Ä…czony tekst do pliku PDF
        save_text_to_pdf_with_titles(final_combined_text, output_pdf_path, DEFAULT_FONT, FONT_SIZE)
        print(f"\n--- Finished merging all PDF files to: {output_pdf_path} ---")
        print(f"\n--- ZakoÅ„czono Å‚Ä…czenie wszystkich plikÃ³w PDF do: {output_pdf_path} ---")
    else:
        print("\nNo text extracted due to errors or missing files.")
        print("\nNie wyodrÄ™bniono Å¼adnego tekstu ze wzglÄ™du na bÅ‚Ä™dy lub brak plikÃ³w.")


if __name__ == "__main__":
    # Entry point of the program. Calls the PDF merging function.
    # Punkt wejÅ›cia programu. WywoÅ‚uje funkcjÄ™ Å‚Ä…czenia PDF-Ã³w.
    merge_pdfs_with_titles()

--- END FILE: scripts/pdf_merger_with_titles.py ---

--- START FILE: scripts/gemini_corrector.py ---
import uno
import unohelper
import json
import http.client
import yaml

with open("config.yaml", "r") as cr:
    config_vals = yaml.full_load(cr)
KEY = config_vals['KEY']

# Main macro function to correct the entire document text using Google Gemini
# GÅ‚Ã³wna funkcja makra do poprawiania caÅ‚ego tekstu dokumentu za pomocÄ… Google Gemini
def correct_entire_document_text_google(*args):
    # Get the current document
    # Pobierz bieÅ¼Ä…cy dokument
    desktop = XSCRIPTCONTEXT.getDesktop()
    model = desktop.getCurrentComponent()
    if not model:
        print("ERROR: No document open.")
        print("BÅÄ„D: Brak otwartego dokumentu.")
        return

    # Get the entire text object of the document
    # Pobierz obiekt caÅ‚ego tekstu dokumentu
    text_object = model.Text
    if not text_object:
        print("ERROR: Could not get the document's text object.")
        print("BÅÄ„D: Nie moÅ¼na uzyskaÄ‡ obiektu tekstu dokumentu.")
        return

    # Get the full text content from the document
    # Pobierz peÅ‚nÄ… zawartoÅ›Ä‡ tekstowÄ… z dokumentu
    full_document_text = text_object.getString()

    if not full_document_text.strip():
        print("INFO: The current document is empty or contains only whitespace. Nothing to correct.")
        print("INFO: BieÅ¼Ä…cy dokument jest pusty lub zawiera tylko biaÅ‚e znaki. Brak tekstu do poprawy.")
        return

    print(f"Text to correct (first 100 characters): {full_document_text[:100]}...")
    print(f"Tekst do poprawy (pierwsze 100 znakÃ³w): {full_document_text[:100]}...")

    # --- GEMINI API KEY CONFIGURATION ---
    # --- KONFIGURACJA KLUCZA API GEMINI ---
    # IMPORTANT: Replace "*****" with your real API key in config.yaml!
    # WAÅ»NE: ZastÄ…p "*****" swoim prawdziwym kluczem API w pliku config.yaml!
    GOOGLE_API_KEY = KEY
    print("Gemini API configured successfully.")
    print("Gemini API skonfigurowane pomyÅ›lnie.")

    API_HOST = "generativelanguage.googleapis.com"
    MODEL_NAME = "gemini-1.5-flash-8b"

    # Prepare the prompt for Gemini
    # Przygotuj prompt dla Gemini
    prompt_content = (
        f"Correct spelling, grammar, and stylistic errors in the following Polish text. "
        f"Ensure the text is grammatically correct and sounds natural in Polish. "
        f"Return only the corrected text, without additional comments. Text to correct:\n\n{full_document_text}"
    )
    prompt_content_pl = (
        f"Popraw bÅ‚Ä™dy ortograficzne, gramatyczne i stylistyczne w poniÅ¼szym tekÅ›cie. "
        f"Upewnij siÄ™, Å¼e tekst jest poprawny jÄ™zykowo i naturalnie brzmiÄ…cy po polsku. "
        f"ZwrÃ³Ä‡ tylko poprawiony tekst, bez dodatkowych komentarzy. Tekst do poprawy:\n\n{full_document_text}"
    )

    # Structure of the request body for Gemini API
    # Struktura ciaÅ‚a Å¼Ä…dania dla API Gemini
    request_body = {
        "contents": [
            {
                "parts": [
                    {"text": prompt_content_pl}  # Using the Polish prompt version
                ]
            }
        ],
        "generationConfig": {
            "temperature": 0.7,
            "topK": 40,
            "topP": 0.95,
            "maxOutputTokens": 8192,
        }
    }

    try:
        # Connect to Gemini API
        # PoÅ‚Ä…cz siÄ™ z API Gemini
        conn = http.client.HTTPSConnection(API_HOST)
        headers = {
            'Content-Type': 'application/json',
            'x-goog-api-key': GOOGLE_API_KEY
        }
        endpoint = f"/v1beta/models/{MODEL_NAME}:generateContent"
        body = json.dumps(request_body)

        print(f"Sending request to {API_HOST}{endpoint}...")
        print(f"WysyÅ‚am zapytanie do {API_HOST}{endpoint}...")

        conn.request("POST", endpoint, body=body, headers=headers)
        response = conn.getresponse()
        response_data = response.read().decode('utf-8')
        conn.close()

        print(f"Gemini API Response Status: {response.status}")
        print(f"Status odpowiedzi API Gemini: {response.status}")
        print(f"Full Gemini API Response (for diagnostics): {response_data}")
        print(f"PeÅ‚na odpowiedÅº API Gemini (do diagnostyki): {response_data}")

        # Check if the response is successful
        # SprawdÅº, czy odpowiedÅº jest pomyÅ›lna
        if response.status == 200:
            result = json.loads(response_data)
            corrected_text = ""

            # Extract the corrected text from the response
            # WyodrÄ™bnij poprawiony tekst z odpowiedzi
            if 'candidates' in result and len(result['candidates']) > 0 \
                    and 'content' in result['candidates'][0] \
                    and 'parts' in result['candidates'][0]['content'] \
                    and len(result['candidates'][0]['content']['parts']) > 0 \
                    and 'text' in result['candidates'][0]['content']['parts'][0]:

                corrected_text = result['candidates'][0]['content']['parts'][0]['text'].strip()

                if corrected_text:
                    # Replace the entire document text with the corrected text
                    # ZastÄ…p caÅ‚y tekst dokumentu poprawionym tekstem
                    text_object.setString(corrected_text)
                    print("SUCCESS: Entire document text corrected successfully by Google Gemini!")
                    print("SUKCES: CaÅ‚y tekst dokumentu poprawiony pomyÅ›lnie przez Google Gemini!")
                else:
                    print("WARNING: Gemini returned an empty corrected text.")
                    print("OSTRZEÅ»ENIE: Gemini zwrÃ³ciÅ‚o pusty poprawiony tekst.")
            else:
                print("ERROR: Invalid Gemini API response structure (missing 'candidates' or 'content').")
                print("BÅÄ„D: NieprawidÅ‚owa struktura odpowiedzi API Gemini (brak 'candidates' lub 'content').")

            # Display token usage
            # WyÅ›wietl zuÅ¼ycie tokenÃ³w
            if 'usageMetadata' in result:
                usage = result['usageMetadata']
                usage_msg = (
                    f"Model: {result.get('modelVersion', 'Unknown version')}\n"
                    f"Input Tokens: {usage.get('promptTokenCount', 0)}\n"
                    f"Output Tokens: {usage.get('candidatesTokenCount', 0)}\n"
                    f"Total Tokens: {usage.get('totalTokenCount', 0)}"
                )
                print(f"--- Gemini Resource Usage ---\n{usage_msg}")
                print(f"--- ZuÅ¼ycie zasobÃ³w Gemini ---\n{usage_msg}")
            else:
                print("INFO: No token usage information in Gemini response.")
                print("INFO: Brak informacji o zuÅ¼yciu tokenÃ³w w odpowiedzi Gemini.")

        else:
            print(f"ERROR: Google Gemini server error: HTTP {response.status} - {response_data}")
            print(f"BÅÄ„D: BÅ‚Ä…d serwera Google Gemini: HTTP {response.status} - {response_data}")

    except Exception as e:
        print(f"CRITICAL ERROR: During communication with Google Gemini: {e}")
        print(f"KRYTYCZNY BÅÄ„D: Podczas komunikacji z Google Gemini: {e}")


# Register the macro for LibreOffice
# Rejestracja makra dla LibreOffice
g_exportedScripts = correct_entire_document_text_google,
--- END FILE: scripts/gemini_corrector.py ---

--- START FILE: scripts/universal_content_merger.py ---
# For filesystem operations like creating paths and folders. / Do operacji na systemie plikÃ³w, jak tworzenie Å›cieÅ¼ek i folderÃ³w.
import os
# For opening and extracting text from PDF files, and converting pages to images. / Do otwierania i wyciÄ…gania tekstu z plikÃ³w PDF oraz konwertowania stron na obrazy.
import pdfplumber
# For generating unique timestamps for filenames. / Do generowania unikalnych znacznikÃ³w czasu dla nazw plikÃ³w.
import datetime
# For loading configuration files in YAML format. / Do wczytywania plikÃ³w konfiguracyjnych w formacie YAML.
import yaml
# For system interaction, e.g., to exit the script. / Do interakcji z systemem, np. do przerwania dziaÅ‚ania skryptu.
import sys
# For creating temporary files to handle image conversion for OCR. / Do tworzenia plikÃ³w tymczasowych do obsÅ‚ugi konwersji obrazÃ³w dla OCR.
import tempfile
# For using regular expressions to find dates in filenames. / Do uÅ¼ywania wyraÅ¼eÅ„ regularnych w celu znalezienia dat w nazwach plikÃ³w.
import re
# For structured logging in JSON format. / Do strukturalnego logowania w formacie JSON.
import json
# For timing operations. / Do mierzenia czasu operacji.
import time
# The official Google library for interacting with the Gemini API. / Oficjalna biblioteka Google do interakcji z API Gemini.
import google.generativeai as genai

# ReportLab imports for advanced text wrapping and PDF creation
# Importy ReportLab do zaawansowanego zawijania tekstu i tworzenia PDF
from reportlab.lib.pagesizes import A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.pdfbase import pdfmetrics
from reportlab.lib.enums import TA_JUSTIFY, TA_LEFT


# --- MAIN CONFIGURATION LOADING FUNCTION ---
# --- GÅÃ“WNA FUNKCJA ÅADUJÄ„CA KONFIGURACJÄ˜ ---
def load_configuration(config_path='config.yaml'):
    """
    Loads configuration from a YAML file.
    Wczytuje konfiguracjÄ™ z pliku YAML.
    """
    try:
        with open(config_path, "r", encoding="utf-8") as cr:
            config = yaml.full_load(cr)

        if 'base_path' not in config:
            raise ValueError("Key 'base_path' is required in the configuration file.")
        if 'merger_script_config' not in config:
            raise ValueError("Section 'merger_script_config' is required in the configuration file.")
        if 'KEY' not in config or not config['KEY'] or config['KEY'] == "TWOJ_KLUCZ_API_GEMINI" or config[
            'KEY'] == "*****":
            raise ValueError("Gemini API Key ('KEY') not found, is empty, or is a placeholder.")

        base_path = config['base_path']
        merger_config = config['merger_script_config']

        conf = {
            "GEMINI_API_KEY": config['KEY'],
            "SOURCE_FOLDER": os.path.join(base_path, merger_config.get('source_folder', 'TEMP')),
            "OUTPUT_FOLDER": os.path.join(base_path, merger_config.get('output_folder', 'FOR_ANALYSIS')),
            "LOG_FOLDER": os.path.join(base_path, merger_config.get('log_folder', 'LOGS')),
            "FONT_PATH": os.path.join(base_path, merger_config.get('font_path', 'UbuntuMono-Regular.ttf')),
            "FONT_NAME": merger_config.get('font_name', 'UbuntuMono'),
            "FONT_SIZE": merger_config.get('font_size', 10),
            "MODEL_NAME": merger_config.get('model_name', 'gemini-1.5-flash'),
            "OCR_PROMPT": merger_config.get('ocr_prompt',
                                            'GEMINI, Make OCR. Do not add any additional information, just the text.'),
            "AUDIO_PROMPT": merger_config.get('audio_prompt',
                                              'Transcribe the audio recording. Return only the final text.'),
            "OCR_RESOLUTION": merger_config.get('ocr_resolution', 150)
        }

        print("Configuration loaded successfully.")
        print("Konfiguracja zaÅ‚adowana pomyÅ›lnie.")
        return conf

    except (FileNotFoundError, ValueError, KeyError) as e:
        print(f"FATAL ERROR in configuration: {e}")
        print(f"BÅÄ„D KRYTYCZNY w konfiguracji: {e}")
        return None


# --- Initialization of Configuration and Services ---
# --- Inicjalizacja Konfiguracji i UsÅ‚ug ---
CONFIG = load_configuration()
MODEL = None

if CONFIG:
    try:
        genai.configure(api_key=CONFIG['GEMINI_API_KEY'])
        MODEL = genai.GenerativeModel(CONFIG['MODEL_NAME'])
        print(f"Gemini API configured successfully with model: {CONFIG['MODEL_NAME']}")
        print(f"Gemini API skonfigurowane pomyÅ›lnie z modelem: {CONFIG['MODEL_NAME']}")
    except Exception as e:
        print(f"ERROR: Could not configure Gemini API: {e}")
        CONFIG = None

    if CONFIG:
        os.makedirs(CONFIG['SOURCE_FOLDER'], exist_ok=True)
        os.makedirs(CONFIG['OUTPUT_FOLDER'], exist_ok=True)
        os.makedirs(CONFIG['LOG_FOLDER'], exist_ok=True)
        try:
            pdfmetrics.registerFont(TTFont(CONFIG['FONT_NAME'], CONFIG['FONT_PATH']))
            print(f"Font '{CONFIG['FONT_NAME']}' registered successfully.")
            print(f"Czcionka '{CONFIG['FONT_NAME']}' zarejestrowana pomyÅ›lnie.")
        except Exception as e:
            print(
                f"ERROR: Could not register font from path '{CONFIG['FONT_PATH']}'. Defaulting to Helvetica. Error: {e}")
            CONFIG['FONT_NAME'] = "Helvetica"
else:
    print("Exiting script due to configuration errors.")
    sys.exit(1)

# --- File Type Constants ---
# --- StaÅ‚e typÃ³w plikÃ³w ---
SUPPORTED_PDF_EXT = ('.pdf',)
SUPPORTED_TXT_EXT = ('.txt',)
SUPPORTED_IMG_EXT = ('.png', '.jpg', '.jpeg', '.webp', '.bmp')
SUPPORTED_AUDIO_EXT = ('.mp3', '.wav', '.m4a', '.flac', '.ogg')


# --- Text Extraction Functions ---
# --- Funkcje Ekstrakcji Tekstu ---
def extract_text_from_text_pdf(pdf_path, start_page, end_page):
    text = ""
    try:
        with pdfplumber.open(pdf_path) as pdf:
            num_pages = len(pdf.pages)
            actual_start_index = max(0, start_page - 1)
            actual_end_index = min(num_pages, end_page)

            if actual_start_index >= actual_end_index: return ""

            print(f"Extracting text from pages {actual_start_index + 1} to {actual_end_index}...")
            print(f"Ekstrakcja tekstu ze stron od {actual_start_index + 1} do {actual_end_index}...")
            for i in range(actual_start_index, actual_end_index):
                page = pdf.pages[i]
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
    except Exception as e:
        print(f"ERROR reading text-based PDF {pdf_path}: {e}")
        return f"[BÅÄ„D ODCZYTU PLIKU PDF: {e}]", None
    return text.strip(), {}


def extract_text_from_scanned_pdf(pdf_path, start_page, end_page):
    all_pages_text = []
    total_token_usage = {'prompt_token_count': 0, 'candidates_token_count': 0, 'total_token_count': 0}
    try:
        with pdfplumber.open(pdf_path) as pdf:
            num_pages = len(pdf.pages)
            actual_start_index = max(0, start_page - 1)
            actual_end_index = min(num_pages, end_page)

            if actual_start_index >= actual_end_index: return "", {}

            print(
                f"PDF appears to be a scan. Performing OCR on pages {actual_start_index + 1} to {actual_end_index}...")
            print(f"PDF wyglÄ…da na skan. WykonujÄ™ OCR na stronach od {actual_start_index + 1} do {actual_end_index}...")

            for i in range(actual_start_index, actual_end_index):
                page = pdf.pages[i]
                img = page.to_image(resolution=CONFIG.get("OCR_RESOLUTION", 150))

                with tempfile.NamedTemporaryFile(suffix=".png", delete=True) as temp_image:
                    img.save(temp_image.name, format="PNG")
                    page_text, usage = extract_text_from_image_with_gemini(temp_image.name)
                    all_pages_text.append(page_text)
                    if usage:
                        total_token_usage['prompt_token_count'] += usage.get('prompt_token_count', 0)
                        total_token_usage['candidates_token_count'] += usage.get('candidates_token_count', 0)
                        total_token_usage['total_token_count'] += usage.get('total_token_count', 0)
    except Exception as e:
        print(f"ERROR during scanned PDF processing {pdf_path}: {e}")
        return f"[BÅÄ„D PRZETWARZANIA ZESKANOWANEGO PDF: {e}]", None

    return "\n\n--- Page Break ---\n\n".join(all_pages_text), total_token_usage


def extract_text_from_txt(txt_path):
    print(f"Reading text file: {os.path.basename(txt_path)}...")
    print(f"OdczytujÄ™ plik tekstowy: {os.path.basename(txt_path)}...")
    try:
        with open(txt_path, 'r', encoding='utf-8') as f:
            return f.read(), {}
    except Exception as e:
        print(f"ERROR reading TXT {txt_path}: {e}")
        return f"[BÅÄ„D ODCZYTU PLIKU TXT: {e}]", None


def extract_text_from_image_with_gemini(image_path):
    if not MODEL: return "[OCR ERROR: Gemini API is not configured]", None

    print(f"Performing OCR on image: {os.path.basename(image_path)}...")
    print(f"Przeprowadzam OCR na obrazie: {os.path.basename(image_path)}...")
    try:
        image_file = genai.upload_file(path=image_path)
        response = MODEL.generate_content([CONFIG['OCR_PROMPT'], image_file])
        genai.delete_file(image_file.name)
        usage_metadata = getattr(response, 'usage_metadata', None)
        usage_dict = {}
        if usage_metadata:
            usage_dict = {
                'prompt_token_count': usage_metadata.prompt_token_count,
                'candidates_token_count': usage_metadata.candidates_token_count,
                'total_token_count': usage_metadata.total_token_count
            }
        return response.text.strip(), usage_dict
    except Exception as e:
        print(f"ERROR during OCR on {image_path}: {e}")
        return f"[BÅÄ„D OCR GEMINI: {e}]", None


def extract_text_from_audio_with_gemini(audio_path):
    if not MODEL: return "[TRANSCRIPTION ERROR: Gemini API is not configured]", None

    print(f"Transcribing audio file: {os.path.basename(audio_path)}...")
    print(f"Przeprowadzam transkrypcjÄ™ pliku audio: {os.path.basename(audio_path)}...")
    audio_file = None
    try:
        audio_file = genai.upload_file(path=audio_path)
        response = MODEL.generate_content([CONFIG['AUDIO_PROMPT'], audio_file])
        usage_metadata = getattr(response, 'usage_metadata', None)
        usage_dict = {}
        if usage_metadata:
            usage_dict = {
                'prompt_token_count': usage_metadata.prompt_token_count,
                'candidates_token_count': usage_metadata.candidates_token_count,
                'total_token_count': usage_metadata.total_token_count
            }
        return response.text.strip(), usage_dict
    except Exception as e:
        print(f"ERROR during audio transcription of {audio_path}: {e}")
        return f"[BÅÄ„D TRANSKRYPCJI AUDIO: {e}]", None
    finally:
        if audio_file:
            genai.delete_file(audio_file.name)
            print(f"Cleaned up temporary audio file: {audio_file.name}")
            print(f"PosprzÄ…tano tymczasowy plik audio: {audio_file.name}")


# --- Helper Functions and Main Logic ---
# --- Funkcje pomocnicze i gÅ‚Ã³wna logika ---
def extract_date_from_path(file_path):
    filename = os.path.basename(file_path)
    match = re.search(r'(\d{4}[-_]?\d{2}[-_]?\d{2})', filename)
    if match:
        date_str = match.group(1).replace('-', '').replace('_', '')
        try:
            return datetime.datetime.strptime(date_str, '%Y%m%d')
        except ValueError:
            pass
    try:
        return datetime.datetime.fromtimestamp(os.path.getmtime(file_path))
    except OSError:
        return datetime.datetime.min


def get_page_range_input(pdf_file_name, total_pages):
    while True:
        choice = input(f"For file '{pdf_file_name}' (total pages: {total_pages}):\n"
                       f"  1. Process all pages\n"
                       f"  2. Specify page range (e.g., 10-20)\n"
                       f"Dla pliku '{pdf_file_name}' (Å‚Ä…cznie stron: {total_pages}):\n"
                       f"  1. PrzetwÃ³rz wszystkie strony\n"
                       f"  2. Podaj zakres stron (np. 10-20)\n"
                       f"Choose an option (1/2) / Wybierz opcjÄ™ (1/2): ").strip()
        if choice == '1':
            return 1, total_pages
        elif choice == '2':
            page_range_str = input("Enter page range / Podaj zakres stron: ").strip()
            try:
                start, end = map(int, page_range_str.split('-'))
                if 1 <= start <= end <= total_pages:
                    return start, end
                else:
                    print("ERROR: Invalid page range.")
            except ValueError:
                print("ERROR: Invalid format. Use 'START-END'.")
        else:
            print("Invalid choice.")


# --- NEW HELPER FUNCTION TO FIX WRAPPING ---
# --- NOWA FUNKCJA POMOCNICZA DO NAPRAWY ZAWIJANIA ---
def reflow_text(text: str) -> str:
    """
    Intelligently joins lines that have been incorrectly broken mid-sentence.
    Inteligentnie Å‚Ä…czy linie, ktÃ³re zostaÅ‚y nieprawidÅ‚owo zÅ‚amane w poÅ‚owie zdania.
    """
    # Step 1: Replace single newlines (likely incorrect breaks) with a space. / Krok 1: ZamieÅ„ pojedyncze znaki nowej linii (prawdopodobnie nieprawidÅ‚owe zÅ‚amania) na spacjÄ™.
    reflowed = re.sub(r'(?<!\n)\n(?!\n)', ' ', text)

    # Step 2: Replace two or more newlines (which signify a paragraph break) with a <br/> tag understood by ReportLab. / Krok 2: ZamieÅ„ dwa (lub wiÄ™cej) znaki nowej linii (ktÃ³re oznaczajÄ… koniec akapitu) na tag <br/> zrozumiaÅ‚y dla ReportLab.
    reflowed = re.sub(r'\n{2,}', '<br/><br/>', reflowed)

    return reflowed


# --- MODIFIED PDF SAVING FUNCTION ---
# --- ZMODYFIKOWANA FUNKCJA ZAPISU DO PDF ---
def save_text_to_pdf(text_content, output_pdf_path):
    """
    Saves the given text content to a PDF file with proper text wrapping.
    Zapisuje podany tekst do pliku PDF z poprawnym zawijaniem tekstu.
    """
    font_name = CONFIG['FONT_NAME']
    font_size = CONFIG.get('FONT_SIZE', 10)
    print(f"Saving PDF with proper wrapping to: {output_pdf_path}...")
    print(f"ZapisujÄ™ PDF z poprawnym zawijaniem do: {output_pdf_path}...")

    # Setup document template. / Ustawienie szablonu dokumentu.
    doc = SimpleDocTemplate(output_pdf_path, pagesize=A4,
                            rightMargin=72, leftMargin=72,
                            topMargin=72, bottomMargin=72)

    # Setup paragraph styles. / Ustawienie stylÃ³w akapitu.
    styles = getSampleStyleSheet()
    style = ParagraphStyle(
        name='Normal_Justified',
        parent=styles['Normal'],
        fontName=font_name,
        fontSize=font_size,
        leading=font_size * 1.5,
        alignment=TA_JUSTIFY,  # Justify text for a cleaner look. / Justowanie tekstu dla czystszego wyglÄ…du.
    )

    # *** KEY CHANGE: Call the `reflow_text` function before creating the Paragraph. ***
    # *** KLUCZOWA ZMIANA: WywoÅ‚anie funkcji `reflow_text` przed utworzeniem akapitu. ***
    processed_text = reflow_text(text_content)

    story = [Paragraph(processed_text, style)]

    # Build the PDF. / Zbuduj (wygeneruj) PDF.
    try:
        doc.build(story)
        print(f"Successfully saved PDF: {output_pdf_path}")
        print(f"PomyÅ›lnie zapisano PDF: {output_pdf_path}")
    except Exception as e:
        print(f"ERROR saving PDF: {e}")
        print(f"BÅÄ„D podczas zapisywania PDF: {e}")


def write_summary_log(log_data):
    """
    Writes a JSON summary log for the entire session.
    Zapisuje podsumowujÄ…cy log JSON dla caÅ‚ej sesji.
    """
    log_file_path = os.path.join(CONFIG['LOG_FOLDER'], f"merger_log_{log_data['session_start_iso']}.json")
    try:
        with open(log_file_path, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, indent=4, ensure_ascii=False)
        print("\n--- Summary log created successfully. ---")
        print(f"--- PodsumowujÄ…cy log zostaÅ‚ pomyÅ›lnie utworzony: {log_file_path} ---")
    except Exception as e:
        print(f"ERROR: Could not write summary log file: {e}")
        print(f"BÅÄ„D: Nie moÅ¼na zapisaÄ‡ pliku z logami: {e}")


def main():
    """
    Main function to merge content from PDF, TXT, image and audio files.
    GÅ‚Ã³wna funkcja do Å‚Ä…czenia treÅ›ci z plikÃ³w PDF, TXT, obrazÃ³w i audio.
    """
    script_start_time = time.time()
    session_start_iso = datetime.datetime.now().isoformat().replace(":", "-")

    print(f"Starting content merge from folder: {CONFIG['SOURCE_FOLDER']}")
    print(f"Rozpoczynam Å‚Ä…czenie treÅ›ci z folderu: {CONFIG['SOURCE_FOLDER']}")

    all_files = []
    supported_extensions = SUPPORTED_PDF_EXT + SUPPORTED_TXT_EXT + SUPPORTED_IMG_EXT + SUPPORTED_AUDIO_EXT
    for dirpath, _, filenames in os.walk(CONFIG['SOURCE_FOLDER']):
        for filename in filenames:
            if filename.lower().endswith(supported_extensions):
                all_files.append(os.path.join(dirpath, filename))

    print(f"Sorting {len(all_files)} files by date (newest first)...")
    print(f"SortujÄ™ {len(all_files)} plikÃ³w wedÅ‚ug daty (od najnowszych)...")
    all_files.sort(key=extract_date_from_path, reverse=True)

    if not all_files:
        print("INFO: No supported files found for processing.")
        print("INFO: Nie znaleziono obsÅ‚ugiwanych plikÃ³w do przetworzenia.")
        return

    combined_text_parts = []
    session_log_details = []

    for file_path in all_files:
        file_start_time = time.time()
        file_name = os.path.basename(file_path)

        file_log = {"file_name": file_name, "full_path": file_path, "status": "Processing", "details": ""}
        print(f"\n--- Processing file: {file_name} ---")
        print(f"\n--- Przetwarzam plik: {file_name} ---")

        combined_text_parts.append(f"\n\n--- BEGINNING OF FILE: {file_name} ---\n\n")
        combined_text_parts.append(f"--- POCZÄ„TEK PLIKU: {file_name} ---\n\n")

        extracted_text = ""
        token_usage = {}
        error_message = None

        try:
            if file_name.lower().endswith(SUPPORTED_PDF_EXT):
                with pdfplumber.open(file_path) as pdf:
                    num_pages = len(pdf.pages)
                start_p, end_p = get_page_range_input(file_name, num_pages)
                extracted_text, token_usage = extract_text_from_text_pdf(file_path, start_p, end_p)
                if not extracted_text.strip():
                    extracted_text, token_usage = extract_text_from_scanned_pdf(file_path, start_p, end_p)
            elif file_name.lower().endswith(SUPPORTED_TXT_EXT):
                extracted_text, token_usage = extract_text_from_txt(file_path)
            elif file_name.lower().endswith(SUPPORTED_IMG_EXT):
                extracted_text, token_usage = extract_text_from_image_with_gemini(file_path)
            elif file_name.lower().endswith(SUPPORTED_AUDIO_EXT):
                extracted_text, token_usage = extract_text_from_audio_with_gemini(file_path)

            file_log['status'] = "Success"
        except Exception as e:
            error_message = str(e)
            extracted_text = f"[BÅÄ„D PRZETWARZANIA PLIKU: {error_message}]"
            file_log['status'] = "ERROR"
            file_log['details'] = error_message

        combined_text_parts.append(extracted_text)
        combined_text_parts.append(f"\n\n--- END OF FILE: {file_name} ---\n\n")
        combined_text_parts.append(f"--- KONIEC PLIKU: {file_name} ---\n\n")

        file_log['processing_time_seconds'] = round(time.time() - file_start_time, 2)
        if token_usage:
            file_log['token_usage'] = {
                'prompt': token_usage.get('prompt_token_count', 0),
                'candidates': token_usage.get('candidates_token_count', 0),
                'total': token_usage.get('total_token_count', 0)
            }
        session_log_details.append(file_log)

    script_end_time = time.time()
    if combined_text_parts:
        final_text = "".join(combined_text_parts)
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        output_pdf_path = os.path.join(CONFIG['OUTPUT_FOLDER'], f"Combined_Content_{timestamp}.pdf")

        # Calling the modified save function. / WywoÅ‚anie zmodyfikowanej funkcji zapisu.
        save_text_to_pdf(final_text, output_pdf_path)

        print("\n--- Finished merging all content. ---")
        print("\n--- ZakoÅ„czono Å‚Ä…czenie wszystkich treÅ›ci. ---")
    else:
        print("\nNo text was extracted from any file.")
        print("\nNie wyekstrahowano tekstu z Å¼adnego pliku.")

    overall_log = {
        "session_start_iso": session_start_iso,
        "total_duration_seconds": round(script_end_time - script_start_time, 2),
        "total_files_processed": len(session_log_details),
        "processed_files": session_log_details
    }
    write_summary_log(overall_log)


if __name__ == "__main__":
    if CONFIG:
        main()
--- END FILE: scripts/universal_content_merger.py ---

--- START FILE: scripts/pdf_translator.py ---
# --- Importing necessary libraries ---
# --- Importowanie niezbÄ™dnych bibliotek ---
# For filesystem operations like creating paths and folders. / Do operacji na systemie plikÃ³w, jak tworzenie Å›cieÅ¼ek i folderÃ³w.
import os
# For opening and extracting text from PDF files. / Do otwierania i wyciÄ…gania tekstu z plikÃ³w PDF.
import pdfplumber
# For generating unique timestamps for filenames. / Do generowania unikalnych znacznikÃ³w czasu dla nazw plikÃ³w.
import datetime
# For loading configuration files in YAML format. / Do wczytywania plikÃ³w konfiguracycyjnych w formacie YAML.
import yaml
# For system interaction, e.g., to exit the script. / Do interakcji z systemem, np. do przerwania dziaÅ‚ania skryptu.
import sys
# For using regular expressions to fix line wrapping. / Do uÅ¼ywania wyraÅ¼eÅ„ regularnych w celu naprawy zawijania wierszy.
import re
# For structured logging in JSON format. / Do strukturalnego logowania w formacie JSON.
import json
# For timing operations and pausing the script. / Do mierzenia czasu operacji i pauzowania skryptu.
import time
# The official Google library for interacting with the Gemini API. / Oficjalna biblioteka Google do interakcji z API Gemini.
import google.generativeai as genai
# For handling specific API errors like rate limiting. / Do obsÅ‚ugi specyficznych bÅ‚Ä™dÃ³w API, takich jak limity zapytaÅ„.
from google.api_core import exceptions

# --- ReportLab imports for advanced PDF creation with paragraphs ---
# --- Importy ReportLab do zaawansowanego tworzenia PDF z akapitami ---
from reportlab.lib.pagesizes import A4
from reportlab.platypus import SimpleDocTemplate, Paragraph
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.pdfbase import pdfmetrics
from reportlab.lib.enums import TA_JUSTIFY


# --- MAIN CONFIGURATION LOADING FUNCTION ---
# --- GÅÃ“WNA FUNKCJA ÅADUJÄ„CA KONFIGURACJÄ˜ ---
def load_configuration(config_path='config.yaml'):
    """
    Loads configuration from a YAML file.
    Wczytuje konfiguracjÄ™ z pliku YAML.
    """
    try:
        with open(config_path, "r", encoding="utf-8") as cr:
            config = yaml.full_load(cr)

        if 'base_path' not in config:
            raise ValueError("Key 'base_path' is required.")
        if 'translator_script_config' not in config:
            raise ValueError("Section 'translator_script_config' is required.")
        if 'KEY' not in config or not config['KEY'] or config['KEY'] == "TWOJ_KLUCZ_API_GEMINI" or config[
            'KEY'] == "*****":
            raise ValueError("Gemini API Key ('KEY') is missing or is a placeholder.")

        base_path = config['base_path']
        translator_config = config['translator_script_config']

        conf = {
            "GEMINI_API_KEY": config['KEY'],
            "SOURCE_FOLDER": os.path.join(base_path, translator_config['source_folder']),
            "OUTPUT_FOLDER": os.path.join(base_path, translator_config['output_folder']),
            "LOG_FOLDER": os.path.join(base_path, translator_config.get('log_folder', 'LOGS')),
            "FONT_PATH": os.path.join(base_path, translator_config['font_path']),
            "FONT_NAME": translator_config['font_name'],
            "FONT_SIZE": translator_config.get('font_size', 10),
            "MODEL_NAME": translator_config.get('model_name', 'gemini-1.5-flash'),
            "CHUNK_SIZE_CHARS": translator_config.get('chunk_size_chars', 30000),
            "TARGET_LANGUAGES": translator_config.get('target_languages', ['English', 'Polish', 'Czech'])
        }
        print("Configuration loaded successfully.")
        print("Konfiguracja zaÅ‚adowana pomyÅ›lnie.")
        return conf
    except Exception as e:
        print(f"FATAL ERROR in configuration: {e}")
        print(f"BÅÄ„D KRYTYCZNY w konfiguracji: {e}")
        return None


# --- Initialization ---
# --- Inicjalizacja ---
CONFIG = load_configuration()
MODEL = None
if CONFIG:
    try:
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]
        genai.configure(api_key=CONFIG['GEMINI_API_KEY'])
        MODEL = genai.GenerativeModel(CONFIG['MODEL_NAME'], safety_settings=safety_settings)
        print("Gemini API configured successfully.")
        print("Gemini API skonfigurowane pomyÅ›lnie.")
    except Exception as e:
        CONFIG = None
    if CONFIG:
        os.makedirs(CONFIG['SOURCE_FOLDER'], exist_ok=True)
        os.makedirs(CONFIG['OUTPUT_FOLDER'], exist_ok=True)
        os.makedirs(CONFIG['LOG_FOLDER'], exist_ok=True)
        try:
            pdfmetrics.registerFont(TTFont(CONFIG['FONT_NAME'], CONFIG['FONT_PATH']))
            print(f"Font '{CONFIG['FONT_NAME']}' registered successfully.")
            print(f"Czcionka '{CONFIG['FONT_NAME']}' zarejestrowana pomyÅ›lnie.")
        except Exception as e:
            print(f"ERROR: Could not register font. Defaulting to Helvetica. Error: {e}")
            print(f"BÅÄ„D: Nie moÅ¼na zarejestrowaÄ‡ czcionki. UÅ¼ywam domyÅ›lnej Helvetica. BÅ‚Ä…d: {e}")
            CONFIG['FONT_NAME'] = "Helvetica"
else:
    print("Exiting script due to configuration errors.")
    print("Zamykanie skryptu z powodu bÅ‚Ä™dÃ³w konfiguracji.")
    sys.exit(1)


# --- Core Functions ---
# --- GÅ‚Ã³wne Funkcje ---

def extract_full_text_from_pdf(pdf_path):
    """
    Extracts all text from a PDF file into a single string.
    Ekstrahuje caÅ‚y tekst z pliku PDF do jednego ciÄ…gu znakÃ³w.
    """
    print(f"Extracting full text from '{os.path.basename(pdf_path)}'...")
    print(f"EkstrahujÄ™ peÅ‚ny tekst z '{os.path.basename(pdf_path)}'...")
    full_text = []
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    full_text.append(page_text)
        return "\n\n".join(full_text)
    except Exception as e:
        print(f"ERROR: Could not read PDF file {pdf_path}: {e}")
        print(f"BÅÄ„D: Nie moÅ¼na odczytaÄ‡ pliku PDF {pdf_path}: {e}")
        return None


def chunk_text(text, chunk_size):
    """
    Splits a large text into smaller chunks based on a character size limit.
    Dzieli duÅ¼y tekst na mniejsze kawaÅ‚ki na podstawie limitu znakÃ³w.
    """
    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]


def translate_text_in_chunks(text_to_translate, target_language):
    """
    Translates text, handles rate limits and overloads by retrying, and manages other safety blocks.
    TÅ‚umaczy tekst, obsÅ‚uguje limity i przeciÄ…Å¼enia poprzez ponawianie prÃ³b i zarzÄ…dza innymi blokadami.
    """
    if not text_to_translate or not text_to_translate.strip():
        return "", 0, 0, "EMPTY_INPUT"

    chunks = chunk_text(text_to_translate, CONFIG['CHUNK_SIZE_CHARS'])
    print(f"Text divided into {len(chunks)} chunk(s) for translation to {target_language}.")
    print(f"Tekst podzielony na {len(chunks)} czÄ™Å›ci do tÅ‚umaczenia na {target_language}.")

    translated_parts = []
    total_input_tokens = 0
    total_output_tokens = 0
    final_status = "Success"

    base_prompt = f"You are a professional translator. Translate the following document fragment into {target_language}. Preserve the original formatting, including paragraph breaks. Return only the translated text, without any additional comments, explanations or introductions."

    for i, chunk in enumerate(chunks):
        print(f"  Translating chunk {i + 1}/{len(chunks)}...")
        print(f"  TÅ‚umaczÄ™ czÄ™Å›Ä‡ {i + 1}/{len(chunks)}...")
        prompt = f"{base_prompt}\n\nFragment to translate:\n\n{chunk}"

        max_retries = 5
        current_retry = 0
        delay = 15

        while current_retry < max_retries:
            try:
                response = MODEL.generate_content(prompt)

                if not response.parts:
                    block_reason = response.prompt_feedback.block_reason.name if response.prompt_feedback else "UNKNOWN"
                    print(
                        f"  WARNING: Chunk {i + 1} translation blocked by API. Reason: {block_reason}. Inserting original text.")
                    print(
                        f"  OSTRZEÅ»ENIE: TÅ‚umaczenie fragmentu {i + 1} zablokowane przez API. PowÃ³d: {block_reason}. Wstawiam oryginalny tekst.")
                    warning_msg = f"[API TRANSLATION BLOCKED (REASON: {block_reason}) - ORIGINAL TEXT INSERTED BELOW]"
                    translated_parts.append(f"\n\n--- {warning_msg} ---\n\n{chunk}\n\n")
                    final_status = "API_BLOCK"
                    break

                translated_parts.append(response.text)
                if hasattr(response, 'usage_metadata'):
                    total_input_tokens += response.usage_metadata.prompt_token_count
                    total_output_tokens += response.usage_metadata.candidates_token_count
                break

            except (exceptions.ResourceExhausted, exceptions.ServiceUnavailable, exceptions.DeadlineExceeded) as e:
                current_retry += 1
                error_type = type(e).__name__
                if current_retry >= max_retries:
                    print(f"  ERROR: Max retries exceeded for chunk. Error: {error_type}. Inserting original text.")
                    print(
                        f"  BÅÄ„D: Przekroczono maksymalnÄ… liczbÄ™ prÃ³b dla fragmentu. BÅ‚Ä…d: {error_type}. Wstawiam oryginalny tekst.")
                    translated_parts.append(
                        f"\n\n[TRANSLATION FAILED AFTER RETRIES: {error_type}] - ORIGINAL TEXT INSERTED BELOW\n\n{chunk}\n\n")
                    final_status = "MAX_RETRIES_EXCEEDED"
                    break

                print(
                    f"  API temporary error ({error_type}). Retrying in {delay} seconds... (Attempt {current_retry}/{max_retries})")
                print(
                    f"  Tymczasowy bÅ‚Ä…d API ({error_type}). Ponawiam prÃ³bÄ™ za {delay} sekund... (PrÃ³ba {current_retry}/{max_retries})")
                time.sleep(delay)
                delay *= 2

            except Exception as e:
                print(f"  An unexpected API error occurred: {e}. Inserting original text.")
                print(f"  WystÄ…piÅ‚ nieoczekiwany bÅ‚Ä…d API: {e}. Wstawiam oryginalny tekst.")
                translated_parts.append(
                    f"\n\n[UNEXPECTED TRANSLATION ERROR: {e}] - ORIGINAL TEXT INSERTED BELOW\n\n{chunk}\n\n")
                final_status = "UNEXPECTED_ERROR"
                break
        time.sleep(1)

    print(f"Token Usage for {target_language}: Input={total_input_tokens}, Output={total_output_tokens}")
    print(f"ZuÅ¼ycie tokenÃ³w dla {target_language}: WejÅ›cie={total_input_tokens}, WyjÅ›cie={total_output_tokens}")
    return "\n\n".join(translated_parts), total_input_tokens, total_output_tokens, final_status


def reflow_text(text: str) -> str:
    """
    Intelligently joins lines for better PDF formatting.
    Inteligentnie Å‚Ä…czy linie dla lepszego formatowania PDF.
    """
    reflowed = re.sub(r'(?<!\n)\n(?!\n)', ' ', text)
    reflowed = re.sub(r'\n{2,}', '<br/><br/>', reflowed)
    return reflowed


def save_text_to_pdf(text_content, output_pdf_path):
    """
    Saves the given text content to a PDF file using Platypus for proper text wrapping.
    Zapisuje podany tekst do pliku PDF, uÅ¼ywajÄ…c biblioteki Platypus do poprawnego zawijania tekstu.
    """
    font_name, font_size = CONFIG['FONT_NAME'], CONFIG['FONT_SIZE']
    print(f"Saving PDF to: {output_pdf_path}...")
    print(f"ZapisujÄ™ PDF do: {output_pdf_path}...")

    doc = SimpleDocTemplate(output_pdf_path, pagesize=A4, rightMargin=72, leftMargin=72, topMargin=72, bottomMargin=72)
    styles = getSampleStyleSheet()
    style = ParagraphStyle(name='Normal_Justified', parent=styles['Normal'], fontName=font_name, fontSize=font_size,
                           leading=font_size * 1.5, alignment=TA_JUSTIFY)

    processed_text = reflow_text(text_content)
    story = [Paragraph(processed_text, style)]

    try:
        doc.build(story)
        print(f"Successfully saved PDF: {output_pdf_path}")
        print(f"PomyÅ›lnie zapisano PDF: {output_pdf_path}")
    except Exception as e:
        print(f"ERROR saving PDF: {e}")
        print(f"BÅÄ„D podczas zapisywania PDF: {e}")


# --- New Logging Function ---
# --- Nowa Funkcja Logowania ---
def write_summary_log(log_data):
    """
    Writes a JSON summary log for the entire session.
    Zapisuje podsumowujÄ…cy log JSON dla caÅ‚ej sesji.
    """
    log_file_path = os.path.join(CONFIG['LOG_FOLDER'], f"translator_log_{log_data['session_start_iso']}.json")
    try:
        with open(log_file_path, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, indent=4, ensure_ascii=False)
        print(f"\n--- Summary log created successfully: {log_file_path} ---")
        print(f"--- PodsumowujÄ…cy log zostaÅ‚ pomyÅ›lnie utworzony: {log_file_path} ---")
    except Exception as e:
        print(f"ERROR: Could not write summary log file: {e}")
        print(f"BÅÄ„D: Nie moÅ¼na zapisaÄ‡ pliku z logami: {e}")


# --- Main Execution ---
# --- GÅ‚Ã³wne Wykonanie ---
def main():
    """
    Main function to find, translate, and save PDFs into separate files per language.
    GÅ‚Ã³wna funkcja do znajdowania, tÅ‚umaczenia i zapisywania plikÃ³w PDF w osobnych plikach dla kaÅ¼dego jÄ™zyka.
    """
    script_start_time = time.time()
    session_start_iso = datetime.datetime.now().isoformat().replace(":", "-")
    session_log_details = []

    print(f"Starting PDF translation from folder: {CONFIG['SOURCE_FOLDER']}")
    print(f"Rozpoczynam tÅ‚umaczenie PDF z folderu: {CONFIG['SOURCE_FOLDER']}")

    pdf_files = sorted([f for f in os.listdir(CONFIG['SOURCE_FOLDER']) if f.lower().endswith(".pdf")])

    if not pdf_files:
        print("INFO: No PDF files found in the source folder for translation.")
        print("INFO: Brak plikÃ³w PDF w folderze ÅºrÃ³dÅ‚owym do tÅ‚umaczenia.")
        return

    for pdf_file in pdf_files:
        print(f"\n--- Processing file: {pdf_file} ---")
        print(f"--- Przetwarzam plik: {pdf_file} ---")

        file_start_time = time.time()
        pdf_path = os.path.join(CONFIG['SOURCE_FOLDER'], pdf_file)

        file_log = {
            "file_name": pdf_file,
            "status": "Processing",
            "details": "",
            "translations": []
        }

        original_text = extract_full_text_from_pdf(pdf_path)

        if not original_text:
            print(f"WARNING: No text extracted from '{pdf_file}'. Skipping.")
            print(f"OSTRZEÅ»ENIE: Nie wyekstrahowano tekstu z '{pdf_file}'. Pomijam.")
            file_log['status'] = "Skipped"
            file_log['details'] = "No text extracted from PDF."
            session_log_details.append(file_log)
            continue

        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = os.path.splitext(pdf_file)[0]

        lang_suffix_map = {
            "english": "EN", "polish": "PL", "arabic": "AR", "chinese": "ZH",
            "czech": "CS", "french": "FR", "german": "DE", "hebrew": "HE",
            "japanese": "JA", "persian": "FA", "russian": "RU", "spanish": "ES",
            "ukrainian": "UK",
        }

        any_translation_failed = False

        for lang in CONFIG.get('TARGET_LANGUAGES', ['English', 'Polish']):
            print(f"\n--- Starting translation for {lang} ---")
            print(f"--- Rozpoczynam tÅ‚umaczenie na {lang} ---")

            lang_start_time = time.time()
            lang_log = {"language": lang}

            translated_text, input_tokens, output_tokens, status = translate_text_in_chunks(original_text, lang)

            lang_log["status"] = status
            lang_log["token_usage"] = {"input": input_tokens, "output": output_tokens,
                                       "total": input_tokens + output_tokens}

            if translated_text and status == "Success":
                lang_lower = lang.lower()
                lang_suffix = lang_suffix_map.get(lang_lower, lang_lower[:2].upper())
                output_path = os.path.join(CONFIG['OUTPUT_FOLDER'], f"{base_name}_{timestamp}_{lang_suffix}.pdf")
                save_text_to_pdf(translated_text, output_path)
                lang_log["output_file"] = output_path
            else:
                print(f"WARNING: Translation for {lang} resulted in empty or problematic text. No file will be saved.")
                print(
                    f"OSTRZEÅ»ENIE: TÅ‚umaczenie na {lang} zwrÃ³ciÅ‚o pusty lub problematyczny tekst. Plik nie zostanie zapisany.")
                lang_log["output_file"] = None
                if status != "API_BLOCK":  # Don't mark as failure if it's a known non-critical issue
                    any_translation_failed = True

            lang_log["processing_time_seconds"] = round(time.time() - lang_start_time, 2)
            file_log["translations"].append(lang_log)

            sleep_duration = 20
            print(f"--- Finished {lang}. Waiting for {sleep_duration} seconds before next language... ---")
            print(f"--- UkoÅ„czono {lang}. Czekam {sleep_duration} sekund przed kolejnym jÄ™zykiem... ---")
            time.sleep(sleep_duration)

        file_log["status"] = "Completed_with_errors" if any_translation_failed else "Completed_successfully"
        file_log["total_processing_time_seconds"] = round(time.time() - file_start_time, 2)
        session_log_details.append(file_log)

    print("\n--- Finished translation process for all files. ---")
    print("\n--- ZakoÅ„czono proces tÅ‚umaczenia dla wszystkich plikÃ³w. ---")

    script_end_time = time.time()
    overall_log = {
        "session_start_iso": session_start_iso,
        "total_duration_seconds": round(script_end_time - script_start_time, 2),
        "total_files_processed": len(pdf_files),
        "model_used": CONFIG['MODEL_NAME'],
        "processed_files": session_log_details
    }
    write_summary_log(overall_log)


if __name__ == "__main__":
    if CONFIG:
        main()
--- END FILE: scripts/pdf_translator.py ---

